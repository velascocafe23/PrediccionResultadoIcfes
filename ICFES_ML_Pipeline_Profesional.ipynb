{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cadf57ee",
   "metadata": {},
   "source": [
    "# ğŸ“Š Modelamiento Predictivo â€“ Resultados ICFES Saber 11\n",
    "### Aprendizaje de MÃ¡quinas | Universidad Pontificia Bolivariana\n",
    "\n",
    "---\n",
    "\n",
    "## DescripciÃ³n del Proyecto\n",
    "\n",
    "Pipeline completo de Machine Learning para predecir los puntajes del examen Saber 11\n",
    "a partir de variables socioeconÃ³micas, familiares e institucionales.\n",
    "\n",
    "## Etapas del Pipeline\n",
    "\n",
    "| # | Etapa | DescripciÃ³n |\n",
    "|---|-------|-------------|\n",
    "| 1 | **ConfiguraciÃ³n** | Importaciones y constantes globales |\n",
    "| 2 | **Carga de datos** | Lectura eficiente del CSV |\n",
    "| 3 | **ExploraciÃ³n (EDA)** | EstadÃ­sticas, distribuciones y anÃ¡lisis de nulos |\n",
    "| 4 | **Limpieza** | NormalizaciÃ³n, correcciÃ³n de valores y tipos de datos |\n",
    "| 5 | **ImputaciÃ³n** | Tratamiento de valores faltantes |\n",
    "| 6 | **CorrelaciÃ³n** | Pearson (numÃ©ricas) y CramÃ©r's V (categÃ³ricas) |\n",
    "| 7 | **Feature Engineering** | Variables derivadas y eliminaciÃ³n de redundantes |\n",
    "| 8 | **CodificaciÃ³n** | Ordinal y Label Encoding de categÃ³ricas |\n",
    "| 9 | **Escalado** | MinMaxScaler [0, 1] para variables numÃ©ricas |\n",
    "| 10 | **Dataset final** | VerificaciÃ³n y exportaciÃ³n para modelado |\n",
    "\n",
    "## Variables\n",
    "\n",
    "**Excluidas en carga:** identificadores ICFES, cÃ³digos DANE y campos administrativos\n",
    "\n",
    "**Variables independientes (X):** caracterÃ­sticas del estudiante, colegio y familia\n",
    "\n",
    "**Variables dependientes (y):** `PUNT_GLOBAL`, `PUNT_MATEMATICAS`, `PUNT_INGLES`,\n",
    "`PUNT_LECTURA_CRITICA`, `PUNT_C_NATURALES`, `PUNT_SOCIALES_CIUDADANAS`, `DESEMP_INGLES`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9da28c6",
   "metadata": {},
   "source": [
    "## âš™ï¸ 1. ConfiguraciÃ³n del Entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b1acbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 1.1  IMPORTACIONES\n",
    "# Todas las librerÃ­as se importan al inicio del notebook (buena prÃ¡ctica PEP8).\n",
    "# =============================================================================\n",
    "\n",
    "import unicodedata\n",
    "import warnings\n",
    "from typing import List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, OrdinalEncoder\n",
    "\n",
    "# â”€â”€ ConfiguraciÃ³n global â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_columns\", 60)\n",
    "pd.set_option(\"display.float_format\", \"{:.4f}\".format)\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"muted\", font_scale=1.1)\n",
    "plt.rcParams.update({\"figure.dpi\": 120, \"figure.figsize\": (12, 5)})\n",
    "\n",
    "print(\"âœ… LibrerÃ­as cargadas.\")\n",
    "print(f\"   pandas {pd.__version__}  |  numpy {np.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d77e6a",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# 1.2  CONSTANTES Y CONFIGURACIÃ“N CENTRALIZADA\n# Centralizar rutas y listas de columnas evita \"magic strings\" dispersos y\n# facilita el mantenimiento del cÃ³digo.\n# =============================================================================\n\nFILE_PATH = \"/content/drive/MyDrive/AprendizajeMaquinas/Resultados_Ãºnicos_Saber_11_20260224.csv\"\n\n# â”€â”€ Columnas administrativas a descartar en la carga â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nCOLUMNS_TO_DROP: List[str] = [\n    \"ESTU_TIPODOCUMENTO\", \"ESTU_CONSECUTIVO\",\n    \"COLE_COD_DANE_ESTABLECIMIENTO\", \"COLE_COD_DANE_SEDE\",\n    \"COLE_COD_DEPTO_UBICACION\", \"COLE_COD_MCPIO_UBICACION\",\n    # COLE_DEPTO_UBICACION se conserva para filtrar por departamento en carga\n    \"COLE_CODIGO_ICFES\", \"COLE_GENERO\", \"COLE_NATURALEZA\",\n    \"COLE_NOMBRE_ESTABLECIMIENTO\", \"COLE_NOMBRE_SEDE\", \"COLE_SEDE_PRINCIPAL\",\n    \"ESTU_COD_DEPTO_PRESENTACION\", \"ESTU_COD_MCPIO_PRESENTACION\",\n    \"ESTU_COD_RESIDE_DEPTO\", \"ESTU_COD_RESIDE_MCPIO\",\n    \"ESTU_DEPTO_PRESENTACION\", \"ESTU_DEPTO_RESIDE\",\n    \"ESTU_ESTADOINVESTIGACION\", \"ESTU_ESTUDIANTE\",\n    \"ESTU_MCPIO_PRESENTACION\", \"ESTU_MCPIO_RESIDE\",\n    \"ESTU_NACIONALIDAD\", \"ESTU_PAIS_RESIDE\",\n]\n\n# â”€â”€ Variables dependientes (targets a predecir) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nTARGET_COLS: List[str] = [\n    \"DESEMP_INGLES\",\n    \"PUNT_INGLES\",\n    \"PUNT_MATEMATICAS\",\n    \"PUNT_SOCIALES_CIUDADANAS\",\n    \"PUNT_C_NATURALES\",\n    \"PUNT_LECTURA_CRITICA\",\n    \"PUNT_GLOBAL\",\n]\n\n# Puntajes numÃ©ricos (subconjunto de targets, excluye la variable ordinal)\nSCORE_COLS: List[str] = [t for t in TARGET_COLS if t != \"DESEMP_INGLES\"]\n\n# Columnas crÃ­ticas: se elimina la fila si alguna es nula\nKEY_COLS: List[str] = [\"PUNT_SOCIALES_CIUDADANAS\", \"PUNT_C_NATURALES\", \"PUNT_GLOBAL\"]\n\n# Representaciones textuales de \"dato faltante\"\nNA_VALUES: List[str] = [\n    \"\", \" \", \"NA\", \"N/A\", \"NULL\", \"null\",\n    \"SIN DATO\", \"Sin dato\", \"-\", \"--\", \"No aplica\", \"NO APLICA\",\n]\n\n# Variables binarias SÃ­/No del hogar\nASSET_COLS: List[str] = [\n    \"FAMI_TIENEAUTOMOVIL\", \"FAMI_TIENECOMPUTADOR\",\n    \"FAMI_TIENEINTERNET\",  \"FAMI_TIENELAVADORA\",\n]\nYN_COLS: List[str] = ASSET_COLS + [\"COLE_BILINGUE\"]\n\n# Variables independientes finales (post feature engineering)\nINDEPENDENT_VARS: List[str] = [\n    \"COLE_AREA_UBICACION\", \"COLE_BILINGUE\", \"COLE_CALENDARIO\",\n    \"COLE_CARACTER\", \"COLE_JORNADA\", \"COLE_MCPIO_UBICACION\",\n    \"ESTU_GENERO\", \"EDAD\",\n    \"FAMI_CUARTOSHOGAR\", \"FAMI_EDUCACIONMADRE\", \"FAMI_EDUCACIONPADRE\",\n    \"FAMI_ESTRATOVIVIENDA\", \"FAMI_PERSONASHOGAR\", \"INDICE_BIENES\",\n    \"TRIMESTRE\",\n]\n\nprint(\"âœ… Constantes del proyecto configuradas.\")\n"
  },
  {
   "cell_type": "markdown",
   "id": "c5df6df7",
   "metadata": {},
   "source": [
    "## ğŸ“¥ 2. Carga del Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a69e7c8",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# 2.1  CARGA EFICIENTE DEL CSV â€” Solo registros del departamento QUINDIO\n# =============================================================================\n\n# Paso 1: Header\nall_columns = pd.read_csv(FILE_PATH, nrows=0, encoding=\"latin-1\").columns.tolist()\n\n# Paso 2: Filtrado de columnas (conservar COLE_DEPTO_UBICACION para filtrar)\ncolumns_to_use = [c for c in all_columns if c not in COLUMNS_TO_DROP]\nprint(f\"Columnas en el archivo  : {len(all_columns)}\")\nprint(f\"Columnas descartadas    : {len(COLUMNS_TO_DROP)}\")\nprint(f\"Columnas que se cargan  : {len(columns_to_use)}\")\n\n# Paso 3: Carga completa\ndf_raw_full = pd.read_csv(\n    FILE_PATH,\n    usecols=columns_to_use,\n    encoding=\"latin-1\",\n    low_memory=False,\n)\n\n# Paso 4: Filtrar solo QUINDIO\n# Normalizar el campo para comparacion robusta (sin tildes, mayusculas)\nimport unicodedata\ndef strip_accents(s):\n    if pd.isna(s): return s\n    return ''.join(c for c in unicodedata.normalize('NFD', str(s))\n                   if unicodedata.category(c) != 'Mn').upper().strip()\n\nif \"COLE_DEPTO_UBICACION\" in df_raw_full.columns:\n    depto_norm = df_raw_full[\"COLE_DEPTO_UBICACION\"].apply(strip_accents)\n    mask_quindio = depto_norm == \"QUINDIO\"\n    df_raw = df_raw_full[mask_quindio].copy().reset_index(drop=True)\n    print(f\"\\n  Registros totales en CSV : {len(df_raw_full):>10,}\")\n    print(f\"  Registros QUINDIO        : {len(df_raw):>10,}  ({len(df_raw)/len(df_raw_full)*100:.2f}%)\")\n    # Eliminar la columna de departamento (ya no es util como variable)\n    df_raw.drop(columns=[\"COLE_DEPTO_UBICACION\"], inplace=True, errors=\"ignore\")\nelse:\n    df_raw = df_raw_full.copy()\n    print(\"  AVISO: COLE_DEPTO_UBICACION no encontrada, se carga dataset completo\")\n\nmem_mb = df_raw.memory_usage(deep=True).sum() / 1024**2\nprint(f\"\\n Dataset final cargado:\")\nprint(f\"   Filas     : {df_raw.shape[0]:>12,}\")\nprint(f\"   Columnas  : {df_raw.shape[1]:>12,}\")\nprint(f\"   Memoria   : {mem_mb:>11.1f} MB\")\n"
  },
  {
   "cell_type": "markdown",
   "id": "b2bff4f9",
   "metadata": {},
   "source": [
    "## ğŸ” 3. ExploraciÃ³n de Datos (EDA)\n",
    "\n",
    "El AnÃ¡lisis Exploratorio de Datos permite entender la estructura del dataset,\n",
    "detectar problemas de calidad y guiar las decisiones de preprocesamiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6e4374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3.1  INFORMACIÃ“N GENERAL\n",
    "# =============================================================================\n",
    "print(\"=\" * 65)\n",
    "print(\" TIPOS DE DATOS Y VALORES NO NULOS\")\n",
    "print(\"=\" * 65)\n",
    "df_raw.info(verbose=True, show_counts=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2257e0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3.2  ESTADÃSTICAS DESCRIPTIVAS\n",
    "# =============================================================================\n",
    "print(\"â”€â”€ Variables numÃ©ricas â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "display(df_raw.describe().round(2))\n",
    "\n",
    "print(\"\\nâ”€â”€ Variables categÃ³ricas (object) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "display(df_raw.describe(include=[\"object\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84671aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3.3  ANÃLISIS DE VALORES NULOS\n",
    "# =============================================================================\n",
    "null_df = pd.DataFrame({\n",
    "    \"Nulos\"   : df_raw.isnull().sum(),\n",
    "    \"% Nulos\" : (df_raw.isnull().mean() * 100).round(2),\n",
    "    \"Tipo\"    : df_raw.dtypes,\n",
    "    \"Ãšnicos\"  : df_raw.nunique(),\n",
    "}).sort_values(\"% Nulos\", ascending=False)\n",
    "\n",
    "print(\"â”€â”€ Columnas con valores nulos â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "display(null_df[null_df[\"Nulos\"] > 0])\n",
    "\n",
    "# Heatmap de nulos (muestra de 5 000 filas)\n",
    "sample_nulls = df_raw.sample(min(5000, len(df_raw)), random_state=42).isnull()\n",
    "cols_w_nulls = sample_nulls.columns[sample_nulls.any()].tolist()\n",
    "\n",
    "if cols_w_nulls:\n",
    "    fig, ax = plt.subplots(figsize=(16, 5))\n",
    "    sns.heatmap(sample_nulls[cols_w_nulls].T, cbar=False, cmap=\"YlOrRd\",\n",
    "                ax=ax, xticklabels=False)\n",
    "    ax.set_title(\"Mapa de calor de valores nulos (muestra 5 000 filas)\", fontsize=13)\n",
    "    ax.set_xlabel(\"Observaciones\"); ax.set_ylabel(\"Variables\")\n",
    "    plt.tight_layout(); plt.savefig(\"heatmap_nulos.png\", dpi=130, bbox_inches=\"tight\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7ada3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3.4  DISTRIBUCIÃ“N DE VARIABLES TARGET (PUNTAJES)\n",
    "# =============================================================================\n",
    "score_present = [c for c in SCORE_COLS if c in df_raw.columns]\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 9))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(score_present):\n",
    "    data = df_raw[col].dropna()\n",
    "    axes[i].hist(data, bins=50, color=\"#4C72B0\", edgecolor=\"white\", linewidth=0.5)\n",
    "    axes[i].axvline(data.mean(),   color=\"#DD8452\", lw=2, ls=\"--\", label=f\"Media {data.mean():.1f}\")\n",
    "    axes[i].axvline(data.median(), color=\"#55A868\", lw=2, ls=\":\",  label=f\"Mediana {data.median():.1f}\")\n",
    "    axes[i].set_title(col, fontsize=11, fontweight=\"bold\")\n",
    "    axes[i].set_xlabel(\"Puntaje\"); axes[i].set_ylabel(\"Frecuencia\")\n",
    "    axes[i].legend(fontsize=8)\n",
    "\n",
    "for j in range(len(score_present), len(axes)):\n",
    "    axes[j].set_visible(False)\n",
    "\n",
    "fig.suptitle(\"DistribuciÃ³n de Variables Target (Puntajes)\", fontsize=14, y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"distribucion_targets.png\", dpi=130, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b04167a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3.5  DISTRIBUCIÃ“N DE VARIABLES CATEGÃ“RICAS CLAVE\n",
    "# =============================================================================\n",
    "cat_eda = [c for c in [\n",
    "    \"ESTU_GENERO\", \"COLE_AREA_UBICACION\", \"COLE_BILINGUE\",\n",
    "    \"COLE_CALENDARIO\", \"COLE_JORNADA\", \"FAMI_ESTRATOVIVIENDA\",\n",
    "    \"FAMI_EDUCACIONMADRE\", \"FAMI_EDUCACIONPADRE\",\n",
    "] if c in df_raw.columns]\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(18, 9))\n",
    "axes = axes.flatten()\n",
    "palette = sns.color_palette(\"muted\")\n",
    "\n",
    "for i, col in enumerate(cat_eda):\n",
    "    vc = df_raw[col].value_counts(dropna=False).head(10)\n",
    "    vc.plot(kind=\"bar\", ax=axes[i], color=palette, edgecolor=\"white\")\n",
    "    axes[i].set_title(col, fontsize=10, fontweight=\"bold\")\n",
    "    axes[i].tick_params(axis=\"x\", rotation=35, labelsize=8)\n",
    "    axes[i].set_ylabel(\"Frecuencia\" if i % 4 == 0 else \"\")\n",
    "\n",
    "for j in range(len(cat_eda), len(axes)):\n",
    "    axes[j].set_visible(False)\n",
    "\n",
    "fig.suptitle(\"DistribuciÃ³n de Variables CategÃ³ricas Clave\", fontsize=14, y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"distribucion_categoricas.png\", dpi=130, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b78995",
   "metadata": {},
   "source": [
    "## ğŸ§¹ 4. Limpieza de Datos\n",
    "\n",
    "Se corrigen inconsistencias de formato, se normalizan valores y se ajustan\n",
    "los tipos de datos de cada variable a los correctos para el modelado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69736eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 4.1  ELIMINACIÃ“N DE FILAS SIN PUNTAJES CLAVE\n",
    "# Los registros sin puntaje objetivo no pueden ser imputados de forma\n",
    "# confiable ya que son exactamente lo que queremos predecir.\n",
    "# =============================================================================\n",
    "df = df_raw.copy()\n",
    "initial_rows = len(df)\n",
    "df = df.dropna(subset=KEY_COLS).copy()\n",
    "removed = initial_rows - len(df)\n",
    "\n",
    "print(f\"Filas originales  : {initial_rows:>10,}\")\n",
    "print(f\"Filas conservadas : {len(df):>10,}  ({len(df)/initial_rows*100:.2f}%)\")\n",
    "print(f\"Filas eliminadas  : {removed:>10,}  ({removed/initial_rows*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3857788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 4.2  NORMALIZACIÃ“N DE CADENAS NULAS Y ESPACIOS\n",
    "# =============================================================================\n",
    "\n",
    "# Reemplazar representaciones textuales de nulo por NaN estÃ¡ndar de pandas\n",
    "df = df.replace(NA_VALUES, np.nan)\n",
    "\n",
    "# Eliminar espacios al inicio/final en columnas de texto\n",
    "for col in df.select_dtypes(include=[\"object\"]).columns:\n",
    "    df[col] = df[col].astype(\"string\").str.strip()\n",
    "\n",
    "print(\"âœ… Cadenas nulas normalizadas y espacios limpiados.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c972251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 4.3  FUNCIONES AUXILIARES DE NORMALIZACIÃ“N\n",
    "# =============================================================================\n",
    "\n",
    "def remove_accents(text) -> str:\n",
    "    \"\"\"Elimina tildes y diacrÃ­ticos de un string.\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    return \"\".join(\n",
    "        c for c in unicodedata.normalize(\"NFKD\", str(text))\n",
    "        if not unicodedata.combining(c)\n",
    "    )\n",
    "\n",
    "def norm_gender(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"Normaliza gÃ©nero a 'M' / 'F'. Valores desconocidos â†’ NaN.\"\"\"\n",
    "    x = series.astype(\"string\").str.upper().str.strip()\n",
    "    return x.replace({\n",
    "        \"MASCULINO\": \"M\", \"FEMENINO\": \"F\",\n",
    "        \"HOMBRE\":    \"M\", \"MUJER\":    \"F\",\n",
    "        \"MALE\":      \"M\", \"FEMALE\":   \"F\",\n",
    "    }).where(lambda s: s.isin([\"M\", \"F\"]), np.nan)\n",
    "\n",
    "def norm_yesno(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"Normaliza variable binaria a 'SI' / 'NO'. Valores desconocidos â†’ NaN.\"\"\"\n",
    "    x = series.astype(\"string\").str.upper().str.strip()\n",
    "    return x.replace({\n",
    "        \"SÃ\": \"SI\", \"YES\": \"SI\", \"Y\": \"SI\", \"1\": \"SI\", \"TRUE\": \"SI\",\n",
    "        \"N\":  \"NO\", \"0\":   \"NO\",             \"FALSE\": \"NO\",\n",
    "    }).where(lambda s: s.isin([\"SI\", \"NO\"]), np.nan)\n",
    "\n",
    "def fix_encoding(text):\n",
    "    \"\"\"Intenta corregir texto latin-1 decodificado incorrectamente como utf-8.\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    try:\n",
    "        return str(text).encode(\"latin1\").decode(\"utf-8\")\n",
    "    except (UnicodeEncodeError, UnicodeDecodeError):\n",
    "        return text\n",
    "\n",
    "print(\"âœ… Funciones auxiliares definidas.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741c2e18",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# 4.4  APLICACION DE NORMALIZACIONES\n# =============================================================================\n\n# Correccion de encoding en texto\nfor col in df.select_dtypes(include=[\"object\", \"string\"]).columns:\n    df[col] = df[col].apply(fix_encoding)\n\n# Genero\ndf[\"ESTU_GENERO\"] = norm_gender(df[\"ESTU_GENERO\"])\n\n# Variables Si/No\nfor col in YN_COLS:\n    if col in df.columns:\n        df[col] = norm_yesno(df[col])\n\n# COLE_BILINGUE: vacio, NaN o cadena vacia = SI (regla de negocio)\ndf[\"COLE_BILINGUE\"] = (\n    df[\"COLE_BILINGUE\"]\n    .astype(\"string\")\n    .str.strip()\n    .replace({\"\": \"SI\", \"<NA>\": \"SI\", \"None\": \"SI\", \"nan\": \"SI\"})\n)\ndf[\"COLE_BILINGUE\"] = df[\"COLE_BILINGUE\"].fillna(\"SI\")\n# Doble verificacion: cualquier valor que no sea NO se considera SI\ndf[\"COLE_BILINGUE\"] = df[\"COLE_BILINGUE\"].apply(\n    lambda x: \"NO\" if str(x).strip().upper() == \"NO\" else \"SI\"\n)\n\n# COLE_CARACTER: unificar variantes tipograficas (sin tildes)\ndf[\"COLE_CARACTER\"] = (\n    df[\"COLE_CARACTER\"].astype(\"string\").str.upper()\n    .str.replace(\"TECNICO/ACADEMICO\", \"TECNICO-ACADEMICO\", regex=False)\n    .str.replace(\"TECNICO\", \"TECNICO\", regex=False)\n    .apply(remove_accents)\n)\n\n# COLE_JORNADA: quitar tildes y caracteres especiales\nif \"COLE_JORNADA\" in df.columns:\n    df[\"COLE_JORNADA\"] = (\n        df[\"COLE_JORNADA\"].astype(\"string\").str.upper().str.strip()\n        .apply(remove_accents)\n    )\n\n# FAMI_EDUCACIONMADRE y FAMI_EDUCACIONPADRE: quitar tildes\nfor col in [\"FAMI_EDUCACIONMADRE\", \"FAMI_EDUCACIONPADRE\"]:\n    if col in df.columns:\n        df[col] = (\n            df[col].astype(\"string\").str.strip()\n            .apply(remove_accents)\n        )\n\n# Departamento: mayusculas y sin tildes para consistencia\nif \"COLE_DEPTO_UBICACION\" in df.columns:\n    df[\"COLE_DEPTO_UBICACION\"] = (\n        df[\"COLE_DEPTO_UBICACION\"].astype(\"string\").str.upper().apply(remove_accents)\n    )\n\nprint(\"Normalizaciones aplicadas.\")\nprint(\"  ESTU_GENERO   :\", df[\"ESTU_GENERO\"].value_counts(dropna=False).to_dict())\nprint(\"  COLE_BILINGUE :\", df[\"COLE_BILINGUE\"].value_counts(dropna=False).to_dict())\nprint(\"  COLE_JORNADA  :\", df[\"COLE_JORNADA\"].value_counts(dropna=False).to_dict() if \"COLE_JORNADA\" in df.columns else \"no disponible\")\nprint(\"  EDU_MADRE vals:\", df[\"FAMI_EDUCACIONMADRE\"].dropna().unique()[:5].tolist() if \"FAMI_EDUCACIONMADRE\" in df.columns else \"no disponible\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42beb8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 4.5  CONVERSIÃ“N DE TIPOS DE DATOS\n",
    "# Se asigna el dtype correcto a cada variable para garantizar cÃ³mputos vÃ¡lidos\n",
    "# y optimizar el uso de memoria.\n",
    "# =============================================================================\n",
    "\n",
    "# â”€â”€ Puntajes numÃ©ricos â†’ float64 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "for col in SCORE_COLS:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "# â”€â”€ Personas en el hogar â†’ numÃ©rico (puede venir como rango \"3 a 4\") â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "if \"FAMI_PERSONASHOGAR\" in df.columns:\n",
    "    df[\"FAMI_PERSONASHOGAR\"] = (\n",
    "        df[\"FAMI_PERSONASHOGAR\"].astype(\"string\").str.upper()\n",
    "        .str.extract(r\"(\\d+)\", expand=False)\n",
    "    )\n",
    "    df[\"FAMI_PERSONASHOGAR\"] = pd.to_numeric(df[\"FAMI_PERSONASHOGAR\"], errors=\"coerce\")\n",
    "\n",
    "# â”€â”€ Fecha de nacimiento â†’ datetime â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "df[\"ESTU_FECHANACIMIENTO\"] = pd.to_datetime(\n",
    "    df[\"ESTU_FECHANACIMIENTO\"], errors=\"coerce\", dayfirst=True\n",
    ")\n",
    "\n",
    "# â”€â”€ Variables categÃ³ricas â†’ dtype category (ahorra memoria en baja cardinalidad) â”€â”€\n",
    "for col in df.select_dtypes(include=[\"object\", \"string\"]).columns:\n",
    "    if df[col].nunique(dropna=False) < 200:\n",
    "        df[col] = df[col].astype(\"category\")\n",
    "\n",
    "print(\"âœ… Tipos de datos convertidos:\")\n",
    "print(df.dtypes.value_counts().to_string())\n",
    "print(f\"\\nğŸ“‰ Memoria tras conversiÃ³n: {df.memory_usage(deep=True).sum()/1024**2:.1f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9436b4a5",
   "metadata": {},
   "source": [
    "### ğŸ—‘ï¸ 4.6 â€” Eliminar columnas con mÃ¡s del 50% de valores nulos\n",
    "\n",
    "Columnas donde la mitad o mÃ¡s de los registros son nulos no pueden imputarse\n",
    "de forma confiable y aÃ±aden ruido al modelo. Se eliminan antes de la imputaciÃ³n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef336a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 4.6  ELIMINAR COLUMNAS CON > 50% DE NULOS\n",
    "#\n",
    "# Umbral: si una columna tiene mÃ¡s del 50% de sus valores como NaN,\n",
    "# se descarta. Imputar mÃ¡s del 50% de los datos es estadÃ­sticamente\n",
    "# indefendible y puede introducir mÃ¡s sesgo que informaciÃ³n.\n",
    "# Se excluyen siempre las columnas TARGET_COLS para no perder variables obj.\n",
    "# =============================================================================\n",
    "\n",
    "NULL_THRESHOLD = 0.50\n",
    "\n",
    "n_rows = len(df)\n",
    "null_ratios = df.drop(columns=[c for c in TARGET_COLS if c in df.columns],\n",
    "                      errors='ignore').isnull().mean()\n",
    "\n",
    "cols_high_null = null_ratios[null_ratios > NULL_THRESHOLD].index.tolist()\n",
    "\n",
    "print(f'Umbral de nulos: > {NULL_THRESHOLD*100:.0f}%')\n",
    "print(f'Columnas analizadas: {len(null_ratios)}')\n",
    "print(f'Columnas a eliminar: {len(cols_high_null)}')\n",
    "print()\n",
    "\n",
    "if cols_high_null:\n",
    "    detalle = pd.DataFrame({\n",
    "        'Columna'  : cols_high_null,\n",
    "        '% Nulos'  : [round(null_ratios[c]*100, 2) for c in cols_high_null],\n",
    "        'Nulos'    : [int(null_ratios[c]*n_rows) for c in cols_high_null],\n",
    "        'No nulos' : [int((1-null_ratios[c])*n_rows) for c in cols_high_null],\n",
    "    }).sort_values('% Nulos', ascending=False).reset_index(drop=True)\n",
    "    display(detalle)\n",
    "\n",
    "    df = df.drop(columns=cols_high_null)\n",
    "\n",
    "    # Actualizar INDEPENDENT_VARS si alguna fue eliminada\n",
    "    INDEPENDENT_VARS = [v for v in INDEPENDENT_VARS if v not in cols_high_null]\n",
    "\n",
    "    print(f'\\nDataFrame tras eliminar columnas muy nulas: {df.shape}')\n",
    "    print(f'INDEPENDENT_VARS actualizado: {len(INDEPENDENT_VARS)} variables')\n",
    "else:\n",
    "    print('No hay columnas con mas del 50% de nulos. No se elimino nada.')\n",
    "\n",
    "print(f'\\nColumnas conservadas: {df.shape[1]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984035fa",
   "metadata": {},
   "source": [
    "### ğŸ—‘ï¸ 4.7 â€” Eliminar columnas quasi-constantes (> 90% un solo valor)\n",
    "\n",
    "Variables donde el 90%+ de los registros tienen el mismo valor aportan\n",
    "informaciÃ³n mÃ­nima al modelo y pueden causar problemas de varianza cero\n",
    "en validaciÃ³n cruzada. Se eliminan antes de la imputaciÃ³n.\n",
    "\n",
    "> **Umbral:** > 90% (mÃ¡s estricto que el 95% del diagnÃ³stico R10 en Â§5,\n",
    "> que era solo informativo). AquÃ­ sÃ­ se aplica la eliminaciÃ³n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a563fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 4.7  ELIMINAR COLUMNAS QUASI-CONSTANTES (> 90% UN SOLO VALOR)\n",
    "#\n",
    "# Una variable donde el 90%+ de los registros tienen el mismo valor\n",
    "# no aporta poder discriminativo al modelo y puede causar:\n",
    "#   - Varianza cero en folds de CV  -> error en algunos estimadores\n",
    "#   - Coeficientes inflados en Ridge/ElasticNet\n",
    "#   - Splits inutiles en arboles\n",
    "# Se excluyen siempre TARGET_COLS.\n",
    "# =============================================================================\n",
    "\n",
    "QC_THRESHOLD = 0.90\n",
    "\n",
    "excluir_qc = set(TARGET_COLS) | {'ESTU_FECHANACIMIENTO', 'PERIODO'}\n",
    "cols_eval  = [c for c in df.columns if c not in excluir_qc]\n",
    "\n",
    "qc_resultados = []\n",
    "cols_qc = []\n",
    "\n",
    "for col in cols_eval:\n",
    "    try:\n",
    "        vc       = df[col].value_counts(normalize=True, dropna=False)\n",
    "        if len(vc) == 0:\n",
    "            continue\n",
    "        top_ratio = float(vc.iloc[0])\n",
    "        top_val   = vc.index[0]\n",
    "        qc_resultados.append({\n",
    "            'Columna'       : col,\n",
    "            'Valor dominante': str(top_val)[:40],\n",
    "            '% dominante'   : round(top_ratio * 100, 2),\n",
    "            'Valores unicos': int(df[col].nunique()),\n",
    "            'Eliminar'      : top_ratio > QC_THRESHOLD,\n",
    "        })\n",
    "        if top_ratio > QC_THRESHOLD:\n",
    "            cols_qc.append(col)\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "df_qc = (pd.DataFrame(qc_resultados)\n",
    "         .sort_values('% dominante', ascending=False)\n",
    "         .reset_index(drop=True))\n",
    "\n",
    "print(f'Umbral quasi-constante: > {QC_THRESHOLD*100:.0f}%')\n",
    "print(f'Columnas evaluadas: {len(df_qc)}')\n",
    "print(f'Columnas a eliminar: {len(cols_qc)}')\n",
    "print()\n",
    "\n",
    "if cols_qc:\n",
    "    display(df_qc[df_qc['Eliminar'] == True]\n",
    "            [['Columna','Valor dominante','% dominante','Valores unicos']])\n",
    "    df = df.drop(columns=cols_qc)\n",
    "    INDEPENDENT_VARS = [v for v in INDEPENDENT_VARS if v not in cols_qc]\n",
    "    print(f'\\nDataFrame tras eliminar quasi-constantes: {df.shape}')\n",
    "    print(f'INDEPENDENT_VARS actualizado: {len(INDEPENDENT_VARS)} variables')\n",
    "else:\n",
    "    print('No hay columnas quasi-constantes (>90%). No se elimino nada.')\n",
    "\n",
    "# Visualizacion top 20 por % dominante\n",
    "top_vis = df_qc.head(20)\n",
    "fig, ax = plt.subplots(figsize=(10, max(4, len(top_vis)*0.38)))\n",
    "colors_vis = ['#C44E52' if e else '#4C72B0' for e in top_vis['Eliminar']]\n",
    "ax.barh(top_vis['Columna'], top_vis['% dominante'],\n",
    "        color=colors_vis, edgecolor='white', height=0.65)\n",
    "ax.axvline(QC_THRESHOLD*100, color='orange', lw=1.8, ls='--',\n",
    "           label=f'Umbral {QC_THRESHOLD*100:.0f}%')\n",
    "ax.set_xlabel('% valor dominante')\n",
    "ax.set_title('4.7 Quasi-constantes (rojo = eliminadas, azul = conservadas)')\n",
    "ax.legend(fontsize=9)\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig('cleaning_quasi_constantes.png', dpi=120, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('cleaning_quasi_constantes.png guardado')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cee514",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ›¡ï¸ 5. ValidaciÃ³n de Calidad de Datos\n",
    "\n",
    "Las reglas se aplican sobre `df` **post-limpieza (Â§4) y pre-imputaciÃ³n (Â§6)**.\n",
    "En este punto los datos estÃ¡n en sus **unidades originales** â€” sin EDAD calculada,\n",
    "sin INDICE_BIENES, sin encoding ni escalado.\n",
    "\n",
    "| Regla | QuÃ© valida | Variable(s) | AcciÃ³n si falla |\n",
    "|-------|-----------|-------------|-----------------|\n",
    "| R01 | Dominio `COLE_AREA_UBICACION` | `df` original | â†’ NaN (imputar luego) |\n",
    "| R02 | Consistencia geogrÃ¡fica municipioâ€“departamento | `df` original | Solo reporte |\n",
    "| R03 | Dominio `COLE_CALENDARIO` | `df` original | â†’ NaN (imputar luego) |\n",
    "| R04 | Dominio `COLE_CARACTER` | `df` original | â†’ NaN (imputar luego) |\n",
    "| R05 | Rango puntajes targets [0, 500] | `PUNT_*` originales | â†’ NaN |\n",
    "| R06 | Coherencia catÃ¡logo educaciÃ³n padres | CategorÃ­as originales | Solo reporte |\n",
    "| R07 | Dominio `FAMI_ESTRATOVIVIENDA` | Valores originales | â†’ NaN (imputar luego) |\n",
    "| R08 | Variables numÃ©ricas con valores negativos | NumÃ©ricas originales | â†’ NaN |\n",
    "| R09 | Multicolinealidad categÃ³rica (CramÃ©r's V > 0.80) | CategÃ³ricas originales | Reporte y recomendaciÃ³n |\n",
    "| R10 | Variables casi constantes (> 95%) | Todas las X disponibles | Reporte |\n",
    "\n",
    "> **Variables que NO existen aÃºn en este punto:**\n",
    "> `EDAD` (se crea en Â§6), `ANIO`, `TRIMESTRE` (Â§6), `INDICE_BIENES` (Â§7).\n",
    "> Las reglas que las necesitarÃ­an se omiten o adaptan para usar alternativas disponibles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c496f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 5.0  MOTOR DE REPORTE DE VALIDACIONES\n",
    "#      Se trabaja exclusivamente sobre `df` (post-limpieza, pre-imputaciÃ³n).\n",
    "#      Las correcciones â†’ NaN se propagan automÃ¡ticamente a la imputaciÃ³n.\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "@dataclass\n",
    "class VRule:\n",
    "    rule_id  : str\n",
    "    name     : str\n",
    "    passed   : bool\n",
    "    affected : int\n",
    "    total    : int\n",
    "    action   : str\n",
    "    detail   : str = \"\"\n",
    "\n",
    "    @property\n",
    "    def pct(self):\n",
    "        return round(self.affected / self.total * 100, 3) if self.total else 0.0\n",
    "\n",
    "    def __str__(self):\n",
    "        st = \"âœ… PASS\" if self.passed else \"âš ï¸  FAIL\"\n",
    "        return (\n",
    "            f\"{st}  [{self.rule_id}] {self.name}\\n\"\n",
    "            f\"        Afectados : {self.affected:,} / {self.total:,}  ({self.pct} %)\\n\"\n",
    "            f\"        AcciÃ³n    : {self.action}\"\n",
    "            + (f\"\\n        Detalle   : {self.detail}\" if self.detail else \"\")\n",
    "        )\n",
    "\n",
    "class DQReport:\n",
    "    def __init__(self): self.rules: List[VRule] = []\n",
    "\n",
    "    def add(self, r: VRule):\n",
    "        self.rules.append(r)\n",
    "        print(r)\n",
    "\n",
    "    def summary(self) -> pd.DataFrame:\n",
    "        return pd.DataFrame([{\n",
    "            \"ID\"        : r.rule_id,\n",
    "            \"Regla\"     : r.name,\n",
    "            \"Estado\"    : \"âœ… PASS\" if r.passed else \"âš ï¸  FAIL\",\n",
    "            \"Afectados\" : r.affected,\n",
    "            \"% Afect.\"  : r.pct,\n",
    "            \"AcciÃ³n\"    : r.action,\n",
    "        } for r in self.rules])\n",
    "\n",
    "    def print_summary(self):\n",
    "        ok = sum(1 for r in self.rules if r.passed)\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"  DQ REPORT  |  {ok}/{len(self.rules)} reglas aprobadas\")\n",
    "        print(f\"  DataFrame : df  (post-Â§4 limpieza, pre-Â§6 imputaciÃ³n)\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "dq = DQReport()\n",
    "print(f\"âœ… Motor DQReport listo.\")\n",
    "print(f\"   df.shape en este punto: {df.shape}\")\n",
    "print(f\"   Columnas: {list(df.columns)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ed2a28",
   "metadata": {},
   "source": [
    "### ğŸ”¹ R01 â€” Dominio vÃ¡lido: `COLE_AREA_UBICACION`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b408bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# R01  DOMINIO VÃLIDO â€” COLE_AREA_UBICACION\n",
    "#      Dominio oficial MEN: {URBANO, RURAL}\n",
    "#      Valores fuera â†’ NaN para que Â§6 impute con la moda.\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "VALID_AREA = {\"URBANO\", \"RURAL\"}\n",
    "COL = \"COLE_AREA_UBICACION\"\n",
    "\n",
    "if COL in df.columns:\n",
    "    vals = df[COL].astype(str).str.strip().str.upper()\n",
    "    mask = ~vals.isin(VALID_AREA) & ~vals.isin({\"NAN\", \"NONE\", \"<NA>\"})\n",
    "    n    = int(mask.sum())\n",
    "\n",
    "    print(f\"DistribuciÃ³n de {COL} (valores originales):\")\n",
    "    vc = vals.value_counts(dropna=False).reset_index()\n",
    "    vc.columns = [\"Valor\", \"Conteo\"]\n",
    "    vc[\"En dominio\"] = vc[\"Valor\"].isin(VALID_AREA).map({True: \"âœ…\", False: \"âŒ\"})\n",
    "    display(vc)\n",
    "\n",
    "    if n:\n",
    "        df.loc[mask, COL] = np.nan\n",
    "        print(f\"\\nâš ï¸  {n:,} valores fuera del dominio corregidos â†’ NaN\")\n",
    "    else:\n",
    "        print(f\"\\nâœ… Todos los valores estÃ¡n en el dominio.\")\n",
    "\n",
    "    dq.add(VRule(\"R01\", f\"Dominio {COL}\", n == 0, n, len(df),\n",
    "                 \"â†’ NaN, imputar en Â§6\" if n else \"Sin acciÃ³n\",\n",
    "                 f\"Dominio: {VALID_AREA}\"))\n",
    "else:\n",
    "    print(f\"âš ï¸  {COL} no presente en df.\")\n",
    "    dq.add(VRule(\"R01\", f\"Dominio {COL}\", True, 0, 0, \"Columna no presente\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8b07cf",
   "metadata": {},
   "source": [
    "### ğŸ”¹ R02 â€” Consistencia geogrÃ¡fica municipioâ€“departamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d930d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# R02  CONSISTENCIA GEOGRÃFICA â€” MUNICIPIO â†” DEPARTAMENTO\n",
    "#      Cada municipio debe pertenecer a un Ãºnico departamento.\n",
    "#      MÃºltiples departamentos para el mismo municipio = error en la fuente.\n",
    "#      No hay correcciÃ³n automÃ¡tica: requiere revisiÃ³n de la fuente.\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "COL_M = \"COLE_MCPIO_UBICACION\"\n",
    "COL_D = \"COLE_DEPTO_UBICACION\"\n",
    "\n",
    "if COL_M in df.columns and COL_D in df.columns:\n",
    "    mdc = (\n",
    "        df.dropna(subset=[COL_M, COL_D])\n",
    "        .groupby(COL_M, observed=True)[COL_D]\n",
    "        .nunique()\n",
    "        .rename(\"n_deptos\")\n",
    "        .reset_index()\n",
    "    )\n",
    "    bad = mdc[mdc[\"n_deptos\"] > 1]\n",
    "    n   = len(bad)\n",
    "\n",
    "    print(f\"Municipios Ãºnicos    : {df[COL_M].nunique():,}\")\n",
    "    print(f\"Departamentos Ãºnicos : {df[COL_D].nunique():,}\")\n",
    "    print(f\"Municipios evaluados : {len(mdc):,}\")\n",
    "\n",
    "    if n:\n",
    "        print(f\"\\nâš ï¸  {n} municipio(s) asociados a >1 departamento:\")\n",
    "        display(bad.head(15))\n",
    "    else:\n",
    "        print(\"\\nâœ… Cada municipio pertenece a un Ãºnico departamento.\")\n",
    "\n",
    "    dq.add(VRule(\"R02\", \"Consistencia geogrÃ¡fica municipioâ€“departamento\",\n",
    "                 n == 0, n, len(mdc),\n",
    "                 \"Revisar fuente de datos\" if n else \"Sin acciÃ³n\",\n",
    "                 f\"{df[COL_M].nunique()} municipios / {df[COL_D].nunique()} departamentos\"))\n",
    "else:\n",
    "    print(\"âš ï¸  Columnas geogrÃ¡ficas no disponibles.\")\n",
    "    dq.add(VRule(\"R02\", \"Consistencia geogrÃ¡fica\", True, 0, 0, \"No evaluable\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdbadbf",
   "metadata": {},
   "source": [
    "### ğŸ”¹ R03 â€” Dominio vÃ¡lido: `COLE_CALENDARIO`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febd8d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# R03  DOMINIO VÃLIDO â€” COLE_CALENDARIO\n",
    "#      Valores oficiales MEN: A, B, OTRO\n",
    "#      Variantes comunes normalizadas primero; resto â†’ NaN.\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "VALID_CAL = {\"A\", \"B\", \"OTRO\"}\n",
    "COL = \"COLE_CALENDARIO\"\n",
    "\n",
    "if COL in df.columns:\n",
    "    df[COL] = df[COL].astype(str).str.strip().str.upper()\n",
    "    # Normalizar variantes comunes antes de evaluar\n",
    "    df[COL].replace({\n",
    "        \"CALENDARIO A\": \"A\", \"CALENDARIO_A\": \"A\",\n",
    "        \"CALENDARIO B\": \"B\", \"CALENDARIO_B\": \"B\",\n",
    "    }, inplace=True)\n",
    "\n",
    "    vals = df[COL]\n",
    "    mask = ~vals.isin(VALID_CAL) & ~vals.isin({\"NAN\", \"NONE\", \"<NA>\"})\n",
    "    n    = int(mask.sum())\n",
    "\n",
    "    vc = vals.value_counts(dropna=False).reset_index()\n",
    "    vc.columns = [\"Valor\", \"Conteo\"]\n",
    "    vc[\"En dominio\"] = vc[\"Valor\"].isin(VALID_CAL).map({True: \"âœ…\", False: \"âŒ\"})\n",
    "    display(vc)\n",
    "\n",
    "    if n:\n",
    "        df.loc[mask, COL] = np.nan\n",
    "        print(f\"\\nâš ï¸  {n:,} registros irregulares â†’ NaN\")\n",
    "    else:\n",
    "        print(\"\\nâœ… Todos los valores en el dominio.\")\n",
    "\n",
    "    dq.add(VRule(\"R03\", f\"Dominio {COL}\", n == 0, n, len(df),\n",
    "                 \"â†’ NaN, imputar en Â§6\" if n else \"Sin acciÃ³n\",\n",
    "                 f\"Dominio: {VALID_CAL}\"))\n",
    "else:\n",
    "    dq.add(VRule(\"R03\", f\"Dominio {COL}\", True, 0, 0, \"No disponible\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e967897",
   "metadata": {},
   "source": [
    "### ğŸ”¹ R04 â€” Dominio vÃ¡lido: `COLE_CARACTER`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329d3f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# R04  DOMINIO VÃLIDO â€” COLE_CARACTER\n",
    "#      Valores oficiales: ACADEMICO, TECNICO, TECNICO-ACADEMICO\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "VALID_CAR = {\"ACADEMICO\", \"TECNICO\", \"TECNICO-ACADEMICO\"}\n",
    "COL = \"COLE_CARACTER\"\n",
    "\n",
    "if COL in df.columns:\n",
    "    df[COL] = df[COL].astype(str).str.strip().str.upper()\n",
    "    mask = ~df[COL].isin(VALID_CAR) & ~df[COL].isin({\"NAN\", \"NONE\", \"<NA>\"})\n",
    "    n    = int(mask.sum())\n",
    "\n",
    "    vc = df[COL].value_counts(dropna=False).reset_index()\n",
    "    vc.columns = [\"Valor\", \"Conteo\"]\n",
    "    vc[\"En dominio\"] = vc[\"Valor\"].isin(VALID_CAR).map({True: \"âœ…\", False: \"âŒ\"})\n",
    "    display(vc)\n",
    "\n",
    "    if n:\n",
    "        df.loc[mask, COL] = np.nan\n",
    "        print(f\"\\nâš ï¸  {n:,} registros fuera del dominio â†’ NaN\")\n",
    "    else:\n",
    "        print(\"\\nâœ… Todos los valores en el dominio.\")\n",
    "\n",
    "    dq.add(VRule(\"R04\", f\"Dominio {COL}\", n == 0, n, len(df),\n",
    "                 \"â†’ NaN, imputar en Â§6\" if n else \"Sin acciÃ³n\",\n",
    "                 f\"Dominio: {VALID_CAR}\"))\n",
    "else:\n",
    "    dq.add(VRule(\"R04\", f\"Dominio {COL}\", True, 0, 0, \"No disponible\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf502a8c",
   "metadata": {},
   "source": [
    "### ğŸ”¹ R05 â€” Puntajes targets: valores fuera del rango esperado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9389ffb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# R05  RANGO VÃLIDO â€” PUNTAJES NUMÃ‰RICOS (PUNT_*)\n",
    "#\n",
    "#  NOTA: EDAD no existe aÃºn (se calcula en Â§6 ImputaciÃ³n).\n",
    "#        Esta regla valida los PUNTAJES que sÃ­ existen en df post-Â§4.\n",
    "#\n",
    "#  Rangos esperados (ICFES oficial):\n",
    "#    PUNT_GLOBAL              : 0 â€“ 500\n",
    "#    PUNT_* (por Ã¡rea)        : 0 â€“ 100\n",
    "#\n",
    "#  AcciÃ³n: valores imposibles â†’ NaN (se excluyen del entrenamiento).\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "RANGO = {\n",
    "    \"PUNT_GLOBAL\"              : (0, 500),\n",
    "    \"PUNT_MATEMATICAS\"         : (0, 100),\n",
    "    \"PUNT_INGLES\"              : (0, 100),\n",
    "    \"PUNT_LECTURA_CRITICA\"     : (0, 100),\n",
    "    \"PUNT_C_NATURALES\"         : (0, 100),\n",
    "    \"PUNT_SOCIALES_CIUDADANAS\" : (0, 100),\n",
    "}\n",
    "\n",
    "total_bad = 0\n",
    "registros = {}\n",
    "\n",
    "print(f\"{'Variable':<35}  {'Min obs':>8}  {'Max obs':>8}  {'< mÃ­n':>8}  {'> mÃ¡x':>8}  Estado\")\n",
    "print(\"â”€\" * 85)\n",
    "\n",
    "for col, (lo, hi) in RANGO.items():\n",
    "    if col not in df.columns:\n",
    "        continue\n",
    "    s   = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "    neg = int((s < lo).sum())\n",
    "    ove = int((s > hi).sum())\n",
    "    bad = neg + ove\n",
    "    total_bad += bad\n",
    "\n",
    "    flag = \"âœ…\" if bad == 0 else \"âš ï¸ \"\n",
    "    print(f\"{flag} {col:<33}  {s.min():>8.1f}  {s.max():>8.1f}  {neg:>8,}  {ove:>8,}  \"\n",
    "          + (\"OK\" if bad == 0 else f\"{bad:,} â†’ NaN\"))\n",
    "\n",
    "    if neg: df.loc[s < lo, col] = np.nan\n",
    "    if ove: df.loc[s > hi, col] = np.nan\n",
    "\n",
    "    registros[col] = {\n",
    "        \"min_obs\": float(s.min()),\n",
    "        \"max_obs\": float(s.max()),\n",
    "        \"n_bad\"  : bad,\n",
    "    }\n",
    "\n",
    "# VisualizaciÃ³n de distribuciones\n",
    "cols_plot = [c for c in RANGO if c in df.columns]\n",
    "fig, axes = plt.subplots(1, len(cols_plot), figsize=(4 * len(cols_plot), 4))\n",
    "if len(cols_plot) == 1: axes = [axes]\n",
    "\n",
    "for ax, col in zip(axes, cols_plot):\n",
    "    lo, hi = RANGO[col]\n",
    "    s = pd.to_numeric(df[col], errors=\"coerce\").dropna()\n",
    "    s.hist(bins=40, ax=ax, color=\"#4C72B0\", edgecolor=\"white\")\n",
    "    ax.axvline(lo, color=\"#C44E52\", lw=1.5, ls=\"--\", label=f\"min={lo}\")\n",
    "    ax.axvline(hi, color=\"#C44E52\", lw=1.5, ls=\"--\", label=f\"max={hi}\")\n",
    "    ax.set_title(col.replace(\"PUNT_\", \"\"), fontsize=9)\n",
    "    ax.tick_params(labelsize=7)\n",
    "\n",
    "fig.suptitle(\"R05 â€” DistribuciÃ³n de puntajes (valores originales, post-Â§4)\", fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"r05_puntajes.png\", dpi=120, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "if total_bad == 0:\n",
    "    print(\"\\nâœ… Todos los puntajes estÃ¡n dentro de los rangos esperados.\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸  {total_bad:,} valores imposibles corregidos â†’ NaN\")\n",
    "\n",
    "dq.add(VRule(\"R05\", \"Puntajes targets en rango esperado\",\n",
    "             total_bad == 0, total_bad, len(df),\n",
    "             f\"â†’ NaN: {total_bad:,} valores imposibles\" if total_bad else \"Sin acciÃ³n\",\n",
    "             \"PUNT_GLOBAL [0,500] | Ã¡reas [0,100]\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ff0beb",
   "metadata": {},
   "source": [
    "### ğŸ”¹ R06 â€” Coherencia del catÃ¡logo de educaciÃ³n de padres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac47f16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# R06  COHERENCIA CATÃLOGOS â€” EDUCACIÃ“N MADRE Y PADRE\n",
    "#      Ambas columnas deben usar el mismo vocabulario de categorÃ­as.\n",
    "#      Discrepancia â†’ error de captura / fuente inconsistente.\n",
    "#      AcciÃ³n: solo reporte (no hay correcciÃ³n automÃ¡tica posible).\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "COL_M = \"FAMI_EDUCACIONMADRE\"\n",
    "COL_P = \"FAMI_EDUCACIONPADRE\"\n",
    "\n",
    "if COL_M in df.columns and COL_P in df.columns:\n",
    "    EXCL = {\"nan\", \"none\", \"<na>\", \"\"}\n",
    "\n",
    "    cats_m = {str(v).strip() for v in df[COL_M].dropna().unique()} - EXCL\n",
    "    cats_p = {str(v).strip() for v in df[COL_P].dropna().unique()} - EXCL\n",
    "    solo_m = cats_m - cats_p\n",
    "    solo_p = cats_p - cats_m\n",
    "    n_diff = len(solo_m) + len(solo_p)\n",
    "\n",
    "    print(f\"CategorÃ­as EDUCACIONMADRE : {len(cats_m)}\")\n",
    "    print(f\"CategorÃ­as EDUCACIONPADRE : {len(cats_p)}\")\n",
    "    print(f\"CategorÃ­as comunes        : {len(cats_m & cats_p)}\")\n",
    "\n",
    "    if solo_m: print(f\"\\nâš ï¸  Solo en MADRE ({len(solo_m)}): {sorted(solo_m)}\")\n",
    "    if solo_p: print(f\"âš ï¸  Solo en PADRE ({len(solo_p)}): {sorted(solo_p)}\")\n",
    "    if not n_diff: print(\"\\nâœ… Ambas columnas usan el mismo catÃ¡logo.\")\n",
    "\n",
    "    # GrÃ¡fico comparativo de distribuciÃ³n\n",
    "    fm = df[COL_M].value_counts(normalize=True).rename(\"Madre\")\n",
    "    fp = df[COL_P].value_counts(normalize=True).rename(\"Padre\")\n",
    "    comp = pd.concat([fm, fp], axis=1).fillna(0).sort_index() * 100\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(13, max(5, len(comp) * 0.45)))\n",
    "    comp.plot(kind=\"barh\", ax=ax, color=[\"#4C72B0\", \"#DD8452\"])\n",
    "    ax.set_xlabel(\"Porcentaje (%)\")\n",
    "    ax.set_title(\"R06 â€” EducaciÃ³n Madre vs Padre (valores originales)\", fontsize=11)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"r06_educacion_padres.png\", dpi=120, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    dq.add(VRule(\"R06\", \"Coherencia catÃ¡logos educaciÃ³n padres\",\n",
    "                 n_diff == 0, n_diff, len(cats_m | cats_p),\n",
    "                 \"Revisar fuente\" if n_diff else \"Sin acciÃ³n\",\n",
    "                 f\"{len(cats_m & cats_p)} cats comunes; {n_diff} divergentes\"))\n",
    "else:\n",
    "    dq.add(VRule(\"R06\", \"Coherencia educaciÃ³n padres\", True, 0, 0, \"No disponible\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ff10cb",
   "metadata": {},
   "source": [
    "### ğŸ”¹ R07 â€” Dominio vÃ¡lido: `FAMI_ESTRATOVIVIENDA`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ad47d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# R07  DOMINIO VÃLIDO â€” FAMI_ESTRATOVIVIENDA\n",
    "#      Colombia: Estrato 1-6 + Sin Estrato (zonas rurales)\n",
    "#      Valores distintos â†’ NaN para imputar en Â§6.\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "VALID_EST = {\"Estrato 1\",\"Estrato 2\",\"Estrato 3\",\n",
    "             \"Estrato 4\",\"Estrato 5\",\"Estrato 6\",\"Sin Estrato\"}\n",
    "COL = \"FAMI_ESTRATOVIVIENDA\"\n",
    "\n",
    "if COL in df.columns:\n",
    "    df[COL] = df[COL].astype(str).str.strip()\n",
    "    mask = ~df[COL].isin(VALID_EST) & ~df[COL].isin({\"nan\",\"None\",\"<NA>\",\"\"})\n",
    "    n    = int(mask.sum())\n",
    "\n",
    "    vc = df[COL].value_counts(dropna=False).reset_index()\n",
    "    vc.columns = [\"Valor\",\"Conteo\"]\n",
    "    vc[\"En dominio\"] = vc[\"Valor\"].isin(VALID_EST).map({True:\"âœ…\", False:\"âŒ\"})\n",
    "    display(vc)\n",
    "\n",
    "    if n:\n",
    "        df.loc[mask, COL] = np.nan\n",
    "        print(f\"\\nâš ï¸  {n:,} registros fuera del dominio â†’ NaN\")\n",
    "    else:\n",
    "        print(\"\\nâœ… Todos los valores en el dominio.\")\n",
    "\n",
    "    dq.add(VRule(\"R07\", f\"Dominio {COL}\", n == 0, n, len(df),\n",
    "                 \"â†’ NaN, imputar en Â§6\" if n else \"Sin acciÃ³n\",\n",
    "                 \"Dominio: Estrato 1-6 + Sin Estrato\"))\n",
    "else:\n",
    "    dq.add(VRule(\"R07\", f\"Dominio {COL}\", True, 0, 0, \"No disponible\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b270525",
   "metadata": {},
   "source": [
    "### ğŸ”¹ R08 â€” Variables numÃ©ricas del hogar: valores negativos o imposibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5751b3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# R08  VALORES NEGATIVOS EN VARIABLES NUMÃ‰RICAS DEL HOGAR\n",
    "#\n",
    "#  Variables disponibles en df en este punto:\n",
    "#    FAMI_CUARTOSHOGAR   : nÃºmero de cuartos (entero â‰¥ 1)\n",
    "#    FAMI_PERSONASHOGAR  : personas en el hogar (entero â‰¥ 1)\n",
    "#\n",
    "#  NOTA: INDICE_BIENES y EDAD no existen aÃºn.\n",
    "#        Los puntajes PUNT_* ya fueron validados en R05.\n",
    "#\n",
    "#  Un valor negativo o cero en estas variables es imposible fÃ­sicamente.\n",
    "#  AcciÃ³n: â†’ NaN para que Â§6 (imputaciÃ³n) los corrija con la mediana.\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "HOGAR_COLS = {\n",
    "    \"FAMI_CUARTOSHOGAR\"  : (1, 20),   # cuartos: mÃ­nimo 1, mÃ¡ximo razonable 20\n",
    "    \"FAMI_PERSONASHOGAR\" : (1, 30),   # personas: mÃ­nimo 1, mÃ¡ximo razonable 30\n",
    "}\n",
    "\n",
    "total_bad = 0\n",
    "\n",
    "print(f\"{'Variable':<28}  {'Min obs':>8}  {'Max obs':>8}  {'Fuera de rango':>14}  Estado\")\n",
    "print(\"â”€\" * 72)\n",
    "\n",
    "for col, (lo, hi) in HOGAR_COLS.items():\n",
    "    if col not in df.columns:\n",
    "        print(f\"   {col:<26}  (no disponible en df)\")\n",
    "        continue\n",
    "\n",
    "    s   = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "    bad = int(((s < lo) | (s > hi)).sum())\n",
    "    total_bad += bad\n",
    "\n",
    "    flag = \"âœ…\" if bad == 0 else \"âš ï¸ \"\n",
    "    min_v = s.min() if not s.isna().all() else \"â€”\"\n",
    "    max_v = s.max() if not s.isna().all() else \"â€”\"\n",
    "    print(f\"{flag} {col:<26}  {str(min_v):>8}  {str(max_v):>8}  {bad:>14,}  \"\n",
    "          + (\"OK\" if bad == 0 else f\"â†’ NaN [{lo},{hi}]\"))\n",
    "\n",
    "    if bad:\n",
    "        df.loc[(s < lo) | (s > hi), col] = np.nan\n",
    "\n",
    "if total_bad == 0:\n",
    "    print(\"\\nâœ… Todas las variables del hogar tienen valores en rango.\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸  {total_bad:,} valores imposibles corregidos â†’ NaN\")\n",
    "\n",
    "dq.add(VRule(\"R08\", \"Variables hogar sin valores imposibles\",\n",
    "             total_bad == 0, total_bad, len(df),\n",
    "             f\"â†’ NaN: {total_bad:,} registros fuera de rango\" if total_bad else \"Sin acciÃ³n\",\n",
    "             \"FAMI_CUARTOSHOGAR [1,20] | FAMI_PERSONASHOGAR [1,30]\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea0b85b",
   "metadata": {},
   "source": [
    "### ğŸ”¹ R09 â€” Multicolinealidad categÃ³rica (CramÃ©r's V > 0.80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc23c522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# R09  MULTICOLINEALIDAD CATEGÃ“RICA â€” CRAMÃ‰R'S V > 0.80\n",
    "#\n",
    "#  Se evalÃºan SOLO las variables categÃ³ricas presentes en df en este punto.\n",
    "#  INDICE_BIENES no existe aÃºn (se crea en Â§7), por lo que no interfiere.\n",
    "#  TARGET_COLS y COLUMNS_TO_DROP estÃ¡n definidas en Â§1 y son constantes.\n",
    "#\n",
    "#  Umbral 0.80: pares prÃ¡cticamente redundantes.\n",
    "#  AcciÃ³n: reporte + recomendaciÃ³n de eliminaciÃ³n.\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "def cramers_v(x, y):\n",
    "    ct   = pd.crosstab(x.dropna(), y.dropna())\n",
    "    if ct.shape[0] < 2 or ct.shape[1] < 2:\n",
    "        return 0.0\n",
    "    chi2 = chi2_contingency(ct, correction=False)[0]\n",
    "    n    = ct.values.sum()\n",
    "    d    = n * (min(ct.shape[0] - 1, ct.shape[1] - 1))\n",
    "    return float(np.sqrt(chi2 / d)) if d > 0 else 0.0\n",
    "\n",
    "THRESHOLD = 0.80\n",
    "\n",
    "# Variables categÃ³ricas disponibles en df ahora (post-Â§4, pre-Â§6)\n",
    "excluir = set(TARGET_COLS) | set(COLUMNS_TO_DROP)\n",
    "cat_r09 = [\n",
    "    c for c in df.columns\n",
    "    if df[c].dtype.name in (\"object\", \"category\", \"string\")\n",
    "    and c not in excluir\n",
    "    and df[c].nunique(dropna=True) < 200\n",
    "    and df[c].nunique(dropna=True) > 1\n",
    "]\n",
    "\n",
    "print(f\"Variables categÃ³ricas evaluadas ({len(cat_r09)}): {cat_r09}\")\n",
    "\n",
    "if len(cat_r09) >= 2:\n",
    "    SAMPLE = min(80_000, len(df))\n",
    "    dfs = df[cat_r09].sample(SAMPLE, random_state=42)\n",
    "\n",
    "    cv_mat = pd.DataFrame(index=cat_r09, columns=cat_r09, dtype=float)\n",
    "    for c1 in cat_r09:\n",
    "        for c2 in cat_r09:\n",
    "            cv_mat.loc[c1, c2] = 1.0 if c1 == c2 else cramers_v(dfs[c1], dfs[c2])\n",
    "\n",
    "    # Pares crÃ­ticos\n",
    "    critical = [\n",
    "        {\"Var1\": c1, \"Var2\": c2, \"CramÃ©r V\": round(cv_mat.loc[c1, c2], 3)}\n",
    "        for i, c1 in enumerate(cat_r09)\n",
    "        for c2 in cat_r09[i+1:]\n",
    "        if cv_mat.loc[c1, c2] > THRESHOLD\n",
    "    ]\n",
    "\n",
    "    # Heatmap\n",
    "    fig, ax = plt.subplots(figsize=(max(8, len(cat_r09)), max(6, len(cat_r09) - 1)))\n",
    "    sns.heatmap(cv_mat.astype(float), ax=ax, cmap=\"YlOrRd\",\n",
    "                annot=True, fmt=\".2f\", vmin=0, vmax=1, linewidths=0.3,\n",
    "                cbar_kws={\"label\": \"CramÃ©r's V\", \"shrink\": 0.7})\n",
    "    ax.set_title(f\"R09 â€” CramÃ©r's V entre variables categÃ³ricas originales\\n\"\n",
    "                 f\"(umbral crÃ­tico = {THRESHOLD})\", fontsize=11)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"r09_cramers_v.png\", dpi=120, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    n_crit = len(critical)\n",
    "    if n_crit:\n",
    "        print(f\"\\nâš ï¸  {n_crit} par(es) con CramÃ©r's V > {THRESHOLD}:\")\n",
    "        display(pd.DataFrame(critical).sort_values(\"CramÃ©r V\", ascending=False))\n",
    "        print(\"   AcciÃ³n recomendada: eliminar la variable con menor correlaciÃ³n con PUNT_GLOBAL\")\n",
    "    else:\n",
    "        print(f\"\\nâœ… Sin pares con CramÃ©r's V > {THRESHOLD}\")\n",
    "\n",
    "    dq.add(VRule(\"R09\", f\"Sin multicolinealidad crÃ­tica (V > {THRESHOLD})\",\n",
    "                 n_crit == 0, n_crit,\n",
    "                 len(cat_r09) * (len(cat_r09) - 1) // 2,\n",
    "                 \"Revisar y eliminar redundante\" if n_crit else \"Sin acciÃ³n\",\n",
    "                 f\"{len(cat_r09)} variables evaluadas\"))\n",
    "else:\n",
    "    print(\"âš ï¸  No hay suficientes variables categÃ³ricas para evaluar.\")\n",
    "    dq.add(VRule(\"R09\", \"Multicolinealidad crÃ­tica\", True, 0, 0, \"No evaluable\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efa1f0a",
   "metadata": {},
   "source": [
    "### ğŸ”¹ R10 â€” Variables casi constantes (> 95 % un mismo valor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103ba38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# R10  VARIABLES QUASI-CONSTANTES (> 95% un solo valor)\n",
    "#\n",
    "#  Una variable donde el 95%+ de los registros tienen el mismo valor\n",
    "#  aporta informaciÃ³n mÃ­nima al modelo.\n",
    "#\n",
    "#  Se evalÃºan TODAS las columnas disponibles en df en este punto,\n",
    "#  excluyendo targets y columnas administrativas ya descartadas en Â§4.\n",
    "#\n",
    "#  NOTA: EDAD, ANIO, TRIMESTRE, INDICE_BIENES no existen aÃºn â†’ no se evalÃºan.\n",
    "#  AcciÃ³n: reporte. Si quasi-constante, considerar eliminar en Â§7.\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "THRESH = 0.95\n",
    "\n",
    "# Solo excluir lo que estÃ¡ definido y lo que ya no estÃ¡ en df\n",
    "excluir_r10 = set(TARGET_COLS) | set(COLUMNS_TO_DROP)\n",
    "\n",
    "# Columnas presentes en df que no son targets ni administrativas descartadas\n",
    "input_cols = [c for c in df.columns if c not in excluir_r10]\n",
    "\n",
    "resultados_r10 = []\n",
    "quasi_const    = []\n",
    "\n",
    "for col in input_cols:\n",
    "    try:\n",
    "        vc       = df[col].value_counts(normalize=True, dropna=False)\n",
    "        if len(vc) == 0:\n",
    "            continue\n",
    "        top_val   = vc.index[0]\n",
    "        top_ratio = float(vc.iloc[0])\n",
    "\n",
    "        resultados_r10.append({\n",
    "            \"Variable\"        : col,\n",
    "            \"Valor dominante\" : str(top_val)[:40],\n",
    "            \"% dominante\"     : round(top_ratio * 100, 2),\n",
    "            \"# Ãšnicos\"        : int(df[col].nunique()),\n",
    "            \"Estado\"          : \"âš ï¸  QUASI-CSTE\" if top_ratio > THRESH else \"âœ… OK\",\n",
    "        })\n",
    "        if top_ratio > THRESH:\n",
    "            quasi_const.append(col)\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "r10_df = (pd.DataFrame(resultados_r10)\n",
    "          .sort_values(\"% dominante\", ascending=False)\n",
    "          .reset_index(drop=True))\n",
    "\n",
    "print(f\"Variables evaluadas: {len(r10_df)}\")\n",
    "\n",
    "if quasi_const:\n",
    "    print(f\"\\nâš ï¸  Variables quasi-constantes ({len(quasi_const)}):\")\n",
    "    display(r10_df[r10_df[\"Estado\"] == \"âš ï¸  QUASI-CSTE\"]\n",
    "            [[\"Variable\", \"Valor dominante\", \"% dominante\", \"# Ãšnicos\"]])\n",
    "\n",
    "# VisualizaciÃ³n top 20\n",
    "fig, ax = plt.subplots(figsize=(12, max(5, min(len(r10_df), 20) * 0.38)))\n",
    "top20  = r10_df.head(20)\n",
    "colors = [\"#C44E52\" if s == \"âš ï¸  QUASI-CSTE\" else \"#4C72B0\" for s in top20[\"Estado\"]]\n",
    "ax.barh(top20[\"Variable\"], top20[\"% dominante\"],\n",
    "        color=colors, edgecolor=\"white\", height=0.65)\n",
    "ax.axvline(THRESH * 100, color=\"black\", lw=1.5, ls=\"--\",\n",
    "           label=f\"Umbral {THRESH*100:.0f}%\")\n",
    "ax.set_xlabel(\"% del valor dominante\")\n",
    "ax.set_title(\"R10 â€” Quasi-constancia de variables (top 20, valores originales)\", fontsize=11)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"r10_quasi_constantes.png\", dpi=120, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "if not quasi_const:\n",
    "    print(f\"\\nâœ… Ninguna variable supera el umbral del {THRESH*100:.0f}%.\")\n",
    "\n",
    "dq.add(VRule(\"R10\", f\"Sin variables quasi-constantes (>{THRESH*100:.0f}%)\",\n",
    "             len(quasi_const) == 0, len(quasi_const), len(input_cols),\n",
    "             f\"Revisar en Â§7: {quasi_const}\" if quasi_const else \"Sin acciÃ³n\",\n",
    "             f\"{len(input_cols)} variables evaluadas\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d5af9a",
   "metadata": {},
   "source": [
    "### ğŸ“‹ Reporte consolidado de validaciÃ³n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fb87fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 5.11  REPORTE CONSOLIDADO DE VALIDACIÃ“N\n",
    "#       Aplicado sobre df POST-Â§4 (limpieza) y PRE-Â§6 (imputaciÃ³n).\n",
    "#       Las correcciones â†’ NaN se propagan automÃ¡ticamente al pipeline.\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "dq.print_summary()\n",
    "print()\n",
    "display(dq.summary())\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "summary_df = dq.summary()\n",
    "colors_bar = [\"#55A868\" if \"PASS\" in s else \"#C44E52\" for s in summary_df[\"Estado\"]]\n",
    "\n",
    "# Panel izq: estado por regla\n",
    "labels = summary_df[\"ID\"] + \"  \" + summary_df[\"Regla\"].str[:30]\n",
    "axes[0].barh(labels, [1] * len(summary_df),\n",
    "             color=colors_bar, edgecolor=\"white\")\n",
    "axes[0].set_xlim(0, 1.35)\n",
    "axes[0].set_title(\"Estado por regla de validaciÃ³n\", fontsize=11, fontweight=\"bold\")\n",
    "axes[0].tick_params(axis=\"y\", labelsize=8.5)\n",
    "for i, (s, label) in enumerate(zip(summary_df[\"Estado\"], labels)):\n",
    "    axes[0].text(1.02, i, s, va=\"center\", fontsize=9)\n",
    "axes[0].set_xticks([])\n",
    "\n",
    "green_p = mpatches.Patch(color=\"#55A868\", label=\"âœ… PASS\")\n",
    "red_p   = mpatches.Patch(color=\"#C44E52\", label=\"âš ï¸  FAIL\")\n",
    "axes[0].legend(handles=[green_p, red_p], loc=\"lower right\", fontsize=9)\n",
    "\n",
    "# Panel der: % afectados (solo reglas con afectados > 0)\n",
    "aff = summary_df[summary_df[\"% Afect.\"] > 0].copy()\n",
    "if not aff.empty:\n",
    "    axes[1].bar(aff[\"ID\"], aff[\"% Afect.\"],\n",
    "                color=\"#DD8452\", edgecolor=\"white\")\n",
    "    for i, row in aff.iterrows():\n",
    "        axes[1].text(aff.index.get_loc(i), row[\"% Afect.\"] + 0.01,\n",
    "                     f\"{row['% Afect.']:.2f}%\", ha=\"center\", fontsize=9)\n",
    "    axes[1].set_xlabel(\"Regla\")\n",
    "    axes[1].set_ylabel(\"% registros afectados\")\n",
    "    axes[1].set_title(\"Impacto por regla (% sobre df)\", fontsize=11, fontweight=\"bold\")\n",
    "else:\n",
    "    axes[1].text(0.5, 0.5, \"Sin registros afectados\\nâœ… Datos limpios\",\n",
    "                 ha=\"center\", va=\"center\", fontsize=14, transform=axes[1].transAxes,\n",
    "                 color=\"#55A868\")\n",
    "    axes[1].set_title(\"Impacto por regla\", fontsize=11, fontweight=\"bold\")\n",
    "\n",
    "fig.suptitle(\n",
    "    \"Dashboard de Calidad de Datos\\n\"\n",
    "    \"Evaluado POST-Â§4 (limpieza) | PRE-Â§6 (imputaciÃ³n) | Datos en unidades originales\",\n",
    "    fontsize=12, fontweight=\"bold\", y=1.02\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"dq_dashboard.png\", dpi=120, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nâœ… ValidaciÃ³n Â§5 completada. df listo para Â§6 (ImputaciÃ³n).\")\n",
    "print(f\"   Shape actual de df: {df.shape}\")\n",
    "total_nan_new = df.isnull().sum().sum()\n",
    "print(f\"   NaN totales en df: {total_nan_new:,}  (serÃ¡n imputados en Â§6)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2429efa8",
   "metadata": {},
   "source": [
    "## ğŸ©¹ 5. ImputaciÃ³n de Valores Faltantes\n",
    "\n",
    "Estrategias aplicadas segÃºn el tipo de variable:\n",
    "\n",
    "| Tipo de variable | Estrategia | JustificaciÃ³n |\n",
    "|---|---|---|\n",
    "| NumÃ©rica continua | **Mediana** | Robusta ante outliers |\n",
    "| CategÃ³rica nominal | **Moda global** | Valor mÃ¡s frecuente |\n",
    "| EducaciÃ³n madre/padre | **Moda por estrato** | Aprovecha correlaciÃ³n socioeconÃ³mica |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4225ec0",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# 5.1  INGENIERIA TEMPORAL - EDAD, ANIO Y TRIMESTRE\n#\n# ANIO: se calcula para extraer EDAD pero NO entra al modelo como variable\n#       predictora. Razon: queremos predecir resultados de futuros estudiantes\n#       que no conocen su anio de presentacion. Se imputa con la mediana del\n#       dataset completo en la seccion de imputacion.\n#\n# TRIMESTRE: SI entra al modelo. Captura estacionalidad del examen\n#            (primer semestre vs segundo semestre tienen patrones diferentes).\n# =============================================================================\n\ndf[\"PERIODO\"]   = df[\"PERIODO\"].astype(str)\ndf[\"ANIO\"]      = df[\"PERIODO\"].str[:4].astype(\"Int16\")\ndf[\"TRIMESTRE\"] = df[\"PERIODO\"].str[4:].astype(\"Int8\")\n\n# Fecha de referencia: 30 de junio del anio del examen\ndf[\"FECHA_REF\"] = pd.to_datetime(\n    df[\"ANIO\"].astype(\"Int64\").astype(\"string\") + \"-06-30\", errors=\"coerce\"\n)\n\n# Edad en anios\ndf[\"EDAD\"] = (df[\"FECHA_REF\"] - df[\"ESTU_FECHANACIMIENTO\"]).dt.days / 365.25\n\n# Filtrar edades imposibles para bachilleres\nmask_bad = (df[\"EDAD\"] < 12) | (df[\"EDAD\"] > 60)\ndf.loc[mask_bad, \"EDAD\"] = np.nan\nprint(f\"Edades imposibles -> NaN: {mask_bad.sum():,}\")\nprint(df[\"EDAD\"].describe().round(2).to_string())\nprint(f\"\\nTRIMESTRE distribucion:\")\nprint(df[\"TRIMESTRE\"].value_counts(dropna=False).sort_index().to_string())\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4444b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 5.2  DIAGNÃ“STICO DE NULOS ANTES DE IMPUTAR\n",
    "# =============================================================================\n",
    "\n",
    "vars_impute = [v for v in INDEPENDENT_VARS if v in df.columns]\n",
    "null_before = df[vars_impute].isnull().sum()\n",
    "\n",
    "print(\"â”€â”€ Nulos por variable independiente â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "display(pd.DataFrame({\n",
    "    \"Nulos\"  : null_before,\n",
    "    \"% Nulos\": (df[vars_impute].isnull().mean() * 100).round(2),\n",
    "    \"Tipo\"   : df[vars_impute].dtypes,\n",
    "}).sort_values(\"Nulos\", ascending=False)[lambda x: x[\"Nulos\"] > 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c537b72",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# 5.3  IMPUTACION NUMERICA (MEDIANA)\n# REGLA: Nunca eliminar filas por una variable vacia.\n# Se imputa SIEMPRE: mediana por grupo o mediana global como fallback.\n# FAMI_PERSONASHOGAR: mediana por municipio para mayor precision.\n# =============================================================================\n\nnum_vars = [\n    v for v in vars_impute\n    if pd.api.types.is_numeric_dtype(df[v]) and df[v].isnull().any()\n]\nif \"EDAD\" in df.columns and df[\"EDAD\"].isnull().any() and \"EDAD\" not in num_vars:\n    num_vars.append(\"EDAD\")\n\nfor col in num_vars:\n    n_null_antes = df[col].isnull().sum()\n\n    if col == \"FAMI_PERSONASHOGAR\" and \"COLE_MCPIO_UBICACION\" in df.columns:\n        # Imputacion por municipio para mayor precision demografica\n        med_mcpio = df.groupby(\"COLE_MCPIO_UBICACION\")[col].transform(\"median\")\n        df[col] = df[col].fillna(med_mcpio)\n        # Fallback global si aun quedan nulos\n        df[col] = df[col].fillna(df[col].median())\n        print(f\"  [{col:<30}] imputado por municipio + fallback global  (nulos: {n_null_antes} -> {df[col].isnull().sum()})\")\n    elif col == \"FAMI_CUARTOSHOGAR\" and \"FAMI_ESTRATOVIVIENDA\" in df.columns:\n        # Imputacion por estrato\n        med_estrato = df.groupby(\"FAMI_ESTRATOVIVIENDA\")[col].transform(\"median\")\n        df[col] = df[col].fillna(med_estrato)\n        df[col] = df[col].fillna(df[col].median())\n        print(f\"  [{col:<30}] imputado por estrato + fallback global  (nulos: {n_null_antes} -> {df[col].isnull().sum()})\")\n    else:\n        med = df[col].median()\n        df[col] = df[col].fillna(med)\n        print(f\"  [{col:<30}] mediana global = {med:.2f}  (nulos: {n_null_antes} -> {df[col].isnull().sum()})\")\n\nprint(\"\\nImputacion numerica completada.\")\nprint(f\"Filas conservadas: {len(df):,}  (sin eliminacion por nulos)\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ce5f37",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# 5.4  IMPUTACION CATEGORICA (MODA / MODA GRUPAL)\n# REGLA: Nunca eliminar filas. Se imputa con moda grupal + fallback global.\n# COLE_CARACTER: moda por area de ubicacion (urbano/rural).\n# FAMI_EDUCACION*: moda por estrato de vivienda.\n# =============================================================================\n\ndef impute_categorical(df, col, group_col=None):\n    \"\"\"\n    Imputa categorica con moda grupal (si group_col existe)\n    y moda global como fallback. Nunca elimina filas.\n    \"\"\"\n    n_null = df[col].isnull().sum()\n    if n_null == 0:\n        return df\n\n    if group_col and group_col in df.columns:\n        group_mode = (\n            df.groupby(group_col, observed=True)[col]\n            .agg(lambda x: x.dropna().mode().iloc[0]\n                 if not x.dropna().mode().empty else np.nan)\n        )\n        mask = df[col].isnull()\n        df.loc[mask, col] = df.loc[mask, group_col].map(group_mode)\n\n    # Fallback: moda global\n    global_mode = df[col].dropna().mode()\n    if not global_mode.empty:\n        df[col] = df[col].fillna(global_mode.iloc[0])\n\n    print(f\"  [{col:<30}] nulos: {n_null} -> {df[col].isnull().sum()} | grupo={group_col or 'ninguno'}\")\n    return df\n\n\ncat_vars = [\n    v for v in vars_impute\n    if df[v].dtype.name in [\"category\", \"object\", \"string\"] and df[v].isnull().any()\n]\n\nfor col in cat_vars:\n    if col == \"COLE_CARACTER\":\n        group = \"COLE_AREA_UBICACION\"   # moda por area (urbano/rural)\n    elif \"EDUCACION\" in col.upper():\n        group = \"FAMI_ESTRATOVIVIENDA\"  # moda por estrato\n    else:\n        group = None\n    df = impute_categorical(df, col, group_col=group)\n\nprint(\"\\nImputacion categorica completada.\")\nprint(f\"Filas conservadas: {len(df):,}  (sin eliminacion por nulos)\")\n\n# Verificacion COLE_CARACTER\nif \"COLE_CARACTER\" in df.columns:\n    print(\"\\n  COLE_CARACTER distribucion:\")\n    print(df[\"COLE_CARACTER\"].value_counts(dropna=False).to_string())\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e47c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 5.5  VERIFICACIÃ“N POST-IMPUTACIÃ“N\n",
    "# =============================================================================\n",
    "null_after = df[vars_impute].isnull().sum()\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    \"Antes\"     : null_before,\n",
    "    \"DespuÃ©s\"   : null_after,\n",
    "    \"ReducciÃ³n\" : null_before - null_after,\n",
    "}).sort_values(\"ReducciÃ³n\", ascending=False)\n",
    "\n",
    "display(comparison[comparison[\"Antes\"] > 0])\n",
    "\n",
    "remaining = null_after[null_after > 0]\n",
    "print(\"\\nâœ… Sin nulos residuales en variables independientes.\" if remaining.empty\n",
    "      else f\"âš ï¸  Nulos residuales: {remaining.to_dict()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5206ede8",
   "metadata": {},
   "source": [
    "## ğŸ”— 6. AnÃ¡lisis de CorrelaciÃ³n y Multicolinealidad\n",
    "\n",
    "| MÃ©trica | Variables | InterpretaciÃ³n |\n",
    "|---|---|---|\n",
    "| **Pearson r** | NumÃ©ricas â†” NumÃ©ricas | r âˆˆ [-1, 1]; |r| > 0.7 = alta correlaciÃ³n |\n",
    "| **CramÃ©r's V** | CategÃ³ricas â†” CategÃ³ricas | V âˆˆ [0, 1]; V > 0.7 = posible redundancia |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771bbff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 6.1  CORRELACIÃ“N DE PEARSON â€“ VARIABLES NUMÃ‰RICAS\n",
    "# Se incluye PUNT_GLOBAL para identificar las variables mÃ¡s predictivas.\n",
    "# =============================================================================\n",
    "\n",
    "num_all = (\n",
    "    df[vars_impute + [\"PUNT_GLOBAL\"]]\n",
    "    .select_dtypes(include=\"number\")\n",
    "    .columns.tolist()\n",
    ")\n",
    "# Eliminar duplicados preservando orden\n",
    "seen = set()\n",
    "num_unique = [c for c in num_all if not (c in seen or seen.add(c))]\n",
    "\n",
    "corr_matrix = df[num_unique].corr(method=\"pearson\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    corr_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\",\n",
    "    center=0, vmin=-1, vmax=1, linewidths=0.4, ax=ax,\n",
    "    cbar_kws={\"label\": \"Pearson r\", \"shrink\": 0.8},\n",
    ")\n",
    "ax.set_title(\"CorrelaciÃ³n de Pearson\\n(Variables NumÃ©ricas + PUNT_GLOBAL)\", fontsize=13, pad=15)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"pearson_correlation.png\", dpi=130, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâ”€â”€ CorrelaciÃ³n de cada variable con PUNT_GLOBAL â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "print(\n",
    "    corr_matrix[\"PUNT_GLOBAL\"]\n",
    "    .drop(\"PUNT_GLOBAL\")\n",
    "    .sort_values(key=abs, ascending=False)\n",
    "    .round(4)\n",
    "    .to_string()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3745242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 6.2  CRAMÃ‰R'S V â€“ VARIABLES CATEGÃ“RICAS\n",
    "# Mide la asociaciÃ³n estadÃ­stica entre pares de variables nominales/ordinales.\n",
    "# Se trabaja con una muestra para eficiencia computacional.\n",
    "# =============================================================================\n",
    "\n",
    "def cramers_v(x: pd.Series, y: pd.Series) -> float:\n",
    "    \"\"\"\n",
    "    Calcula CramÃ©r's V entre dos Series categÃ³ricas.\n",
    "    Retorna un valor entre 0 (sin asociaciÃ³n) y 1 (asociaciÃ³n perfecta).\n",
    "    \"\"\"\n",
    "    ct   = pd.crosstab(x, y)\n",
    "    chi2 = chi2_contingency(ct, correction=False)[0]\n",
    "    n    = ct.values.sum()\n",
    "    r, k = ct.shape\n",
    "    denom = n * (min(k - 1, r - 1))\n",
    "    return float(np.sqrt(chi2 / denom)) if denom > 0 else 0.0\n",
    "\n",
    "\n",
    "cat_ind = [\n",
    "    v for v in vars_impute\n",
    "    if df[v].dtype.name == \"category\"\n",
    "]\n",
    "\n",
    "SAMPLE_SIZE = 150_000\n",
    "df_sample = df[cat_ind].dropna().sample(min(SAMPLE_SIZE, len(df)), random_state=42)\n",
    "print(f\"Calculando CramÃ©r's V en muestra de {len(df_sample):,} filas Ã— {len(cat_ind)} variables...\")\n",
    "\n",
    "# Matriz de asociaciÃ³n\n",
    "cv_df = pd.DataFrame(index=cat_ind, columns=cat_ind, dtype=float)\n",
    "for c1 in cat_ind:\n",
    "    for c2 in cat_ind:\n",
    "        cv_df.loc[c1, c2] = 1.0 if c1 == c2 else cramers_v(df_sample[c1], df_sample[c2])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "sns.heatmap(\n",
    "    cv_df.astype(float), annot=True, fmt=\".2f\", cmap=\"Reds\",\n",
    "    vmin=0, vmax=1, linewidths=0.4, ax=ax,\n",
    "    cbar_kws={\"label\": \"CramÃ©r's V\", \"shrink\": 0.8},\n",
    ")\n",
    "ax.set_title(\"Multicolinealidad â€“ CramÃ©r's V\\n(Variables CategÃ³ricas Independientes)\", fontsize=13, pad=15)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"cramers_v_matrix.png\", dpi=130, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Reporte de pares problemÃ¡ticos\n",
    "print(\"\\nğŸ” Pares con CramÃ©r's V > 0.70 (posible redundancia):\")\n",
    "pairs = [\n",
    "    {\"Variable 1\": c1, \"Variable 2\": c2, \"CramÃ©r's V\": round(cv_df.loc[c1, c2], 3)}\n",
    "    for i, c1 in enumerate(cat_ind)\n",
    "    for c2 in cat_ind[i+1:]\n",
    "    if cv_df.loc[c1, c2] > 0.70\n",
    "]\n",
    "if pairs:\n",
    "    display(pd.DataFrame(pairs).sort_values(\"CramÃ©r's V\", ascending=False))\n",
    "else:\n",
    "    print(\"  No se encontraron pares con V > 0.70 âœ…\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6aafade",
   "metadata": {},
   "source": [
    "## âš™ï¸ 7. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1df1e3",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# 7.1  INDICE COMPUESTO DE BIENES DEL HOGAR (0-4)\n# Suma binaria de los 4 bienes. Se verifica que la suma sea correcta.\n# REGLA: si la columna no existe o tiene NaN, se cuenta como 0 (no como faltante).\n# =============================================================================\n\navailable_assets = [c for c in ASSET_COLS if c in df.columns]\nprint(f\"Columnas de bienes disponibles: {available_assets}\")\n\nfor col in available_assets:\n    # Normalizar a mayusculas sin espacios antes de mapear\n    serie_norm = (\n        df[col].astype(\"string\").str.strip().str.upper()\n        .replace({\"<NA>\": np.nan, \"NONE\": np.nan, \"NAN\": np.nan})\n    )\n    df[col + \"_BIN\"] = serie_norm.map({\"SI\": 1, \"NO\": 0}).fillna(0).astype(\"Int8\")\n    n_si  = (df[col + \"_BIN\"] == 1).sum()\n    n_no  = (df[col + \"_BIN\"] == 0).sum()\n    print(f\"  {col:<30} SI={n_si:,}  NO={n_no:,}  nulos_tratados_como_0={serie_norm.isna().sum():,}\")\n\nbin_cols = [c + \"_BIN\" for c in available_assets]\ndf[\"INDICE_BIENES\"] = df[bin_cols].sum(axis=1, min_count=0).astype(float)\n\nprint(\"\\nINDICE_BIENES (0=ningun bien, 4=todos los bienes):\")\nprint(df[\"INDICE_BIENES\"].value_counts(dropna=False).sort_index().to_string())\nprint(f\"  Media  : {df['INDICE_BIENES'].mean():.2f}\")\nprint(f\"  Nulos  : {df['INDICE_BIENES'].isnull().sum()}\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cfe776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 7.2  WINSORIZACIÃ“N DE OUTLIERS EN PUNTAJES\n",
    "# Se recortan valores extremos al percentil [0.5%, 99.5%] para evitar que\n",
    "# errores de digitaciÃ³n afecten el entrenamiento de los modelos.\n",
    "# =============================================================================\n",
    "\n",
    "def winsorize(series: pd.Series, low: float = 0.005, high: float = 0.995) -> pd.Series:\n",
    "    \"\"\"Recorta los valores de una serie a los cuantiles [low, high].\"\"\"\n",
    "    lo, hi = series.quantile(low), series.quantile(high)\n",
    "    return series.clip(lower=lo, upper=hi)\n",
    "\n",
    "for col in SCORE_COLS:\n",
    "    if col in df.columns:\n",
    "        df.loc[df[col] < 0, col] = np.nan      # puntajes negativos son imposibles\n",
    "        df[col] = winsorize(df[col])\n",
    "\n",
    "print(\"âœ… WinsorizaciÃ³n aplicada a puntajes (0.5%â€“99.5%):\")\n",
    "display(df[SCORE_COLS].describe().round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d48c2c",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# 7.3  ELIMINACION DE VARIABLES REDUNDANTES\n#\n# ANIO: se elimina como variable predictora. Razon: los futuros estudiantes\n#       no conocen su anio de presentacion. Solo se uso para calcular EDAD.\n# TRIMESTRE: se conserva. Captura estacionalidad del examen.\n# =============================================================================\n\nCOLS_TO_REMOVE = (\n    ASSET_COLS                                      # reemplazadas por INDICE_BIENES\n    + bin_cols                                       # columnas _BIN auxiliares\n    + [\"COLE_DEPTO_UBICACION\",                       # ya se uso para filtrar Quindio\n       \"ESTU_PRIVADO_LIBERTAD\",                      # valor casi constante\n       \"FECHA_REF\", \"ESTU_FECHANACIMIENTO\",          # ya se extrajeron EDAD/TRIMESTRE\n       \"ANIO\",                                       # no predictora para futuros estudiantes\n       \"PERIODO\",]                                   # reemplazado por TRIMESTRE\n)\n\nremoved_cols = [c for c in COLS_TO_REMOVE if c in df.columns]\ndf.drop(columns=removed_cols, inplace=True)\n\nprint(f\"{len(removed_cols)} columnas eliminadas: {removed_cols}\")\nprint(f\"   Dimensiones actuales: {df.shape}\")\nprint(f\"   TRIMESTRE en df: {'TRIMESTRE' in df.columns}\")\nprint(f\"   ANIO en df     : {'ANIO' in df.columns}\")\n"
  },
  {
   "cell_type": "markdown",
   "id": "7525e629",
   "metadata": {},
   "source": [
    "### ğŸ“ 7.4 â€” Variables contextuales agregadas (nivel municipio y colegio)\n",
    "\n",
    "Se calculan **promedios de PUNT_GLOBAL** agregados por municipio y por colegio.  \n",
    "Estas variables capturan el **efecto institucional y geografico** que va mas alla  \n",
    "de las caracteristicas individuales del estudiante.\n",
    "\n",
    "> Se calculan aqui (pre-encoding) porque necesitan `COLE_CODIGO` y `PUNT_GLOBAL`\n",
    "> que son eliminados o no estan disponibles despues de Â§8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56429d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 7.4  VARIABLES CONTEXTUALES AGREGADAS\n",
    "# Se usa transform('mean') para preservar el indice y no perder filas.\n",
    "# Deben calcularse ANTES del encoding porque necesitan COLE_CODIGO y PUNT_GLOBAL.\n",
    "#\n",
    "# PROM_GLOBAL_MCPIO  : promedio de PUNT_GLOBAL por municipio del colegio\n",
    "# PROM_GLOBAL_COLEGIO: promedio de PUNT_GLOBAL por colegio (COLE_CODIGO)\n",
    "#\n",
    "# NOTA DE LEAKAGE: estos promedios incluyen al propio estudiante.\n",
    "# Esto es aceptable para un modelo descriptivo/exploratorio. Si se requiere\n",
    "# un pipeline de produccion estricto, calcular solo sobre el train set.\n",
    "# =============================================================================\n",
    "\n",
    "NUEVAS_CONTEXTUALES = []\n",
    "\n",
    "# --- Promedio por municipio del colegio ---\n",
    "if 'COLE_MCPIO_UBICACION' in df.columns and 'PUNT_GLOBAL' in df.columns:\n",
    "    df['PROM_GLOBAL_MCPIO'] = (\n",
    "        df.groupby('COLE_MCPIO_UBICACION')['PUNT_GLOBAL']\n",
    "        .transform('mean')\n",
    "    )\n",
    "    NUEVAS_CONTEXTUALES.append('PROM_GLOBAL_MCPIO')\n",
    "    print(f'  PROM_GLOBAL_MCPIO   : {df[\"PROM_GLOBAL_MCPIO\"].notna().sum():,} valores '\n",
    "          f'| media={df[\"PROM_GLOBAL_MCPIO\"].mean():.2f} '\n",
    "          f'| grupos={df[\"COLE_MCPIO_UBICACION\"].nunique()}')\n",
    "else:\n",
    "    print('  AVISO: COLE_MCPIO_UBICACION o PUNT_GLOBAL no disponibles -> omitida')\n",
    "\n",
    "# --- Promedio por colegio ---\n",
    "if 'COLE_CODIGO' in df.columns and 'PUNT_GLOBAL' in df.columns:\n",
    "    df['PROM_GLOBAL_COLEGIO'] = (\n",
    "        df.groupby('COLE_CODIGO')['PUNT_GLOBAL']\n",
    "        .transform('mean')\n",
    "    )\n",
    "    NUEVAS_CONTEXTUALES.append('PROM_GLOBAL_COLEGIO')\n",
    "    print(f'  PROM_GLOBAL_COLEGIO : {df[\"PROM_GLOBAL_COLEGIO\"].notna().sum():,} valores '\n",
    "          f'| media={df[\"PROM_GLOBAL_COLEGIO\"].mean():.2f} '\n",
    "          f'| grupos={df[\"COLE_CODIGO\"].nunique()}')\n",
    "else:\n",
    "    print('  AVISO: COLE_CODIGO o PUNT_GLOBAL no disponibles -> omitida')\n",
    "\n",
    "# Registrar en INDEPENDENT_VARS para que pasen por encoding y scaling\n",
    "for v in NUEVAS_CONTEXTUALES:\n",
    "    if v not in INDEPENDENT_VARS:\n",
    "        INDEPENDENT_VARS.append(v)\n",
    "\n",
    "print(f'\\nVariables contextuales creadas: {NUEVAS_CONTEXTUALES}')\n",
    "print(f'INDEPENDENT_VARS actualizado: {len(INDEPENDENT_VARS)} variables')\n",
    "\n",
    "# Distribucion rapida\n",
    "if NUEVAS_CONTEXTUALES:\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig, axes = plt.subplots(1, len(NUEVAS_CONTEXTUALES),\n",
    "                             figsize=(7*len(NUEVAS_CONTEXTUALES), 4))\n",
    "    if len(NUEVAS_CONTEXTUALES) == 1:\n",
    "        axes = [axes]\n",
    "    for ax, col in zip(axes, NUEVAS_CONTEXTUALES):\n",
    "        df[col].dropna().plot.hist(ax=ax, bins=40, color='#4f8ef7', edgecolor='white')\n",
    "        ax.set_title(col, fontsize=10, fontweight='bold')\n",
    "        ax.set_xlabel('Promedio PUNT_GLOBAL', fontsize=9)\n",
    "    plt.suptitle('7.4 Variables Contextuales - Distribucion',\n",
    "                 fontsize=11, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c403f28",
   "metadata": {},
   "source": [
    "## ğŸ·ï¸ 8. CodificaciÃ³n de Variables CategÃ³ricas\n",
    "\n",
    "Los algoritmos de ML requieren entradas numÃ©ricas. Se aplican dos estrategias:\n",
    "\n",
    "| Estrategia | CuÃ¡ndo | Ejemplo |\n",
    "|---|---|---|\n",
    "| **OrdinalEncoder** | Variable con orden natural claro | Nivel educativo, estrato |\n",
    "| **LabelEncoder** | Variable nominal (sin orden) | GÃ©nero, calendario, jornada |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3c6a89",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# 8.1  PREPARACION DEL DATASET FINAL\n# REGLA: Solo se elimina una fila si TODAS las variables de entrada son nulas.\n# No se elimina por tener una sola variable vacia (ya fue imputada).\n# =============================================================================\n\nINDEPENDENT_VARS_FINAL = [v for v in INDEPENDENT_VARS if v in df.columns]\nTARGETS_FINAL          = [t for t in TARGET_COLS       if t in df.columns]\n\ndf_model = df[INDEPENDENT_VARS_FINAL + TARGETS_FINAL].copy()\n\n# Solo eliminar filas donde TODAS las variables X son nulas simultaneamente\nmask_all_null = df_model[INDEPENDENT_VARS_FINAL].isnull().all(axis=1)\nn_drop = mask_all_null.sum()\nif n_drop > 0:\n    df_model = df_model[~mask_all_null].copy()\n    print(f\"  Filas eliminadas (todas las X nulas): {n_drop:,}\")\nelse:\n    print(\"  Sin filas eliminadas (ninguna tiene todas las X nulas)\")\n\nprint(f\"Dataset de modelado: {df_model.shape[0]:,} filas x {df_model.shape[1]} cols\")\nprint(f\"   Variables X : {len(INDEPENDENT_VARS_FINAL)}\")\nprint(f\"   Variables y : {len(TARGETS_FINAL)}\")\nprint(f\"\\n   Variables X incluidas:\")\nfor v in INDEPENDENT_VARS_FINAL:\n    n_null = df_model[v].isnull().sum()\n    print(f\"     {v:<35} nulos={n_null:,}\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8770ffc",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# 8.2  ORDINAL ENCODING - VARIABLES CON ORDEN NATURAL\n# Las categorias deben estar SIN tildes (se normalizaron en 4.4).\n# =============================================================================\n\n# Categorias en orden ascendente sin tildes ni caracteres especiales\nEDU_LEVELS = [\n    \"Ninguno\",\n    \"Primaria incompleta\",     \"Primaria completa\",\n    \"Secundaria (Bachillerato) incompleta\", \"Secundaria (Bachillerato) completa\",\n    \"Tecnica o tecnologica incompleta\",     \"Tecnica o tecnologica completa\",\n    \"Educacion profesional incompleta\",     \"Educacion profesional completa\",\n    \"Postgrado\",\n]\n\nESTRATO_LEVELS = [\n    \"Sin Estrato\",\n    \"Estrato 1\", \"Estrato 2\", \"Estrato 3\",\n    \"Estrato 4\", \"Estrato 5\", \"Estrato 6\",\n]\n\nORDINAL_VARS = {\n    \"FAMI_EDUCACIONMADRE\" : EDU_LEVELS,\n    \"FAMI_EDUCACIONPADRE\" : EDU_LEVELS,\n    \"FAMI_ESTRATOVIVIENDA\": ESTRATO_LEVELS,\n}\n\nencoders = {}\n\nfor col, cat_order in ORDINAL_VARS.items():\n    if col not in df_model.columns:\n        continue\n\n    df_model[col] = df_model[col].astype(str).str.strip()\n\n    # Verificar categorias presentes en datos\n    present_raw = df_model[col].unique().tolist()\n    print(f\"  {col} - categorias en datos: {sorted([x for x in present_raw if x not in ['nan','None','<NA>']])}\")\n\n    present = [c for c in cat_order if c in df_model[col].unique()]\n    extras  = [c for c in df_model[col].unique()\n               if c not in cat_order and c not in [\"nan\", \"None\", \"<NA>\"]]\n    all_cats = present + extras\n\n    enc = OrdinalEncoder(\n        categories=[all_cats],\n        handle_unknown=\"use_encoded_value\",\n        unknown_value=-1,\n        encoded_missing_value=-1,\n    )\n    df_model[col] = enc.fit_transform(df_model[[col]])\n    encoders[col] = enc\n    print(f\"    -> OrdinalEncoding: {len(all_cats)} niveles -> rango [{df_model[col].min():.0f}, {df_model[col].max():.0f}]\")\n\nprint(\"\\nOrdinal Encoding completado.\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934fd5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 8.3  LABEL ENCODING â€“ VARIABLES NOMINALES\n",
    "# LabelEncoder asigna un entero Ãºnico a cada categorÃ­a sin implicar orden.\n",
    "# Adecuado para variables nominales en modelos basados en Ã¡rboles.\n",
    "# =============================================================================\n",
    "\n",
    "LABEL_VARS = [\n",
    "    v for v in INDEPENDENT_VARS_FINAL\n",
    "    if (df_model[v].dtype.name in [\"category\", \"object\", \"string\"])\n",
    "    and v not in ORDINAL_VARS\n",
    "]\n",
    "\n",
    "for col in LABEL_VARS:\n",
    "    if col not in df_model.columns:\n",
    "        continue\n",
    "    df_model[col] = df_model[col].astype(str)\n",
    "    le = LabelEncoder()\n",
    "    df_model[col] = le.fit_transform(df_model[col])\n",
    "    encoders[col] = le\n",
    "    print(f\"  âœ… [{col:<30}] LabelEncoding  â†’ {len(le.classes_)} clases\")\n",
    "\n",
    "print(f\"\\nâœ… Label Encoding completado. Total encoders almacenados: {len(encoders)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7863a2fa",
   "metadata": {},
   "source": [
    "### ğŸ”§ 8.5 â€” Interacciones economicas y transformaciones no lineales\n",
    "\n",
    "Se crean variables derivadas a partir de las variables ya **codificadas numericamente**  \n",
    "(post OrdinalEncoding de Â§8.2). Deben crearse antes del escalado para que  \n",
    "MinMaxScaler las normalice en el mismo rango que el resto.\n",
    "\n",
    "| Grupo | Variable | Formula | Captura |\n",
    "|-------|----------|---------|--------|\n",
    "| Economico | `ESTRATO_X_EDU_MADRE` | estrato x edu_madre | Capital social combinado |\n",
    "| Economico | `ESTRATO_X_EDU_PADRE` | estrato x edu_padre | Capital social combinado |\n",
    "| Hogar | `DENSIDAD_HOGAR` | personas / (cuartos + 1) | Hacinamiento |\n",
    "| Economico | `INTERNET_X_EDU_MADRE` | internet x edu_madre | Acceso digital educado |\n",
    "| Log | `LOG_PERSONAS` | log1p(personas) | Atenua asimetria |\n",
    "| Log | `LOG_CUARTOS` | log1p(cuartos) | Atenua asimetria |\n",
    "\n",
    "> Las transformaciones logaritmicas (`log1p`) se aplican porque personas y cuartos\n",
    "> tienen distribuciones con cola derecha. `log1p` maneja el cero de forma segura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a312a504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 8.5  INTERACCIONES ECONOMICAS Y TRANSFORMACIONES NO LINEALES\n",
    "#\n",
    "# FAMI_ESTRATOVIVIENDA, FAMI_EDUCACIONMADRE, FAMI_EDUCACIONPADRE\n",
    "# ya son numericas (OrdinalEncoder aplicado en 8.2).\n",
    "# FAMI_TIENEINTERNET fue codificada como 0/1 en 8.3 (label encoding).\n",
    "# FAMI_PERSONASHOGAR y FAMI_CUARTOSHOGAR son numericas desde la imputacion.\n",
    "# =============================================================================\n",
    "\n",
    "NUEVAS_FE = []\n",
    "problemas = []\n",
    "\n",
    "def safe_create(nombre, formula_fn, df_m):\n",
    "    \"\"\"Crea variable nueva manejando errores sin romper el pipeline.\"\"\"\n",
    "    try:\n",
    "        serie = formula_fn(df_m)\n",
    "        # Reemplazar inf por NaN\n",
    "        serie = serie.replace([np.inf, -np.inf], np.nan)\n",
    "        df_m[nombre] = serie\n",
    "        n_null = serie.isna().sum()\n",
    "        n_ok   = serie.notna().sum()\n",
    "        print(f'  OK  {nombre:<30} '\n",
    "              f'media={serie.mean():.3f}  '\n",
    "              f'nulos={n_null:,} ({n_null/len(serie)*100:.1f}%)')\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        problemas.append((nombre, str(e)))\n",
    "        print(f'  OMITIDA  {nombre:<24} -> {str(e)[:60]}')\n",
    "        return False\n",
    "\n",
    "\n",
    "print('--- Grupo 1: Interacciones economicas ---')\n",
    "\n",
    "if safe_create(\n",
    "    'ESTRATO_X_EDU_MADRE',\n",
    "    lambda d: d['FAMI_ESTRATOVIVIENDA'] * d['FAMI_EDUCACIONMADRE'],\n",
    "    df_model\n",
    "):\n",
    "    NUEVAS_FE.append('ESTRATO_X_EDU_MADRE')\n",
    "\n",
    "if safe_create(\n",
    "    'ESTRATO_X_EDU_PADRE',\n",
    "    lambda d: d['FAMI_ESTRATOVIVIENDA'] * d['FAMI_EDUCACIONPADRE'],\n",
    "    df_model\n",
    "):\n",
    "    NUEVAS_FE.append('ESTRATO_X_EDU_PADRE')\n",
    "\n",
    "if safe_create(\n",
    "    'DENSIDAD_HOGAR',\n",
    "    lambda d: d['FAMI_PERSONASHOGAR'] / (d['FAMI_CUARTOSHOGAR'] + 1),\n",
    "    df_model\n",
    "):\n",
    "    NUEVAS_FE.append('DENSIDAD_HOGAR')\n",
    "\n",
    "# FAMI_TIENEINTERNET puede venir de label encoding (0/1) o no existir\n",
    "internet_col = 'FAMI_TIENEINTERNET'\n",
    "if internet_col in df_model.columns:\n",
    "    if safe_create(\n",
    "        'INTERNET_X_EDU_MADRE',\n",
    "        lambda d: d[internet_col] * d['FAMI_EDUCACIONMADRE'],\n",
    "        df_model\n",
    "    ):\n",
    "        NUEVAS_FE.append('INTERNET_X_EDU_MADRE')\n",
    "else:\n",
    "    print(f'  OMITIDA  INTERNET_X_EDU_MADRE -> {internet_col} no disponible')\n",
    "\n",
    "\n",
    "print('\\n--- Grupo 2: Transformaciones logaritmicas ---')\n",
    "\n",
    "if safe_create(\n",
    "    'LOG_PERSONAS',\n",
    "    lambda d: np.log1p(d['FAMI_PERSONASHOGAR'].clip(lower=0)),\n",
    "    df_model\n",
    "):\n",
    "    NUEVAS_FE.append('LOG_PERSONAS')\n",
    "\n",
    "if safe_create(\n",
    "    'LOG_CUARTOS',\n",
    "    lambda d: np.log1p(d['FAMI_CUARTOSHOGAR'].clip(lower=0)),\n",
    "    df_model\n",
    "):\n",
    "    NUEVAS_FE.append('LOG_CUARTOS')\n",
    "\n",
    "\n",
    "# Registrar en INDEPENDENT_VARS_FINAL para que pasen por el escalado\n",
    "for v in NUEVAS_FE:\n",
    "    if v not in INDEPENDENT_VARS_FINAL:\n",
    "        INDEPENDENT_VARS_FINAL.append(v)\n",
    "\n",
    "print(f'\\nVariables creadas exitosamente: {len(NUEVAS_FE)}')\n",
    "print(f'  {NUEVAS_FE}')\n",
    "if problemas:\n",
    "    print(f'\\nVariables omitidas por error: {len(problemas)}')\n",
    "    for nm, err in problemas:\n",
    "        print(f'  {nm}: {err}')\n",
    "print(f'\\nINDEPENDENT_VARS_FINAL: {len(INDEPENDENT_VARS_FINAL)} variables en total')\n",
    "\n",
    "# Visualizacion rapida de distribucion de las nuevas variables\n",
    "if NUEVAS_FE:\n",
    "    cols_vis = [v for v in NUEVAS_FE if v in df_model.columns]\n",
    "    n_cols   = len(cols_vis)\n",
    "    n_rows   = (n_cols + 2) // 3\n",
    "    fig, axes = plt.subplots(n_rows, 3,\n",
    "                             figsize=(18, 4.5*n_rows),\n",
    "                             constrained_layout=True)\n",
    "    axes_flat = axes.flatten() if n_cols > 1 else [axes]\n",
    "\n",
    "    grupos = {\n",
    "        'ESTRATO_X_EDU_MADRE' : ('Interaccion economica', '#4f8ef7'),\n",
    "        'ESTRATO_X_EDU_PADRE' : ('Interaccion economica', '#4f8ef7'),\n",
    "        'DENSIDAD_HOGAR'      : ('Hacinamiento',          '#f0a840'),\n",
    "        'INTERNET_X_EDU_MADRE': ('Interaccion economica', '#4f8ef7'),\n",
    "        'LOG_PERSONAS'        : ('Log transform',         '#42d9a8'),\n",
    "        'LOG_CUARTOS'         : ('Log transform',         '#42d9a8'),\n",
    "    }\n",
    "\n",
    "    for ax, col in zip(axes_flat, cols_vis):\n",
    "        label, color = grupos.get(col, ('Variable', '#999'))\n",
    "        df_model[col].dropna().plot.hist(\n",
    "            ax=ax, bins=35, color=color, edgecolor='white', alpha=0.85\n",
    "        )\n",
    "        ax.set_title(f'{col}\\n[{label}]', fontsize=9.5, fontweight='bold')\n",
    "        ax.set_xlabel('Valor', fontsize=8.5)\n",
    "        stats_txt = (f\"media={df_model[col].mean():.3f}\\n\"\n",
    "                     f\"std ={df_model[col].std():.3f}\\n\"\n",
    "                     f\"nulos={df_model[col].isna().sum():,}\")\n",
    "        ax.text(0.97, 0.95, stats_txt, transform=ax.transAxes,\n",
    "                ha='right', va='top', fontsize=7.5,\n",
    "                bbox=dict(facecolor='white', alpha=0.8, pad=3, edgecolor='#ccc'))\n",
    "\n",
    "    # Ocultar ejes sobrantes\n",
    "    for ax in axes_flat[n_cols:]:\n",
    "        ax.set_visible(False)\n",
    "\n",
    "    fig.suptitle('8.5 Nuevas Variables: Interacciones + Log Transforms\\n'\n",
    "                 '(distribucion post-encoding, pre-escalado)',\n",
    "                 fontsize=12, fontweight='bold')\n",
    "    plt.savefig('fe_nuevas_variables.png', dpi=130, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print('fe_nuevas_variables.png guardado')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a374c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 8.4  VERIFICACIÃ“N POST-CODIFICACIÃ“N\n",
    "# Todas las variables independientes deben ser de tipo numÃ©rico.\n",
    "# =============================================================================\n",
    "\n",
    "non_num = df_model[INDEPENDENT_VARS_FINAL].select_dtypes(exclude=\"number\").columns.tolist()\n",
    "\n",
    "if non_num:\n",
    "    print(f\"âš ï¸  AÃºn no numÃ©ricas: {non_num}\")\n",
    "else:\n",
    "    print(\"âœ… Todas las variables independientes son numÃ©ricas.\")\n",
    "\n",
    "display(df_model[INDEPENDENT_VARS_FINAL].dtypes.to_frame(\"Dtype tras codificaciÃ³n\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17a53ec",
   "metadata": {},
   "source": [
    "## ğŸ“ 9. Escalado con MinMaxScaler\n",
    "\n",
    "**MinMaxScaler** transforma cada variable al rango [0, 1]:\n",
    "\n",
    "$$x_{scaled} = \\frac{x - x_{min}}{x_{max} - x_{min}}$$\n",
    "\n",
    "**Â¿Por quÃ© escalar?** Algoritmos como KNN, SVM, RegresiÃ³n Lineal con regularizaciÃ³n\n",
    "y Redes Neuronales son sensibles a la magnitud de las variables. Sin escalado,\n",
    "una variable como `ANIO` (âˆ¼2020) dominarÃ­a sobre `EDAD` (âˆ¼17).\n",
    "\n",
    "> Solo se escalan las variables independientes X. Las variables y (targets)\n",
    "> no se escalan para preservar la interpretabilidad de las predicciones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c49ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 9.1  SEPARAR X e y\n",
    "# =============================================================================\n",
    "\n",
    "X = df_model[INDEPENDENT_VARS_FINAL].copy()\n",
    "y = df_model[TARGETS_FINAL].copy()\n",
    "\n",
    "print(\"Rangos de X antes del escalado:\")\n",
    "display(X.agg([\"min\", \"max\"]).T.rename(columns={\"min\": \"MÃ­nimo\", \"max\": \"MÃ¡ximo\"}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050ab01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 9.2  APLICAR MINMAXSCALER\n",
    "# Se guarda el scaler para poder invertir la transformaciÃ³n en producciÃ³n\n",
    "# (necesario para interpretar predicciones).\n",
    "# =============================================================================\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_scaled_arr = scaler.fit_transform(X)\n",
    "\n",
    "X_scaled = pd.DataFrame(X_scaled_arr, columns=INDEPENDENT_VARS_FINAL, index=X.index)\n",
    "\n",
    "print(\"âœ… MinMaxScaler aplicado.\")\n",
    "print(\"\\nRango de X despuÃ©s del escalado (debe ser [0.000 â€“ 1.000]):\")\n",
    "display(\n",
    "    X_scaled.agg([\"min\", \"max\"]).T\n",
    "    .rename(columns={\"min\": \"MÃ­nimo\", \"max\": \"MÃ¡ximo\"})\n",
    "    .round(4)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906541aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 9.3  VISUALIZACIÃ“N ANTES vs DESPUÃ‰S DEL ESCALADO\n",
    "# =============================================================================\n",
    "\n",
    "viz_cols = [c for c in [\"EDAD\", \"FAMI_PERSONASHOGAR\", \"INDICE_BIENES\", \"ANIO\"]\n",
    "            if c in INDEPENDENT_VARS_FINAL]\n",
    "\n",
    "fig, axes = plt.subplots(2, len(viz_cols), figsize=(5 * len(viz_cols), 8))\n",
    "\n",
    "for i, col in enumerate(viz_cols):\n",
    "    # Antes\n",
    "    axes[0, i].hist(X[col].dropna(), bins=40, color=\"#4C72B0\", edgecolor=\"white\")\n",
    "    axes[0, i].set_title(f\"{col}\\n(Original)\", fontsize=10)\n",
    "    axes[0, i].set_ylabel(\"Frecuencia\" if i == 0 else \"\")\n",
    "\n",
    "    # DespuÃ©s\n",
    "    axes[1, i].hist(X_scaled[col].dropna(), bins=40, color=\"#DD8452\", edgecolor=\"white\")\n",
    "    axes[1, i].set_title(f\"{col}\\nEscalado [0, 1]\", fontsize=10)\n",
    "    axes[1, i].set_xlabel(\"Valor\")\n",
    "    axes[1, i].set_ylabel(\"Frecuencia\" if i == 0 else \"\")\n",
    "\n",
    "fig.suptitle(\"Distribuciones antes y despuÃ©s de MinMaxScaler\", fontsize=13, y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"escalado_comparacion.png\", dpi=130, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc0cd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 9.4  DATASET FINAL UNIFICADO\n",
    "# =============================================================================\n",
    "\n",
    "df_final = pd.concat(\n",
    "    [X_scaled.reset_index(drop=True), y.reset_index(drop=True)],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(f\"âœ… Dataset final: {df_final.shape[0]:,} filas Ã— {df_final.shape[1]} columnas\")\n",
    "print(f\"   Variables X    : {len(INDEPENDENT_VARS_FINAL)}\")\n",
    "print(f\"   Variables y    : {len(TARGETS_FINAL)}\")\n",
    "print(f\"   Valores nulos  : {df_final.isnull().sum().sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd83586",
   "metadata": {},
   "source": [
    "## âœ… 10. VerificaciÃ³n Final y ExportaciÃ³n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989d199a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 10.1  ESTADÃSTICAS FINALES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 65)\n",
    "print(\"  ESTADÃSTICAS DEL DATASET FINAL\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "print(\"\\nâ”€â”€ Variables independientes X (escaladas) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "display(X_scaled.describe().round(4))\n",
    "\n",
    "print(\"\\nâ”€â”€ Variables objetivo y (sin escalar) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "display(y.describe().round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0316b55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 10.2  EXPORTACIÃ“N\n",
    "# Parquet: mÃ¡s eficiente para pipelines ML (preserva tipos, menor tamaÃ±o).\n",
    "# CSV: compatibilidad universal.\n",
    "# =============================================================================\n",
    "\n",
    "try:\n",
    "    df_final.to_parquet(\"icfes_procesado.parquet\", index=False, compression=\"snappy\")\n",
    "    print(\"âœ… Exportado: icfes_procesado.parquet\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  Parquet no disponible: {e}\")\n",
    "\n",
    "df_final.to_csv(\"icfes_procesado.csv\", index=False, encoding=\"utf-8\")\n",
    "print(\"âœ… Exportado: icfes_procesado.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da992f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 10.3  RESUMEN VISUAL DEL PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "steps = [\n",
    "    (\"1. Carga de datos\",              f\"{df_raw.shape[0]:,} filas  Ã—  {df_raw.shape[1]} columnas\"),\n",
    "    (\"2. EDA\",                         \"Distribuciones, nulos, outliers\"),\n",
    "    (\"3. Limpieza\",                    \"Tipos, normalizaciÃ³n, encoding\"),\n",
    "    (\"4. ImputaciÃ³n\",                  \"Mediana (num) | Moda grupal (cat)\"),\n",
    "    (\"5. AnÃ¡lisis de correlaciÃ³n\",     \"Pearson + CramÃ©r's V\"),\n",
    "    (\"6. Feature Engineering\",         \"EDAD, ANIO, TRIMESTRE, INDICE_BIENES\"),\n",
    "    (\"7. EliminaciÃ³n de redundantes\",  f\"{len(removed_cols)} columnas removidas\"),\n",
    "    (\"8. CodificaciÃ³n categÃ³rica\",     f\"Ordinal ({len(ORDINAL_VARS)}) + Label ({len(LABEL_VARS)})\"),\n",
    "    (\"9. Escalado MinMaxScaler\",       f\"{len(INDEPENDENT_VARS_FINAL)} vars â†’ [0, 1]\"),\n",
    "    (\"10. Dataset final\",              f\"{df_final.shape[0]:,} filas  Ã—  {df_final.shape[1]} columnas\"),\n",
    "]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"  PIPELINE COMPLETO â€“ RESUMEN\")\n",
    "print(\"=\" * 70)\n",
    "for step, detail in steps:\n",
    "    print(f\"  âœ…  {step:<40} {detail}\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nğŸš€ Dataset listo para entrenar modelos de Machine Learning.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259fb4b2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ 12. PreparaciÃ³n Final del Dataset para Modelado\n",
    "\n",
    "Esta secciÃ³n realiza los Ãºltimos pasos antes de entrenar los modelos:\n",
    "\n",
    "| Paso | DescripciÃ³n |\n",
    "|------|-------------|\n",
    "| **12.1** | Limpieza estructural: eliminar columnas no escaladas y residuales |\n",
    "| **12.2** | CodificaciÃ³n de la variable de salida `DESEMP_INGLES` (ordinal) |\n",
    "| **12.3** | AuditorÃ­a final: tipos, nulos, rangos, duplicados |\n",
    "| **12.4** | SerializaciÃ³n de objetos del pipeline (scaler, encoders) |\n",
    "| **12.5** | SeparaciÃ³n definitiva X / y por tipo de problema |\n",
    "| **12.6** | ExportaciÃ³n del dataset y artefactos listos para modelado |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3136c840",
   "metadata": {},
   "source": [
    "### ğŸ§¹ 12.1 â€” Limpieza estructural: solo columnas escaladas y targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89ddf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 12.1  ELIMINAR COLUMNAS NO ESCALADAS Y RESIDUALES\n",
    "#\n",
    "# El dataset df_final puede contener columnas que no pasaron por el escalado\n",
    "# (ej. variables categÃ³ricas sin codificar, columnas auxiliares de cÃ¡lculo,\n",
    "# o variables que quedaron del proceso de validaciÃ³n).\n",
    "#\n",
    "# Regla de oro para modelado:\n",
    "#   âœ… Variables X â†’ todas numÃ©ricas, en rango [0, 1] (MinMaxScaler)\n",
    "#   âœ… Variables y â†’ targets originales sin escalar (interpretables)\n",
    "#   âŒ Cualquier otra columna â†’ eliminar\n",
    "# =============================================================================\n",
    "\n",
    "# Variables que deben estar en el dataset final\n",
    "EXPECTED_X = [c for c in INDEPENDENT_VARS_FINAL if c in df_final.columns]\n",
    "EXPECTED_Y = [c for c in TARGETS_FINAL          if c in df_final.columns]\n",
    "EXPECTED_ALL = EXPECTED_X + EXPECTED_Y\n",
    "\n",
    "# Detectar columnas extra (no esperadas)\n",
    "extra_cols = [c for c in df_final.columns if c not in EXPECTED_ALL]\n",
    "\n",
    "print(\"â”€â”€ DiagnÃ³stico de columnas â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "print(f\"  Columnas esperadas (X): {len(EXPECTED_X)}\")\n",
    "print(f\"  Columnas esperadas (y): {len(EXPECTED_Y)}\")\n",
    "print(f\"  Columnas extra        : {len(extra_cols)}\")\n",
    "\n",
    "if extra_cols:\n",
    "    print(f\"\\n  âš ï¸  Columnas extra detectadas â†’ serÃ¡n eliminadas:\")\n",
    "    for c in extra_cols:\n",
    "        print(f\"     â€¢ {c}  (dtype: {df_final[c].dtype})\")\n",
    "    df_final.drop(columns=extra_cols, inplace=True)\n",
    "\n",
    "# Reordenar: primero X, luego y\n",
    "df_final = df_final[EXPECTED_X + EXPECTED_Y].copy()\n",
    "\n",
    "print(f\"\\nâœ… Dataset limpio:\")\n",
    "print(f\"   Shape      : {df_final.shape}\")\n",
    "print(f\"   Columnas X : {len(EXPECTED_X)}\")\n",
    "print(f\"   Columnas y : {len(EXPECTED_Y)}\")\n",
    "print(f\"\\n   Columnas finales:\")\n",
    "for i, c in enumerate(df_final.columns):\n",
    "    tag = \"[X]\" if c in EXPECTED_X else \"[y]\"\n",
    "    print(f\"   {i+1:>2}. {tag} {c}  ({df_final[c].dtype})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f298a6aa",
   "metadata": {},
   "source": [
    "### ğŸ·ï¸ 12.2 â€” CodificaciÃ³n de la variable de salida `DESEMP_INGLES`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f36c7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 12.2  CODIFICACIÃ“N DE DESEMP_INGLES (variable de salida categÃ³rica ordinal)\n",
    "#\n",
    "# DESEMP_INGLES es una variable de desempeÃ±o en inglÃ©s con niveles ordenados:\n",
    "#   A- < A1 < A2 < B1 < B+ \n",
    "# (Marco ComÃºn Europeo de Referencia para las Lenguas - MCER)\n",
    "#\n",
    "# Se aplica OrdinalEncoder para preservar el orden jerÃ¡rquico de los niveles.\n",
    "# El encoder se guarda para poder invertir la transformaciÃ³n en predicciÃ³n.\n",
    "# =============================================================================\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import numpy as np\n",
    "\n",
    "COL_DESEMP = \"DESEMP_INGLES\"\n",
    "\n",
    "if COL_DESEMP in df_final.columns:\n",
    "    # Revisar los valores presentes antes de codificar\n",
    "    print(f\"Valores presentes en {COL_DESEMP}:\")\n",
    "    vc = df_final[COL_DESEMP].value_counts(dropna=False).reset_index()\n",
    "    vc.columns = [\"Nivel\", \"Conteo\"]\n",
    "    vc[\"% del total\"] = (vc[\"Conteo\"] / len(df_final) * 100).round(2)\n",
    "    display(vc)\n",
    "\n",
    "    # Orden oficial del MCER (ascendente)\n",
    "    DESEMP_ORDER = [\"A-\", \"A1\", \"A2\", \"B1\", \"B+\"]\n",
    "\n",
    "    # CategorÃ­as presentes + desconocidas al final\n",
    "    present_cats = [c for c in DESEMP_ORDER if c in df_final[COL_DESEMP].dropna().unique()]\n",
    "    extra_cats   = [c for c in df_final[COL_DESEMP].dropna().unique()\n",
    "                    if c not in DESEMP_ORDER and str(c) not in [\"nan\", \"None\"]]\n",
    "    all_cats     = present_cats + extra_cats\n",
    "\n",
    "    enc_desemp = OrdinalEncoder(\n",
    "        categories=[all_cats],\n",
    "        handle_unknown=\"use_encoded_value\",\n",
    "        unknown_value=-1,\n",
    "        encoded_missing_value=np.nan,\n",
    "    )\n",
    "\n",
    "    # Convertir a string para el encoder\n",
    "    df_final[COL_DESEMP] = df_final[COL_DESEMP].astype(str).replace(\"nan\", np.nan)\n",
    "    df_final[[COL_DESEMP]] = enc_desemp.fit_transform(df_final[[COL_DESEMP]])\n",
    "\n",
    "    # Guardar encoder en el diccionario de encoders del pipeline\n",
    "    encoders[COL_DESEMP] = enc_desemp\n",
    "\n",
    "    print(f\"\\nâœ… {COL_DESEMP} codificada ordinalmente:\")\n",
    "    print(f\"   Mapeo de niveles:\")\n",
    "    for i, cat in enumerate(enc_desemp.categories_[0]):\n",
    "        print(f\"     {cat:>4}  â†’  {i}\")\n",
    "\n",
    "    print(f\"\\n   EstadÃ­sticas tras codificaciÃ³n:\")\n",
    "    print(df_final[COL_DESEMP].describe().round(2).to_string())\n",
    "\n",
    "    # VisualizaciÃ³n de la distribuciÃ³n codificada\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(13, 4))\n",
    "\n",
    "    # DistribuciÃ³n de frecuencias\n",
    "    sns.countplot(\n",
    "        x=df_final[COL_DESEMP].dropna(),\n",
    "        ax=axes[0],\n",
    "        palette=\"Blues_d\",\n",
    "        order=sorted(df_final[COL_DESEMP].dropna().unique()),\n",
    "    )\n",
    "    axes[0].set_title(f\"DistribuciÃ³n de {COL_DESEMP} (codificada)\", fontsize=11)\n",
    "    axes[0].set_xlabel(\"CÃ³digo numÃ©rico del nivel MCER\")\n",
    "    axes[0].set_ylabel(\"Frecuencia\")\n",
    "\n",
    "    # Etiquetas originales en el eje\n",
    "    labels = [f\"{i}\n",
    "({cat})\" for i, cat in enumerate(enc_desemp.categories_[0])]\n",
    "    axes[0].set_xticklabels(labels, fontsize=9)\n",
    "\n",
    "    # Boxplot de PUNT_INGLES por nivel de DESEMP_INGLES\n",
    "    if \"PUNT_INGLES\" in df_final.columns:\n",
    "        merged_plot = df_final[[COL_DESEMP, \"PUNT_INGLES\"]].dropna()\n",
    "        sns.boxplot(\n",
    "            x=COL_DESEMP,\n",
    "            y=\"PUNT_INGLES\",\n",
    "            data=merged_plot,\n",
    "            ax=axes[1],\n",
    "            palette=\"Blues\",\n",
    "            order=sorted(merged_plot[COL_DESEMP].unique()),\n",
    "        )\n",
    "        axes[1].set_title(f\"PUNT_INGLES por nivel de {COL_DESEMP}\", fontsize=11)\n",
    "        axes[1].set_xlabel(\"Nivel MCER (codificado)\")\n",
    "        axes[1].set_ylabel(\"Puntaje InglÃ©s\")\n",
    "        xticklabels_1 = [f\"{i}\n",
    "({cat})\" for i, cat in enumerate(enc_desemp.categories_[0])\n",
    "                         if i in merged_plot[COL_DESEMP].unique()]\n",
    "        axes[1].set_xticklabels(xticklabels_1, fontsize=9)\n",
    "    else:\n",
    "        axes[1].set_visible(False)\n",
    "\n",
    "    plt.suptitle(f\"CodificaciÃ³n Ordinal â€“ {COL_DESEMP}\", fontsize=13, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"desemp_ingles_codificado.png\", dpi=130, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(f\"âš ï¸  '{COL_DESEMP}' no encontrada en df_final.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcb9db6",
   "metadata": {},
   "source": [
    "### ğŸ” 12.3 â€” AuditorÃ­a final del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a72f0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 12.3  AUDITORÃA FINAL â€” VERIFICACIÃ“N COMPLETA DEL DATASET\n",
    "#\n",
    "# Antes de entrenar cualquier modelo, el dataset debe cumplir:\n",
    "#   1. Sin valores nulos en variables X\n",
    "#   2. Todos los tipos de datos son numÃ©ricos\n",
    "#   3. Variables X escaladas en [0, 1]\n",
    "#   4. Sin filas duplicadas\n",
    "#   5. Sin columnas constantes\n",
    "#   6. TamaÃ±o suficiente para entrenamiento\n",
    "# =============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"  AUDITORÃA FINAL DEL DATASET\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "EXPECTED_X_FINAL = [c for c in EXPECTED_X if c in df_final.columns]\n",
    "EXPECTED_Y_FINAL = [c for c in EXPECTED_Y if c in df_final.columns]\n",
    "\n",
    "audit_issues = []\n",
    "\n",
    "# â”€â”€ CHECK 1: Tipos de datos â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "non_numeric_X = df_final[EXPECTED_X_FINAL].select_dtypes(exclude=\"number\").columns.tolist()\n",
    "status_1 = \"âœ…\" if not non_numeric_X else \"âŒ\"\n",
    "print(f\"\\n{status_1} CHECK 1 â€“ Tipos numÃ©ricos en X\")\n",
    "if non_numeric_X:\n",
    "    print(f\"   Columnas no numÃ©ricas: {non_numeric_X}\")\n",
    "    audit_issues.append(f\"Columnas no numÃ©ricas: {non_numeric_X}\")\n",
    "else:\n",
    "    print(f\"   Todas las {len(EXPECTED_X_FINAL)} variables X son numÃ©ricas.\")\n",
    "\n",
    "# â”€â”€ CHECK 2: Nulos en X â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "null_X = df_final[EXPECTED_X_FINAL].isnull().sum()\n",
    "null_X_pos = null_X[null_X > 0]\n",
    "status_2 = \"âœ…\" if null_X_pos.empty else \"âŒ\"\n",
    "print(f\"\\n{status_2} CHECK 2 â€“ Sin valores nulos en X\")\n",
    "if not null_X_pos.empty:\n",
    "    print(f\"   Columnas con nulos:\")\n",
    "    print(null_X_pos.to_string())\n",
    "    audit_issues.append(f\"Nulos en X: {null_X_pos.to_dict()}\")\n",
    "else:\n",
    "    print(f\"   0 nulos en las {len(EXPECTED_X_FINAL)} variables X.\")\n",
    "\n",
    "# â”€â”€ CHECK 3: Rango [0, 1] en X â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "X_min = df_final[EXPECTED_X_FINAL].min()\n",
    "X_max = df_final[EXPECTED_X_FINAL].max()\n",
    "out_of_range = ((X_min < -0.001) | (X_max > 1.001))\n",
    "cols_oor = out_of_range[out_of_range].index.tolist()\n",
    "status_3 = \"âœ…\" if not cols_oor else \"âš ï¸\"\n",
    "print(f\"\\n{status_3} CHECK 3 â€“ Rango [0, 1] en X (MinMaxScaler)\")\n",
    "if cols_oor:\n",
    "    print(f\"   Variables fuera de [0,1] (pueden ser NaN tratados):\")\n",
    "    for c in cols_oor:\n",
    "        print(f\"     {c}: min={X_min[c]:.4f}, max={X_max[c]:.4f}\")\n",
    "    audit_issues.append(f\"Variables fuera de rango: {cols_oor}\")\n",
    "else:\n",
    "    print(f\"   Todas las variables X estÃ¡n en [0.0000, 1.0000].\")\n",
    "\n",
    "# â”€â”€ CHECK 4: Duplicados â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "n_dups = df_final.duplicated().sum()\n",
    "status_4 = \"âœ…\" if n_dups == 0 else \"âš ï¸\"\n",
    "print(f\"\\n{status_4} CHECK 4 â€“ Sin filas duplicadas\")\n",
    "if n_dups > 0:\n",
    "    print(f\"   {n_dups:,} filas duplicadas detectadas ({n_dups/len(df_final)*100:.3f}%)\")\n",
    "    df_final.drop_duplicates(inplace=True)\n",
    "    print(f\"   â†’ Eliminadas. Filas restantes: {len(df_final):,}\")\n",
    "    audit_issues.append(f\"{n_dups} duplicados eliminados\")\n",
    "else:\n",
    "    print(f\"   Sin filas duplicadas en {len(df_final):,} registros.\")\n",
    "\n",
    "# â”€â”€ CHECK 5: Variables constantes â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "var_zero = df_final[EXPECTED_X_FINAL].std()\n",
    "const_cols = var_zero[var_zero == 0].index.tolist()\n",
    "status_5 = \"âœ…\" if not const_cols else \"âŒ\"\n",
    "print(f\"\\n{status_5} CHECK 5 â€“ Sin variables constantes (std = 0)\")\n",
    "if const_cols:\n",
    "    print(f\"   Columnas constantes: {const_cols}\")\n",
    "    df_final.drop(columns=const_cols, inplace=True)\n",
    "    EXPECTED_X_FINAL = [c for c in EXPECTED_X_FINAL if c not in const_cols]\n",
    "    print(f\"   â†’ Eliminadas del dataset.\")\n",
    "    audit_issues.append(f\"Columnas constantes eliminadas: {const_cols}\")\n",
    "else:\n",
    "    print(f\"   Ninguna variable tiene varianza cero.\")\n",
    "\n",
    "# â”€â”€ CHECK 6: TamaÃ±o del dataset â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "MIN_ROWS = 1000\n",
    "status_6 = \"âœ…\" if len(df_final) >= MIN_ROWS else \"âŒ\"\n",
    "print(f\"\\n{status_6} CHECK 6 â€“ TamaÃ±o suficiente (mÃ­nimo {MIN_ROWS:,} registros)\")\n",
    "print(f\"   Registros disponibles: {len(df_final):,}\")\n",
    "if len(df_final) < MIN_ROWS:\n",
    "    audit_issues.append(f\"Dataset muy pequeÃ±o: {len(df_final)} registros\")\n",
    "\n",
    "# â”€â”€ CHECK 7: Nulos en y â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "null_y = df_final[EXPECTED_Y_FINAL].isnull().sum()\n",
    "null_y_pos = null_y[null_y > 0]\n",
    "status_7 = \"âœ…\" if null_y_pos.empty else \"âš ï¸\"\n",
    "print(f\"\\n{status_7} CHECK 7 â€“ Nulos en y (variables objetivo)\")\n",
    "if not null_y_pos.empty:\n",
    "    print(f\"   Variables con nulos (normal para multioutput):\")\n",
    "    display(null_y_pos.to_frame(\"Nulos\").assign(\n",
    "        Porcentaje=lambda df: (df[\"Nulos\"] / len(df_final) * 100).round(2)\n",
    "    ))\n",
    "else:\n",
    "    print(f\"   Sin nulos en las {len(EXPECTED_Y_FINAL)} variables objetivo.\")\n",
    "\n",
    "# â”€â”€ Resumen final â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"  RESULTADO DE AUDITORÃA: {'âš ï¸  ' + str(len(audit_issues)) + ' issue(s)' if audit_issues else 'âœ… DATASET APROBADO'}\")\n",
    "print(\"=\" * 70)\n",
    "if audit_issues:\n",
    "    for i, issue in enumerate(audit_issues, 1):\n",
    "        print(f\"  {i}. {issue}\")\n",
    "else:\n",
    "    print(\"  El dataset supera todos los checks de calidad.\")\n",
    "print(f\"\\n  ğŸ“ Shape final: {df_final.shape[0]:,} filas Ã— {df_final.shape[1]} columnas\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ad8bf4",
   "metadata": {},
   "source": [
    "### ğŸ’¾ 12.4 â€” SerializaciÃ³n del pipeline de preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0267355d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 12.4  SERIALIZACIÃ“N DEL PIPELINE DE PREPROCESAMIENTO\n",
    "#\n",
    "# Se serializa con joblib (recomendado para objetos sklearn) y pickle como\n",
    "# respaldo. Esto permite:\n",
    "#   â€¢ Reutilizar las mismas transformaciones en producciÃ³n sin re-entrenar\n",
    "#   â€¢ Reproducir el preprocesamiento exacto sobre nuevos datos\n",
    "#   â€¢ Invertir el escalado para interpretar predicciones (inverse_transform)\n",
    "#\n",
    "# Artefactos serializados:\n",
    "#   pipeline_artefacts.joblib  â† objeto completo (recomendado)\n",
    "#   scaler.joblib              â† MinMaxScaler individual\n",
    "#   encoders.joblib            â† diccionario de encoders\n",
    "# =============================================================================\n",
    "\n",
    "import joblib\n",
    "import pickle\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# â”€â”€ Construir el objeto completo del pipeline â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "pipeline_artefacts = {\n",
    "    # Transformadores\n",
    "    \"scaler\"             : scaler,        # MinMaxScaler fitted\n",
    "    \"encoders\"           : encoders,      # dict {col: encoder} (OrdinalEncoder, LabelEncoder)\n",
    "\n",
    "    # Metadata del pipeline\n",
    "    \"independent_vars\"   : EXPECTED_X_FINAL,\n",
    "    \"target_vars\"        : EXPECTED_Y_FINAL,\n",
    "    \"score_cols\"         : SCORE_COLS,\n",
    "    \"asset_cols\"         : ASSET_COLS,\n",
    "    \"yn_cols\"            : YN_COLS,\n",
    "    \"na_values\"          : NA_VALUES,\n",
    "    \"columns_to_drop\"    : COLUMNS_TO_DROP,\n",
    "\n",
    "    # InformaciÃ³n del dataset\n",
    "    \"n_train_rows\"       : len(df_final),\n",
    "    \"n_features\"         : len(EXPECTED_X_FINAL),\n",
    "    \"n_targets\"          : len(EXPECTED_Y_FINAL),\n",
    "    \"feature_range\"      : (0, 1),\n",
    "    \"created_at\"         : datetime.now().isoformat(),\n",
    "    \"dataset_version\"    : \"Resultados_Ãºnicos_Saber_11_20260224\",\n",
    "}\n",
    "\n",
    "# â”€â”€ Serializar con joblib â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "OUTPUT_DIR = \".\"   # Cambiar a ruta de Google Drive si se desea persistencia\n",
    "\n",
    "artefacts_path = os.path.join(OUTPUT_DIR, \"pipeline_artefacts.joblib\")\n",
    "scaler_path    = os.path.join(OUTPUT_DIR, \"scaler.joblib\")\n",
    "encoders_path  = os.path.join(OUTPUT_DIR, \"encoders.joblib\")\n",
    "\n",
    "joblib.dump(pipeline_artefacts, artefacts_path, compress=3)\n",
    "joblib.dump(scaler,             scaler_path,    compress=3)\n",
    "joblib.dump(encoders,           encoders_path,  compress=3)\n",
    "\n",
    "print(\"âœ… Artefactos serializados con joblib:\")\n",
    "print(f\"   ğŸ“¦ pipeline_artefacts.joblib  â†’ {os.path.getsize(artefacts_path)/1024:.1f} KB\")\n",
    "print(f\"   ğŸ“¦ scaler.joblib              â†’ {os.path.getsize(scaler_path)/1024:.1f} KB\")\n",
    "print(f\"   ğŸ“¦ encoders.joblib            â†’ {os.path.getsize(encoders_path)/1024:.1f} KB\")\n",
    "\n",
    "# â”€â”€ Verificar que los artefactos se pueden cargar correctamente â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "loaded = joblib.load(artefacts_path)\n",
    "\n",
    "assert loaded[\"n_features\"]     == len(EXPECTED_X_FINAL), \"Error: n_features no coincide\"\n",
    "assert loaded[\"n_train_rows\"]   == len(df_final),          \"Error: n_train_rows no coincide\"\n",
    "assert len(loaded[\"encoders\"])  == len(encoders),          \"Error: encoders no coinciden\"\n",
    "\n",
    "print(\"\\nâœ… VerificaciÃ³n de integridad: todos los artefactos cargados correctamente.\")\n",
    "print(f\"\\nâ”€â”€ Contenido de pipeline_artefacts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "for k, v in loaded.items():\n",
    "    if isinstance(v, (list, dict)):\n",
    "        print(f\"   {k:<25}: {type(v).__name__} con {len(v)} elementos\")\n",
    "    elif hasattr(v, '__class__'):\n",
    "        print(f\"   {k:<25}: {type(v).__name__}\")\n",
    "    else:\n",
    "        print(f\"   {k:<25}: {v}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4031ce0",
   "metadata": {},
   "source": [
    "### âœ‚ï¸ 12.5 â€” SeparaciÃ³n definitiva X / y por tipo de problema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3586543f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 12.5  SEPARACIÃ“N X / y POR TIPO DE PROBLEMA DE MODELADO\n",
    "#\n",
    "# Se generan distintas versiones de y segÃºn el tipo de modelo a entrenar:\n",
    "#\n",
    "#   y_regresion  â†’ Puntajes numÃ©ricos continuos (PUNT_*)\n",
    "#                  Modelos: Linear Regression, Random Forest Regressor, XGBoost\n",
    "#\n",
    "#   y_clasificacion â†’ DESEMP_INGLES codificado ordinalmente (A- â†’ B+)\n",
    "#                     Modelos: Logistic Regression, Random Forest Classifier\n",
    "#\n",
    "#   y_multioutput â†’ Todos los targets juntos para regresiÃ³n multisalida\n",
    "#                   Modelos: MultiOutputRegressor, Neural Network\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# â”€â”€ X final (variables independientes escaladas) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "X_final = df_final[[c for c in EXPECTED_X_FINAL if c in df_final.columns]].copy()\n",
    "\n",
    "# â”€â”€ y â€“ RegresiÃ³n: puntajes numÃ©ricos â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "score_targets = [c for c in SCORE_COLS if c in df_final.columns]\n",
    "y_regresion   = df_final[score_targets].copy()\n",
    "\n",
    "# â”€â”€ y â€“ ClasificaciÃ³n: desempeÃ±o en inglÃ©s â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "desemp_target = [c for c in [\"DESEMP_INGLES\"] if c in df_final.columns]\n",
    "y_clasificacion = df_final[desemp_target].copy() if desemp_target else None\n",
    "\n",
    "# â”€â”€ y â€“ Multioutput: todos los targets disponibles â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "y_multioutput = df_final[[c for c in EXPECTED_Y_FINAL if c in df_final.columns]].copy()\n",
    "\n",
    "# â”€â”€ Reporte de splits â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"=\" * 65)\n",
    "print(\"  SPLITS PARA MODELADO\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "print(f\"\\nğŸ“Š X_final         : {X_final.shape}  â† Variables independientes escaladas [0,1]\")\n",
    "print(f\"\\nğŸ“Š y_regresion     : {y_regresion.shape}\")\n",
    "for c in score_targets:\n",
    "    stats = y_regresion[c].describe()\n",
    "    print(f\"   â€¢ {c:<35} media={stats['mean']:.1f}  std={stats['std']:.1f}\")\n",
    "\n",
    "if y_clasificacion is not None:\n",
    "    print(f\"\\nğŸ“Š y_clasificacion : {y_clasificacion.shape}\")\n",
    "    print(f\"   â€¢ DESEMP_INGLES  â†’ {int(y_clasificacion['DESEMP_INGLES'].nunique())} clases\")\n",
    "    print(f\"   DistribuciÃ³n de clases:\")\n",
    "    desemp_dist = y_clasificacion[\"DESEMP_INGLES\"].value_counts(normalize=True).sort_index() * 100\n",
    "    print(desemp_dist.round(2).to_string())\n",
    "\n",
    "print(f\"\\nğŸ“Š y_multioutput   : {y_multioutput.shape}  â† Todos los targets\")\n",
    "\n",
    "# â”€â”€ Verificar alineaciÃ³n de Ã­ndices â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "assert len(X_final) == len(y_multioutput), \"âŒ X e y tienen diferente nÃºmero de filas\"\n",
    "print(f\"\\nâœ… AlineaciÃ³n X / y verificada: {len(X_final):,} registros en ambos.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bef65e",
   "metadata": {},
   "source": [
    "### ğŸ“¤ 12.6 â€” ExportaciÃ³n del dataset y artefactos listos para modelado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc76392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 12.6  EXPORTACIÃ“N FINAL DE TODOS LOS ARTEFACTOS\n",
    "#\n",
    "# Estructura de archivos generados:\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#  icfes_X_final.parquet          â† Variables independientes (escaladas)\n",
    "#  icfes_y_regresion.parquet      â† Targets numÃ©ricos (PUNT_*)\n",
    "#  icfes_y_clasificacion.parquet  â† Target categÃ³rico (DESEMP_INGLES)\n",
    "#  icfes_y_multioutput.parquet    â† Todos los targets\n",
    "#  icfes_dataset_completo.parquet â† Dataset completo X + y\n",
    "#  pipeline_artefacts.joblib      â† Scaler + Encoders + Metadata\n",
    "# =============================================================================\n",
    "\n",
    "import joblib, os\n",
    "import pandas as pd\n",
    "\n",
    "exports = {\n",
    "    \"icfes_X_final.parquet\"         : X_final,\n",
    "    \"icfes_y_regresion.parquet\"     : y_regresion,\n",
    "    \"icfes_y_multioutput.parquet\"   : y_multioutput,\n",
    "    \"icfes_dataset_completo.parquet\": df_final,\n",
    "}\n",
    "if y_clasificacion is not None:\n",
    "    exports[\"icfes_y_clasificacion.parquet\"] = y_clasificacion\n",
    "\n",
    "print(\"â”€â”€ Exportando archivos Parquet â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "for fname, data in exports.items():\n",
    "    try:\n",
    "        data.to_parquet(fname, index=False, compression=\"snappy\")\n",
    "        size_kb = os.path.getsize(fname) / 1024\n",
    "        print(f\"   âœ… {fname:<45} {data.shape}  {size_kb:>7.1f} KB\")\n",
    "    except Exception as e:\n",
    "        data.to_csv(fname.replace(\".parquet\", \".csv\"), index=False)\n",
    "        print(f\"   âš ï¸  {fname} (Parquet fallÃ³, exportado como CSV): {e}\")\n",
    "\n",
    "print(\"\\nâ”€â”€ Exportando CSVs de respaldo â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "X_final.to_csv(\"icfes_X_final.csv\",          index=False, encoding=\"utf-8\")\n",
    "y_regresion.to_csv(\"icfes_y_regresion.csv\",   index=False, encoding=\"utf-8\")\n",
    "y_multioutput.to_csv(\"icfes_y_multioutput.csv\", index=False, encoding=\"utf-8\")\n",
    "print(\"   âœ… CSVs exportados.\")\n",
    "\n",
    "print(\"\\nâ”€â”€ Artefactos del pipeline (ya serializados en 12.4) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "for f in [\"pipeline_artefacts.joblib\", \"scaler.joblib\", \"encoders.joblib\"]:\n",
    "    if os.path.exists(f):\n",
    "        print(f\"   âœ… {f:<45} {os.path.getsize(f)/1024:>7.1f} KB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536b2a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 12.7  RESUMEN VISUAL â€” ESTADO FINAL DEL DATASET\n",
    "# =============================================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "gs  = gridspec.GridSpec(2, 3, figure=fig, hspace=0.45, wspace=0.35)\n",
    "\n",
    "# â”€â”€ Panel 1: DistribuciÃ³n de X_final (boxplot escalado) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ax1 = fig.add_subplot(gs[0, :2])\n",
    "X_final.boxplot(ax=ax1, vert=False, patch_artist=True,\n",
    "                boxprops=dict(facecolor=\"#4C72B0\", alpha=0.6),\n",
    "                medianprops=dict(color=\"#C44E52\", lw=2))\n",
    "ax1.set_title(\"Variables X â€” DistribuciÃ³n tras MinMaxScaler [0, 1]\", fontsize=11, fontweight=\"bold\")\n",
    "ax1.set_xlabel(\"Valor escalado\")\n",
    "ax1.axvline(0, color=\"gray\", lw=0.8, ls=\"--\")\n",
    "ax1.axvline(1, color=\"gray\", lw=0.8, ls=\"--\")\n",
    "\n",
    "# â”€â”€ Panel 2: Heatmap de correlaciÃ³n de X_final â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ax2 = fig.add_subplot(gs[0, 2])\n",
    "corr_X = X_final.corr(method=\"pearson\")\n",
    "mask_upper = np.triu(np.ones_like(corr_X, dtype=bool), k=1)\n",
    "sns.heatmap(corr_X, ax=ax2, cmap=\"coolwarm\", center=0,\n",
    "            vmin=-1, vmax=1, linewidths=0.3,\n",
    "            cbar_kws={\"shrink\": 0.7, \"label\": \"r\"},\n",
    "            xticklabels=True, yticklabels=True)\n",
    "ax2.set_title(\"CorrelaciÃ³n Pearson X_final\", fontsize=11, fontweight=\"bold\")\n",
    "ax2.tick_params(axis=\"both\", labelsize=7)\n",
    "\n",
    "# â”€â”€ Panel 3: DistribuciÃ³n de targets numÃ©ricos â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ax3 = fig.add_subplot(gs[1, :2])\n",
    "y_plot = y_regresion[[c for c in score_targets if c in y_regresion.columns]]\n",
    "y_plot.boxplot(ax=ax3, vert=False, patch_artist=True,\n",
    "               boxprops=dict(facecolor=\"#55A868\", alpha=0.6),\n",
    "               medianprops=dict(color=\"#C44E52\", lw=2))\n",
    "ax3.set_title(\"Variables y (targets) â€” Puntajes sin escalar\", fontsize=11, fontweight=\"bold\")\n",
    "ax3.set_xlabel(\"Puntaje\")\n",
    "\n",
    "# â”€â”€ Panel 4: Nulos en y â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ax4 = fig.add_subplot(gs[1, 2])\n",
    "null_y_pct = (y_multioutput.isnull().mean() * 100)\n",
    "colors_null = [\"#C44E52\" if v > 0 else \"#55A868\" for v in null_y_pct]\n",
    "ax4.barh(null_y_pct.index, null_y_pct.values, color=colors_null, edgecolor=\"white\")\n",
    "ax4.set_title(\"Nulos en variables y (%)\", fontsize=11, fontweight=\"bold\")\n",
    "ax4.set_xlabel(\"% de nulos\")\n",
    "ax4.axvline(0, color=\"gray\", lw=0.8)\n",
    "for i, v in enumerate(null_y_pct.values):\n",
    "    ax4.text(v + 0.1, i, f\"{v:.1f}%\", va=\"center\", fontsize=8)\n",
    "\n",
    "fig.suptitle(\"Estado Final del Dataset â€” Listo para Modelado\", fontsize=14, y=1.01, fontweight=\"bold\")\n",
    "plt.savefig(\"estado_final_dataset.png\", dpi=130, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7a7017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 12.8  RESUMEN COMPLETO DEL PIPELINE â€” TODO EL PROYECTO\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=\" * 72)\n",
    "print(\"  PIPELINE COMPLETO ICFES SABER 11 â€” RESUMEN FINAL\")\n",
    "print(\"=\" * 72)\n",
    "\n",
    "pipeline_summary = [\n",
    "    # (SecciÃ³n, DescripciÃ³n, Resultado)\n",
    "    (\"Â§ 2\",  \"Carga de datos\",              f\"{df_raw.shape[0]:,} filas Ã— {df_raw.shape[1]} cols\"),\n",
    "    (\"Â§ 3\",  \"EDA\",                         \"Distribuciones, nulos, estadÃ­sticas\"),\n",
    "    (\"Â§ 4\",  \"Limpieza\",                    \"Tipos â†’ category, normalizaciÃ³n strings\"),\n",
    "    (\"Â§ 5\",  \"ImputaciÃ³n\",                  \"Mediana (num) | Moda grupal (cat)\"),\n",
    "    (\"Â§ 6\",  \"CorrelaciÃ³n\",                 \"Pearson + CramÃ©r's V documentados\"),\n",
    "    (\"Â§ 7\",  \"Feature Engineering\",         \"EDAD, ANIO, TRIMESTRE, INDICE_BIENES\"),\n",
    "    (\"Â§ 8\",  \"CodificaciÃ³n\",                f\"OrdinalEncoder ({len(ORDINAL_VARS)}) + LabelEncoder ({len(LABEL_VARS)})\"),\n",
    "    (\"Â§ 9\",  \"Escalado MinMaxScaler\",        f\"{len(EXPECTED_X_FINAL)} vars â†’ [0, 1]\"),\n",
    "    (\"Â§ 10\", \"Dataset procesado\",           f\"{df_final.shape[0]:,} filas\"),\n",
    "    (\"Â§ 11\", \"ValidaciÃ³n de calidad\",       \"10 reglas evaluadas\"),\n",
    "    (\"Â§ 12.1\",\"Limpieza estructural\",        \"Solo vars escaladas + targets\"),\n",
    "    (\"Â§ 12.2\",\"CodificaciÃ³n DESEMP_INGLES\",  \"OrdinalEncoder MCER (A- â†’ B+)\"),\n",
    "    (\"Â§ 12.3\",\"AuditorÃ­a final\",            \"7 checks: tipos, nulos, rangos, dups\"),\n",
    "    (\"Â§ 12.4\",\"SerializaciÃ³n\",              \"pipeline_artefacts.joblib\"),\n",
    "    (\"Â§ 12.5\",\"Splits X/y\",                f\"X: {X_final.shape} | y_reg: {y_regresion.shape}\"),\n",
    "    (\"Â§ 12.6\",\"ExportaciÃ³n\",               \"Parquet + CSV + Joblib\"),\n",
    "]\n",
    "\n",
    "for sec, desc, result in pipeline_summary:\n",
    "    print(f\"  âœ…  {sec:<8} {desc:<32} â†’ {result}\")\n",
    "\n",
    "print(\"=\" * 72)\n",
    "print(f\"\\n  ğŸ“ Dataset final listo para modelado:\")\n",
    "print(f\"     X_final         : {X_final.shape[0]:>10,} filas Ã— {X_final.shape[1]:>2} variables independientes\")\n",
    "print(f\"     y_regresion     : {y_regresion.shape[0]:>10,} filas Ã— {y_regresion.shape[1]:>2} targets continuos\")\n",
    "if y_clasificacion is not None:\n",
    "    print(f\"     y_clasificacion : {y_clasificacion.shape[0]:>10,} filas Ã— {y_clasificacion.shape[1]:>2} targets categÃ³ricos\")\n",
    "print(f\"     y_multioutput   : {y_multioutput.shape[0]:>10,} filas Ã— {y_multioutput.shape[1]:>2} targets totales\")\n",
    "\n",
    "print(f\"\\n  ğŸ§  Modelos recomendados para siguiente paso:\")\n",
    "print(f\"     RegresiÃ³n  â†’ LinearRegression, RandomForestRegressor, XGBoostRegressor\")\n",
    "print(f\"     Clasificac.â†’ LogisticRegression, RandomForestClassifier, XGBoostClassifier\")\n",
    "print(f\"     Multioutputâ†’ MultiOutputRegressor, MLPRegressor (Red Neuronal)\")\n",
    "\n",
    "print(f\"\\n  â±ï¸  Pipeline generado: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\" * 72)\n",
    "print(\"  ğŸš€ Â¡Dataset 100% listo para entrenar modelos de Machine Learning!\")\n",
    "print(\"=\" * 72)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f54154",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ¤– 13. Modelado â€” 6 Modelos Ã— 6 Variables a Predecir\n",
    "\n",
    "Para cada **variable objetivo** se entrenan **6 modelos de regresiÃ³n** de forma independiente.\n",
    "Esto genera **36 experimentos** que permiten identificar quÃ© modelo es Ã³ptimo para cada puntaje.\n",
    "\n",
    "| # | Modelo | Tipo |\n",
    "|---|--------|------|\n",
    "| M1 | **Ridge** | Lineal regularizado (baseline) |\n",
    "| M2 | **KNN Regressor** | Basado en distancias |\n",
    "| M3 | **Ãrbol de DecisiÃ³n** | Ãrbol Ãºnico, interpretable |\n",
    "| M4 | **MLP** | Red neuronal multicapa |\n",
    "| M5 | **Random Forest** | Ensemble de Ã¡rboles (bagging) |\n",
    "| M6 | **HistGradientBoosting** | Ensemble secuencial (boosting) |\n",
    "\n",
    "**Variables objetivo (6):**\n",
    "`PUNT_GLOBAL` Â· `PUNT_MATEMATICAS` Â· `PUNT_INGLES` Â· `PUNT_LECTURA_CRITICA` Â· `PUNT_C_NATURALES` Â· `PUNT_SOCIALES_CIUDADANAS`\n",
    "\n",
    "**MÃ©tricas:** RÂ² Â· RMSE Â· MAE Â· MAPE Â· Cross-Validation K=5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb02dea6",
   "metadata": {},
   "source": [
    "### âš™ï¸ 13.1 â€” Importaciones y configuraciÃ³n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d161fe",
   "metadata": {},
   "outputs": [],
   "source": "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# 13.1  LIBRERIAS DE MODELADO â€” 10 modelos\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nimport copy, time, warnings\nimport numpy  as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.linear_model   import Ridge\nfrom sklearn.neighbors      import KNeighborsRegressor\nfrom sklearn.tree           import DecisionTreeRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.svm            import SVR\nfrom sklearn.ensemble       import (\n    RandomForestRegressor,\n    HistGradientBoostingRegressor,\n    GradientBoostingRegressor,\n    VotingRegressor,\n)\nfrom sklearn.model_selection import (\n    train_test_split, KFold, cross_val_score,\n    HalvingGridSearchCV\n)\nfrom sklearn.metrics import (\n    r2_score, mean_squared_error,\n    mean_absolute_error, mean_absolute_percentage_error\n)\nfrom sklearn.inspection import permutation_importance\n\nimport joblib, os\n\nRANDOM_STATE = 42\nN_FOLDS      = 5\n\nprint(\"Librerias cargadas.\")\n"
  },
  {
   "cell_type": "markdown",
   "id": "5088307f",
   "metadata": {},
   "source": [
    "### ğŸ—‚ï¸ 13.2 â€” DefiniciÃ³n de los 6 modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ebe79a",
   "metadata": {},
   "outputs": [],
   "source": "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# 13.2  CATALOGO DE 10 MODELOS\n#\n# M1_Ridge    Ridge (Baseline)\n# M2_KNN      KNN Regressor\n# M3_DTree    Arbol de Decision\n# M4_MLP      MLP (128-64-32)\n# M5_RF       Random Forest\n# M6_HGB      HistGradientBoosting\n# M7_GBR      GradientBoosting\n# M8_SVR      SVR (omitido si dataset grande)\n# M9_Deep     Red Neuronal Profunda (256-128-64-32)\n# M10_Voting  Voting Regressor (RF + HGB + GBR)\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n# SVR es lento con datasets grandes - omitir si hay mas de 20k filas\nSVR_SKIP = len(X_final) > 20_000\nif SVR_SKIP:\n    print(f\"  SVR omitido: dataset tiene {len(X_final):,} filas (> 20,000)\")\n\n# Voting base: RF + HGB + GBR (estimadores ligeros para que sea ejecutable)\n_rf_base  = RandomForestRegressor(n_estimators=80, max_depth=10,\n                                   min_samples_leaf=5, n_jobs=-1,\n                                   random_state=RANDOM_STATE)\n_hgb_base = HistGradientBoostingRegressor(max_iter=100, learning_rate=0.1,\n                                            max_depth=6, random_state=RANDOM_STATE)\n_gbr_base = GradientBoostingRegressor(n_estimators=80, learning_rate=0.1,\n                                       max_depth=4, random_state=RANDOM_STATE)\n\nMODELOS = [\n    (\n        \"M1_Ridge\",\n        \"Ridge (Baseline)\",\n        Ridge(alpha=1.0)\n    ),\n    (\n        \"M2_KNN\",\n        \"KNN Regressor\",\n        KNeighborsRegressor(\n            n_neighbors=7, weights=\"distance\",\n            metric=\"euclidean\", n_jobs=-1\n        )\n    ),\n    (\n        \"M3_DTree\",\n        \"Arbol de Decision\",\n        DecisionTreeRegressor(\n            max_depth=8, min_samples_leaf=20,\n            random_state=RANDOM_STATE\n        )\n    ),\n    (\n        \"M4_MLP\",\n        \"MLP (128-64-32)\",\n        MLPRegressor(\n            hidden_layer_sizes=(128, 64, 32),\n            activation=\"relu\", max_iter=500,\n            early_stopping=True, validation_fraction=0.1,\n            random_state=RANDOM_STATE\n        )\n    ),\n    (\n        \"M5_RF\",\n        \"Random Forest\",\n        RandomForestRegressor(\n            n_estimators=150, max_depth=12,\n            min_samples_leaf=5, n_jobs=-1,\n            random_state=RANDOM_STATE\n        )\n    ),\n    (\n        \"M6_HGB\",\n        \"HistGradientBoosting\",\n        HistGradientBoostingRegressor(\n            max_iter=200, learning_rate=0.05,\n            max_depth=8, random_state=RANDOM_STATE\n        )\n    ),\n    (\n        \"M7_GBR\",\n        \"GradientBoosting\",\n        GradientBoostingRegressor(\n            n_estimators=150, learning_rate=0.05,\n            max_depth=5, subsample=0.8,\n            random_state=RANDOM_STATE\n        )\n    ),\n    (\n        \"M8_SVR\",\n        \"SVR\" if not SVR_SKIP else \"SVR (omitido)\",\n        SVR(C=1.0, epsilon=0.1, kernel=\"rbf\") if not SVR_SKIP else None\n    ),\n    (\n        \"M9_Deep\",\n        \"Red Neuronal Profunda\",\n        MLPRegressor(\n            hidden_layer_sizes=(256, 128, 64, 32),\n            activation=\"relu\", max_iter=600,\n            early_stopping=True, validation_fraction=0.1,\n            learning_rate_init=0.001,\n            random_state=RANDOM_STATE\n        )\n    ),\n    (\n        \"M10_Voting\",\n        \"Voting Regressor\",\n        VotingRegressor(estimators=[\n            (\"rf\",  _rf_base),\n            (\"hgb\", _hgb_base),\n            (\"gbr\", _gbr_base),\n        ], n_jobs=-1)\n    ),\n]\n\nTARGETS = [t for t in [\n    \"PUNT_GLOBAL\", \"PUNT_MATEMATICAS\", \"PUNT_INGLES\",\n    \"PUNT_LECTURA_CRITICA\", \"PUNT_C_NATURALES\", \"PUNT_SOCIALES_CIUDADANAS\"\n] if t in y_regresion.columns]\n\nn_efectivos = sum(1 for _,_,m in MODELOS if m is not None)\nprint(f\"{n_efectivos} modelos efectivos x {len(TARGETS)} targets = {n_efectivos*len(TARGETS)} experimentos\")\nprint()\nprint(\"  Modelos:\")\nfor mid, mname, m in MODELOS:\n    estado = \"OMITIDO\" if m is None else \"OK\"\n    print(f\"    {mid:<12}  {mname:<30}  [{estado}]\")\nprint()\nprint(\"  Targets:\")\nfor t in TARGETS:\n    s = y_regresion[t].dropna()\n    print(f\"    {t:<35}  mu={s.mean():.1f}  sigma={s.std():.1f}  n={len(s):,}\")\n"
  },
  {
   "cell_type": "markdown",
   "id": "46cb274d",
   "metadata": {},
   "source": [
    "### âœ‚ï¸ 13.3 â€” ParticiÃ³n Train / Test (80 / 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa431c07",
   "metadata": {},
   "outputs": [],
   "source": "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# 13.3  PARTICION UNICA 70/30\n#\n# Una sola particion 70/30, compartida por todos los modelos y targets.\n# Garantiza comparaciones justas: mismo conjunto de test siempre.\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\nX_tr, X_te, y_tr_all, y_te_all = train_test_split(\n    X_final, y_regresion,\n    test_size=0.30, random_state=RANDOM_STATE, shuffle=True\n)\n\nprint(f\"  Train : {len(X_tr):>10,} filas  ({len(X_tr)/len(X_final)*100:.1f} %)\")\nprint(f\"  Test  : {len(X_te):>10,} filas  ({len(X_te)/len(X_final)*100:.1f} %)\")\nprint(f\"  X vars: {X_tr.shape[1]}\")\nprint()\n\n# Verificar distribuciÃ³n similar entre train y test\nprint(f\"  {'Target':<35}  {'Î¼_train':>8}  {'Î¼_test':>8}  {'Î”':>6}  Estado\")\nprint(f\"  {'â”€'*35}  {'â”€'*8}  {'â”€'*8}  {'â”€'*6}  â”€â”€â”€â”€â”€â”€\")\nfor t in TARGETS:\n    m_tr = y_tr_all[t].dropna().mean()\n    m_te = y_te_all[t].dropna().mean()\n    dif  = abs(m_tr - m_te)\n    flag = \"âœ…\" if dif < 1.5 else \"âš ï¸\"\n    print(f\"  {flag} {t:<33}  {m_tr:>8.2f}  {m_te:>8.2f}  {dif:>6.2f}\")\n"
  },
  {
   "cell_type": "markdown",
   "id": "defef9a3",
   "metadata": {},
   "source": [
    "### ğŸ”§ 13.4 â€” FunciÃ³n de entrenamiento y evaluaciÃ³n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4826bcbb",
   "metadata": {},
   "outputs": [],
   "source": "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# 13.4  entrenar_modelo()\n#\n# Entrena un modelo para un target especÃ­fico y retorna:\n#   - RÂ², RMSE, MAE, MAPE en test\n#   - RÂ² en Cross-Validation (K=5 folds)\n#   - Tiempo de entrenamiento\n#   - Modelo ya ajustado, predicciones y valores reales\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\ndef entrenar_modelo(model_id, model_name, model,\n                    target, X_train, X_test, y_train, y_test):\n    # Modelo omitido (ej. SVR con dataset grande)\n    if model is None:\n        return None\n    # Extraer serie y alinear Ã­ndices (target puede tener NaN)\n    y_tr = y_train[target].dropna()\n    y_te = y_test[target].dropna()\n    X_tr = X_train.loc[y_tr.index]\n    X_te = X_test.loc[y_te.index]\n\n    # Clonar para no contaminar experimentos posteriores\n    m  = copy.deepcopy(model)\n    t0 = time.time()\n    m.fit(X_tr, y_tr)\n    t_fit = round(time.time() - t0, 2)\n\n    # MÃ©tricas en test\n    y_pred = m.predict(X_te)\n    y_real = y_te.values\n\n    r2   = round(r2_score(y_real, y_pred), 4)\n    rmse = round(np.sqrt(mean_squared_error(y_real, y_pred)), 4)\n    mae  = round(mean_absolute_error(y_real, y_pred), 4)\n    mape = round(mean_absolute_percentage_error(y_real, y_pred) * 100, 2)\n\n    # Cross-Validation sobre train\n    kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n    cv = cross_val_score(\n        copy.deepcopy(model), X_tr, y_tr,\n        cv=kf, scoring=\"r2\", n_jobs=-1\n    )\n    cv_mean = round(float(cv.mean()), 4)\n    cv_std  = round(float(cv.std()),  4)\n\n    return {\n        \"model_id\"   : model_id,\n        \"model_name\" : model_name,\n        \"target\"     : target,\n        \"r2_test\"    : r2,\n        \"rmse_test\"  : rmse,\n        \"mae_test\"   : mae,\n        \"mape_test\"  : mape,\n        \"r2_cv_mean\" : cv_mean,\n        \"r2_cv_std\"  : cv_std,\n        \"r2_cv_fold\" : list(cv),\n        \"tiempo_s\"   : t_fit,\n        # Para visualizaciones posteriores\n        \"modelo_fit\" : m,\n        \"y_pred\"     : y_pred,\n        \"y_real\"     : y_real,\n    }\n\nprint(\"âœ… entrenar_modelo() lista.\")\n"
  },
  {
   "cell_type": "markdown",
   "id": "1cd268f8",
   "metadata": {},
   "source": [
    "---\n",
    "### ğŸ”„ 13.5 â€” Entrenamiento: un bloque por variable objetivo\n",
    "\n",
    "Cada celda entrena los **6 modelos** para una variable objetivo especÃ­fica\n",
    "y muestra los resultados inmediatamente, permitiendo anÃ¡lisis progresivo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa3b5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Almacenes globales\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "RESULTADOS  = []                          # lista â†’ DataFrame al final\n",
    "MODELOS_FIT = {t: {} for t in TARGETS}   # {target: {model_id: resultado_completo}}\n",
    "\n",
    "def entrenar_bloque(target):\n",
    "    \"\"\"Entrena los 6 modelos para `target` e imprime resultados.\"\"\"\n",
    "    y_tr_t = y_tr_all[[target]]\n",
    "    y_te_t = y_te_all[[target]]\n",
    "\n",
    "    n_tr = y_tr_t[target].dropna().__len__()\n",
    "    n_te = y_te_t[target].dropna().__len__()\n",
    "\n",
    "    print(f\"\\n{'â•'*68}\")\n",
    "    print(f\"  ğŸ¯  {target}  |  train={n_tr:,}  test={n_te:,}\")\n",
    "    print(f\"{'â•'*68}\")\n",
    "    print(f\"  {'#':<3}  {'Modelo':<26}  {'RÂ²_test':>8}  {'RMSE':>7}  \"\n",
    "          f\"{'MAE':>7}  {'RÂ²_CV (Î¼Â±Ïƒ)':>14}  {'t(s)':>5}\")\n",
    "    print(f\"  {'â”€'*3}  {'â”€'*26}  {'â”€'*8}  {'â”€'*7}  {'â”€'*7}  {'â”€'*14}  {'â”€'*5}\")\n",
    "\n",
    "    for k, (mid, mname, model) in enumerate(MODELOS, 1):\n",
    "        res = entrenar_modelo(\n",
    "            mid, mname, model, target,\n",
    "            X_tr, X_te, y_tr_t, y_te_t\n",
    "        )\n",
    "        MODELOS_FIT[target][mid] = res\n",
    "        # Guardar versiÃ³n sin arrays para el DataFrame\n",
    "        row = {kk: vv for kk, vv in res.items()\n",
    "               if kk not in (\"modelo_fit\", \"y_pred\", \"y_real\", \"r2_cv_fold\")}\n",
    "        RESULTADOS.append(row)\n",
    "\n",
    "        cv_str = f\"{res['r2_cv_mean']:.4f}Â±{res['r2_cv_std']:.4f}\"\n",
    "        print(f\"  {k:<3}  {mname:<26}  {res['r2_test']:>8.4f}  \"\n",
    "              f\"{res['rmse_test']:>7.3f}  {res['mae_test']:>7.3f}  \"\n",
    "              f\"{cv_str:>14}  {res['tiempo_s']:>5.1f}\")\n",
    "\n",
    "print(\"âœ… entrenar_bloque() lista. Ejecuta las celdas siguientes.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a7a4dc",
   "metadata": {},
   "source": [
    "#### ğŸ“ Bloque 1/6 â€” `PUNT_GLOBAL`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1461892",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"PUNT_GLOBAL\" in TARGETS:\n",
    "    entrenar_bloque(\"PUNT_GLOBAL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807f50bb",
   "metadata": {},
   "source": [
    "#### â• Bloque 2/6 â€” `PUNT_MATEMATICAS`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0686f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"PUNT_MATEMATICAS\" in TARGETS:\n",
    "    entrenar_bloque(\"PUNT_MATEMATICAS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592e4e6b",
   "metadata": {},
   "source": [
    "#### ğŸ‡¬ğŸ‡§ Bloque 3/6 â€” `PUNT_INGLES`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1299954",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"PUNT_INGLES\" in TARGETS:\n",
    "    entrenar_bloque(\"PUNT_INGLES\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b154b214",
   "metadata": {},
   "source": [
    "#### ğŸ“– Bloque 4/6 â€” `PUNT_LECTURA_CRITICA`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79aed26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"PUNT_LECTURA_CRITICA\" in TARGETS:\n",
    "    entrenar_bloque(\"PUNT_LECTURA_CRITICA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13983e89",
   "metadata": {},
   "source": [
    "#### ğŸ”¬ Bloque 5/6 â€” `PUNT_C_NATURALES`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62be9216",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"PUNT_C_NATURALES\" in TARGETS:\n",
    "    entrenar_bloque(\"PUNT_C_NATURALES\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c728d6",
   "metadata": {},
   "source": [
    "#### ğŸ›ï¸ Bloque 6/6 â€” `PUNT_SOCIALES_CIUDADANAS`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d877e0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"PUNT_SOCIALES_CIUDADANAS\" in TARGETS:\n",
    "    entrenar_bloque(\"PUNT_SOCIALES_CIUDADANAS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea99d58a",
   "metadata": {},
   "source": [
    "### ğŸ“Š 13.6 â€” Tabla maestra de resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c49d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 13.6  CONSTRUIR DATAFRAME MAESTRO Y MOSTRAR RANKING POR TARGET\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "df_res = (\n",
    "    pd.DataFrame(RESULTADOS)\n",
    "    .sort_values([\"target\", \"r2_test\"], ascending=[True, False])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(f\"DataFrame maestro: {df_res.shape[0]} filas Ã— {df_res.shape[1]} cols\\n\")\n",
    "\n",
    "RENAME = {\n",
    "    \"model_name\": \"Modelo\",\n",
    "    \"r2_test\"   : \"RÂ² Test\",\n",
    "    \"r2_cv_mean\": \"RÂ² CV Î¼\",\n",
    "    \"r2_cv_std\" : \"RÂ² CV Ïƒ\",\n",
    "    \"rmse_test\" : \"RMSE\",\n",
    "    \"mae_test\"  : \"MAE\",\n",
    "    \"mape_test\" : \"MAPE %\",\n",
    "    \"tiempo_s\"  : \"t (s)\",\n",
    "}\n",
    "\n",
    "for target in TARGETS:\n",
    "    sub = (df_res[df_res[\"target\"] == target]\n",
    "           .sort_values(\"r2_test\", ascending=False)\n",
    "           .reset_index(drop=True))\n",
    "    sub.index += 1\n",
    "\n",
    "    print(f\"\\n{'â•'*60}\")\n",
    "    print(f\"  ğŸ¯  {target}\")\n",
    "    print(f\"{'â•'*60}\")\n",
    "    display(\n",
    "        sub[list(RENAME)].rename(columns=RENAME)\n",
    "        .style\n",
    "        .background_gradient(subset=[\"RÂ² Test\",\"RÂ² CV Î¼\"], cmap=\"YlGn\",  vmin=0, vmax=1)\n",
    "        .background_gradient(subset=[\"RMSE\",\"MAE\"],         cmap=\"YlOrRd_r\")\n",
    "        .format({\n",
    "            \"RÂ² Test\":\"{:.4f}\", \"RÂ² CV Î¼\":\"{:.4f}\", \"RÂ² CV Ïƒ\":\"{:.4f}\",\n",
    "            \"RMSE\":\"{:.3f}\",    \"MAE\":\"{:.3f}\",     \"MAPE %\":\"{:.2f}\",\n",
    "            \"t (s)\":\"{:.1f}\",\n",
    "        })\n",
    "        .highlight_max(subset=[\"RÂ² Test\"], color=\"#1e4d2b\")\n",
    "        .highlight_min(subset=[\"RMSE\",\"MAE\"], color=\"#1e4d2b\")\n",
    "        .set_properties(**{\"font-size\":\"11px\"})\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdb4a34",
   "metadata": {},
   "source": [
    "### ğŸ—ºï¸ 13.7 â€” Heatmaps: RÂ², RMSE y MAE (modelo Ã— target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513f235b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 13.7  HEATMAPS COMPARATIVOS\n",
    "#   Filas  = modelos  |  Columnas = targets\n",
    "#   Borde verde = mejor celda de cada columna\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "model_order = [m for _, m, _ in MODELOS]   # orden del catÃ¡logo\n",
    "\n",
    "def pivot(col):\n",
    "    p = df_res.pivot_table(\n",
    "        index=\"model_name\", columns=\"target\",\n",
    "        values=col, aggfunc=\"first\"\n",
    "    )\n",
    "    return p.reindex(index=model_order, columns=TARGETS)\n",
    "\n",
    "piv_r2   = pivot(\"r2_test\")\n",
    "piv_rmse = pivot(\"rmse_test\")\n",
    "piv_mae  = pivot(\"mae_test\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(24, 5))\n",
    "\n",
    "specs = [\n",
    "    (axes[0], piv_r2,   \"YlGn\",     \"RÂ² Test  â†‘ mejor\",    \"max\"),\n",
    "    (axes[1], piv_rmse, \"YlOrRd_r\", \"RMSE  â†“ menor mejor\", \"min\"),\n",
    "    (axes[2], piv_mae,  \"PuRd_r\",   \"MAE   â†“ menor mejor\", \"min\"),\n",
    "]\n",
    "\n",
    "for ax, piv, cmap, title, best in specs:\n",
    "    sns.heatmap(\n",
    "        piv, ax=ax, cmap=cmap,\n",
    "        annot=True, fmt=\".3f\", linewidths=0.4,\n",
    "        cbar_kws={\"shrink\": 0.75}, annot_kws={\"size\": 8.5}\n",
    "    )\n",
    "    ax.set_title(title, fontsize=11, fontweight=\"bold\", pad=10)\n",
    "    ax.set_xlabel(\"Variable objetivo\", fontsize=9)\n",
    "    ax.set_ylabel(\"Modelo\", fontsize=9)\n",
    "    ax.tick_params(axis=\"x\", rotation=35, labelsize=8)\n",
    "    ax.tick_params(axis=\"y\", rotation=0,  labelsize=8)\n",
    "    # Marcar mejor celda por columna con borde verde\n",
    "    for j, col in enumerate(piv.columns):\n",
    "        br = piv[col].idxmax() if best == \"max\" else piv[col].idxmin()\n",
    "        ri = list(piv.index).index(br)\n",
    "        ax.add_patch(plt.Rectangle(\n",
    "            (j, ri), 1, 1, fill=False,\n",
    "            edgecolor=\"#42d9a8\", lw=2.5, zorder=5\n",
    "        ))\n",
    "\n",
    "fig.suptitle(\n",
    "    \"ComparaciÃ³n de mÃ©tricas â€” 6 Modelos Ã— 6 Targets  (â–ª = mejor por columna)\",\n",
    "    fontsize=13, fontweight=\"bold\", y=1.02\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"heatmaps_metricas.png\", dpi=130, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a4cf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 13.8  BOXPLOTS DE CROSS-VALIDATION POR TARGET\n",
    "#   Muestra la distribuciÃ³n de RÂ² en los 5 folds.\n",
    "#   IQR pequeÃ±o = modelo estable.  Mediana alta = modelo preciso.\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 10))\n",
    "axes = axes.flatten()\n",
    "palette = sns.color_palette(\"Blues_d\", len(MODELOS))\n",
    "\n",
    "for i, target in enumerate(TARGETS):\n",
    "    ax     = axes[i]\n",
    "    data   = []\n",
    "    labels = []\n",
    "\n",
    "    for mid, mname, _ in MODELOS:\n",
    "        folds = MODELOS_FIT[target][mid][\"r2_cv_fold\"]\n",
    "        if folds:\n",
    "            data.append(folds)\n",
    "            labels.append(mname)\n",
    "\n",
    "    if not data:\n",
    "        ax.set_visible(False); continue\n",
    "\n",
    "    bp = ax.boxplot(data, patch_artist=True, vert=True,\n",
    "                    medianprops=dict(color=\"#e05c7a\", lw=2.5),\n",
    "                    whiskerprops=dict(color=\"#5c6280\"),\n",
    "                    capprops=dict(color=\"#5c6280\"))\n",
    "    for patch, col in zip(bp[\"boxes\"], palette):\n",
    "        patch.set_facecolor(col); patch.set_alpha(0.85)\n",
    "\n",
    "    ax.set_xticks(range(1, len(labels)+1))\n",
    "    ax.set_xticklabels(labels, fontsize=8, rotation=30, ha=\"right\")\n",
    "    ax.set_ylabel(\"RÂ²  (fold)\", fontsize=9)\n",
    "    ax.set_title(target, fontsize=10, fontweight=\"bold\")\n",
    "    ax.axhline(0, color=\"#5c6280\", lw=0.7, ls=\"--\")\n",
    "    ax.grid(axis=\"y\", alpha=0.25)\n",
    "    ax.tick_params(axis=\"y\", labelsize=8)\n",
    "\n",
    "fig.suptitle(\n",
    "    f\"Cross-Validation ({N_FOLDS} folds) â€” DistribuciÃ³n RÂ² por Modelo y Target\",\n",
    "    fontsize=13, fontweight=\"bold\", y=1.01\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"cv_boxplots.png\", dpi=130, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1145ef",
   "metadata": {},
   "source": [
    "### ğŸ† 13.9 â€” Mejor modelo por variable objetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00415aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 13.9  SELECCIÃ“N DEL MEJOR MODELO\n",
    "#   Criterio 1: RÂ² test mÃ¡ximo\n",
    "#   Criterio 2 (desempate): RMSE mÃ­nimo\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "MEJOR = {}\n",
    "\n",
    "print(f\"{'='*72}\")\n",
    "print(f\"  MEJOR MODELO POR TARGET  (criterio: RÂ² test mÃ¡ximo)\")\n",
    "print(f\"{'='*72}\")\n",
    "print(f\"  {'Target':<35}  {'Modelo':<24}  {'RÂ²':>7}  {'RMSE':>7}  {'MAE':>7}  {'MAPE%':>6}\")\n",
    "print(f\"  {'â”€'*35}  {'â”€'*24}  {'â”€'*7}  {'â”€'*7}  {'â”€'*7}  {'â”€'*6}\")\n",
    "\n",
    "for target in TARGETS:\n",
    "    sub  = (df_res[df_res[\"target\"] == target]\n",
    "            .sort_values([\"r2_test\",\"rmse_test\"], ascending=[False,True]))\n",
    "    best = sub.iloc[0]\n",
    "    mid  = best[\"model_id\"]\n",
    "    full = MODELOS_FIT[target][mid]\n",
    "\n",
    "    MEJOR[target] = {\n",
    "        \"model_id\"  : mid,\n",
    "        \"model_name\": best[\"model_name\"],\n",
    "        \"r2\"        : best[\"r2_test\"],\n",
    "        \"rmse\"      : best[\"rmse_test\"],\n",
    "        \"mae\"       : best[\"mae_test\"],\n",
    "        \"mape\"      : best[\"mape_test\"],\n",
    "        \"r2_cv\"     : best[\"r2_cv_mean\"],\n",
    "        \"r2_cv_std\" : best[\"r2_cv_std\"],\n",
    "        \"modelo_fit\": full[\"modelo_fit\"],\n",
    "        \"y_pred\"    : full[\"y_pred\"],\n",
    "        \"y_real\"    : full[\"y_real\"],\n",
    "    }\n",
    "\n",
    "    gap = sub[\"r2_test\"].iloc[0] - sub[\"r2_test\"].iloc[1]\n",
    "    print(f\"  {'ğŸ¥‡' if best['r2_test']==max(s['r2_test'] for s in sub.to_dict('records')) else '  '}\"\n",
    "          f\" {target:<33}  {best['model_name']:<24}  \"\n",
    "          f\"{best['r2_test']:>7.4f}  {best['rmse_test']:>7.3f}  \"\n",
    "          f\"{best['mae_test']:>7.3f}  {best['mape_test']:>6.2f}\"\n",
    "          f\"   Î”={gap:+.4f}\")\n",
    "\n",
    "print()\n",
    "df_mejor = pd.DataFrame([{\n",
    "    \"Target\":\"\", \"Mejor Modelo\":\"\", \"RÂ² Test\":0.,\n",
    "    \"RÂ² CV\":0., \"RMSE\":0., \"MAE\":0., \"MAPE %\":0.\n",
    "}])  # placeholder para display\n",
    "df_mejor = pd.DataFrame([{\n",
    "    \"Target\"      : t,\n",
    "    \"Mejor Modelo\": v[\"model_name\"],\n",
    "    \"RÂ² Test\"     : v[\"r2\"],\n",
    "    \"RÂ² CV\"       : v[\"r2_cv\"],\n",
    "    \"RMSE\"        : v[\"rmse\"],\n",
    "    \"MAE\"         : v[\"mae\"],\n",
    "    \"MAPE %\"      : v[\"mape\"],\n",
    "} for t, v in MEJOR.items()])\n",
    "\n",
    "display(\n",
    "    df_mejor.style\n",
    "    .background_gradient(subset=[\"RÂ² Test\",\"RÂ² CV\"], cmap=\"YlGn\",  vmin=0, vmax=1)\n",
    "    .background_gradient(subset=[\"RMSE\",\"MAE\"],       cmap=\"YlOrRd_r\")\n",
    "    .format({\"RÂ² Test\":\"{:.4f}\",\"RÂ² CV\":\"{:.4f}\",\n",
    "             \"RMSE\":\"{:.3f}\",\"MAE\":\"{:.3f}\",\"MAPE %\":\"{:.2f}\"})\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfabda72",
   "metadata": {},
   "source": [
    "### ğŸ“‰ 13.10 â€” Predicciones vs. Valores Reales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f869e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 13.10  SCATTER PREDICCIONES vs REALES â€” MEJOR MODELO POR TARGET\n",
    "#   Diagonal roja  = predicciÃ³n perfecta\n",
    "#   LÃ­nea verde    = tendencia real del modelo\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "rng       = np.random.RandomState(RANDOM_STATE)\n",
    "MAX_PTS   = 4_000\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 11))\n",
    "axes      = axes.flatten()\n",
    "\n",
    "for i, target in enumerate(TARGETS):\n",
    "    info   = MEJOR[target]\n",
    "    y_real = info[\"y_real\"]\n",
    "    y_pred = info[\"y_pred\"]\n",
    "    ax     = axes[i]\n",
    "\n",
    "    n   = min(MAX_PTS, len(y_real))\n",
    "    idx = rng.choice(len(y_real), n, replace=False)\n",
    "\n",
    "    ax.scatter(y_real[idx], y_pred[idx],\n",
    "               alpha=0.18, s=9, color=\"#4f8ef7\", rasterized=True)\n",
    "\n",
    "    lo = min(y_real.min(), y_pred.min())\n",
    "    hi = max(y_real.max(), y_pred.max())\n",
    "    ax.plot([lo,hi],[lo,hi], \"r--\", lw=1.8, alpha=0.9, label=\"Ideal  y = Å·\")\n",
    "\n",
    "    z  = np.polyfit(y_real, y_pred, 1)\n",
    "    xs = np.linspace(lo, hi, 300)\n",
    "    ax.plot(xs, np.poly1d(z)(xs), \"#42d9a8\", lw=1.5, alpha=0.85, label=\"Tendencia\")\n",
    "\n",
    "    ax.set_title(f\"{target}\\n{info['model_name']}\", fontsize=9.5, fontweight=\"bold\")\n",
    "    ax.set_xlabel(\"Valor Real\",  fontsize=8.5)\n",
    "    ax.set_ylabel(\"PredicciÃ³n\",  fontsize=8.5)\n",
    "    ax.tick_params(labelsize=7.5)\n",
    "    ax.legend(fontsize=7.5, loc=\"upper left\")\n",
    "    ax.text(0.97, 0.05,\n",
    "            f\"RÂ²  = {info['r2']:.3f}\\nRMSE = {info['rmse']:.2f}\\nMAE  = {info['mae']:.2f}\",\n",
    "            transform=ax.transAxes, ha=\"right\", va=\"bottom\", fontsize=8,\n",
    "            bbox=dict(facecolor=\"#0d0f14\", alpha=0.75, pad=4, edgecolor=\"#252a3a\"),\n",
    "            color=\"white\")\n",
    "\n",
    "fig.suptitle(\"Predicciones vs. Valores Reales â€” Mejor Modelo por Target\",\n",
    "             fontsize=13, fontweight=\"bold\", y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"pred_vs_real.png\", dpi=130, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfd0c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 13.11  ANÃLISIS DE RESIDUOS   residuo = y_real âˆ’ Å·_pred\n",
    "#\n",
    "#   âœ… Residuos centrados en 0  â†’ sin sesgo\n",
    "#   âœ… Sin patrÃ³n en funciÃ³n de Å· â†’ homocedasticidad\n",
    "#   LÃ­nea naranja = tendencia suavizada (debe ser â‰ˆ 0 horizontal)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, target in enumerate(TARGETS):\n",
    "    info   = MEJOR[target]\n",
    "    y_real = info[\"y_real\"]\n",
    "    y_pred = info[\"y_pred\"]\n",
    "    res    = y_real - y_pred\n",
    "    ax     = axes[i]\n",
    "\n",
    "    n   = min(4_000, len(res))\n",
    "    idx = rng.choice(len(res), n, replace=False)\n",
    "\n",
    "    ax.scatter(y_pred[idx], res[idx],\n",
    "               alpha=0.18, s=8, color=\"#dd8452\", rasterized=True)\n",
    "    ax.axhline(0,           color=\"#e05c7a\", lw=1.8, ls=\"--\", label=\"Î¼ = 0\")\n",
    "    ax.axhline( res.std(),  color=\"#8fb4f8\", lw=1,   ls=\":\",  alpha=0.7, label=\"+1Ïƒ\")\n",
    "    ax.axhline(-res.std(),  color=\"#8fb4f8\", lw=1,   ls=\":\",  alpha=0.7, label=\"âˆ’1Ïƒ\")\n",
    "\n",
    "    # Tendencia suavizada\n",
    "    ys  = np.argsort(y_pred)\n",
    "    wnd = max(1, len(y_pred) // 60)\n",
    "    smt = pd.Series((y_real - y_pred)[ys]).rolling(wnd, center=True, min_periods=1).mean()\n",
    "    ax.plot(y_pred[ys], smt.values, \"#f0a840\", lw=1.5, alpha=0.8, label=\"Tendencia\")\n",
    "\n",
    "    ax.set_title(f\"{target}\\n({info['model_name']})\", fontsize=9.5, fontweight=\"bold\")\n",
    "    ax.set_xlabel(\"PredicciÃ³n\",  fontsize=8.5)\n",
    "    ax.set_ylabel(\"Residuo\",     fontsize=8.5)\n",
    "    ax.tick_params(labelsize=7.5)\n",
    "    ax.legend(fontsize=7)\n",
    "    ax.text(0.97, 0.97,\n",
    "            f\"Î¼ = {res.mean():.2f}\\nÏƒ = {res.std():.2f}\",\n",
    "            transform=ax.transAxes, ha=\"right\", va=\"top\", fontsize=8,\n",
    "            bbox=dict(facecolor=\"#0d0f14\", alpha=0.75, pad=4, edgecolor=\"#252a3a\"),\n",
    "            color=\"white\")\n",
    "\n",
    "fig.suptitle(\"AnÃ¡lisis de Residuos â€” Mejor Modelo por Target\",\n",
    "             fontsize=13, fontweight=\"bold\", y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"residuos.png\", dpi=130, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b1d62d",
   "metadata": {},
   "source": [
    "### ğŸ”¬ 13.12 â€” Feature Importance por target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ed4799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 13.12  IMPORTANCIA DE VARIABLES\n",
    "#\n",
    "#   Ãrboles / RF / HGB  â†’ feature_importances_\n",
    "#   Ridge               â†’ |coef_|\n",
    "#   KNN / MLP           â†’ Permutation Importance\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def get_importancias(model, X_te, y_te):\n",
    "    feats = list(X_te.columns)\n",
    "    if hasattr(model, \"feature_importances_\"):\n",
    "        imp = model.feature_importances_\n",
    "    elif hasattr(model, \"coef_\"):\n",
    "        imp = np.abs(model.coef_).flatten()[:len(feats)]\n",
    "    else:\n",
    "        pi  = permutation_importance(\n",
    "            model, X_te, y_te,\n",
    "            n_repeats=5, random_state=RANDOM_STATE, n_jobs=-1\n",
    "        )\n",
    "        imp = pi.importances_mean\n",
    "    return pd.Series(np.abs(imp[:len(feats)]), index=feats).sort_values(ascending=False)\n",
    "\n",
    "\n",
    "TOP_N = 12\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 13))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, target in enumerate(TARGETS):\n",
    "    info  = MEJOR[target]\n",
    "    m     = info[\"modelo_fit\"]\n",
    "    ax    = axes[i]\n",
    "\n",
    "    y_te_t = y_te_all[target].dropna()\n",
    "    X_te_t = X_te.loc[y_te_t.index]\n",
    "\n",
    "    try:\n",
    "        imp = get_importancias(m, X_te_t, y_te_t.values)\n",
    "        top = imp.head(TOP_N)\n",
    "\n",
    "        cols = [\"#4f8ef7\"] + [\"#1e3a5f\"] * (len(top) - 1)\n",
    "        ax.barh(top.index[::-1], top.values[::-1],\n",
    "                color=cols[::-1], edgecolor=\"#1a1e2b\", height=0.65)\n",
    "        for bar, val in zip(ax.patches, top.values[::-1]):\n",
    "            ax.text(val + top.max() * 0.01,\n",
    "                    bar.get_y() + bar.get_height() / 2,\n",
    "                    f\"{val:.3f}\", va=\"center\", fontsize=7.5, color=\"#8fb4f8\")\n",
    "    except Exception as e:\n",
    "        ax.text(0.5, 0.5, f\"No disponible\\n{e}\",\n",
    "                ha=\"center\", va=\"center\", fontsize=8, transform=ax.transAxes)\n",
    "\n",
    "    ax.set_title(f\"{target}\\n({info['model_name']})\", fontsize=9.5, fontweight=\"bold\")\n",
    "    ax.set_xlabel(\"Importancia\", fontsize=8.5)\n",
    "    ax.tick_params(axis=\"y\", labelsize=8.5)\n",
    "    ax.tick_params(axis=\"x\", labelsize=7.5)\n",
    "\n",
    "fig.suptitle(f\"Top {TOP_N} Variables Predictoras â€” Mejor Modelo por Target\",\n",
    "             fontsize=13, fontweight=\"bold\", y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"feature_importance.png\", dpi=130, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64313a7a",
   "metadata": {},
   "source": [
    "### ğŸ… 13.13 â€” Ranking global de modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e245c999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 13.13  RANKING GLOBAL\n",
    "#   Panel izq : cuÃ¡ntas veces ganÃ³ cada modelo (victorias)\n",
    "#   Panel der : RÂ² promedio de cada modelo sobre los 6 targets\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "model_order = [m for _, m, _ in MODELOS]\n",
    "wins        = Counter(v[\"model_name\"] for v in MEJOR.values())\n",
    "win_vals    = [wins.get(n, 0) for n in model_order]\n",
    "\n",
    "mean_r2_all = (\n",
    "    df_res.groupby(\"model_name\")[\"r2_test\"]\n",
    "    .mean().reindex(model_order).fillna(0)\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Victorias\n",
    "c1 = [\"#42d9a8\" if w > 0 else \"#1e3a5f\" for w in win_vals]\n",
    "b1 = axes[0].barh(model_order, win_vals, color=c1,\n",
    "                   edgecolor=\"#1a1e2b\", height=0.55)\n",
    "axes[0].set_xlabel(\"Targets donde fue el mejor modelo\")\n",
    "axes[0].set_title(\"Â¿CuÃ¡ntas veces ganÃ³ cada modelo?\", fontsize=11, fontweight=\"bold\")\n",
    "axes[0].set_xlim(0, max(win_vals) + 1.5)\n",
    "axes[0].tick_params(axis=\"y\", labelsize=9.5)\n",
    "for bar, val in zip(b1, win_vals):\n",
    "    if val > 0:\n",
    "        axes[0].text(val + 0.05, bar.get_y() + bar.get_height()/2,\n",
    "                     str(val), va=\"center\", fontsize=12,\n",
    "                     color=\"#42d9a8\", fontweight=\"bold\")\n",
    "\n",
    "# RÂ² promedio\n",
    "best_r2 = mean_r2_all.max()\n",
    "c2 = [\"#4f8ef7\" if v == best_r2 else \"#1e3a5f\" for v in mean_r2_all]\n",
    "b2 = axes[1].barh(model_order, mean_r2_all.values, color=c2,\n",
    "                   edgecolor=\"#1a1e2b\", height=0.55)\n",
    "axes[1].set_xlabel(\"RÂ² promedio (6 targets)\")\n",
    "axes[1].set_title(\"RÂ² Promedio por Modelo\", fontsize=11, fontweight=\"bold\")\n",
    "axes[1].tick_params(axis=\"y\", labelsize=9.5)\n",
    "for bar, val in zip(b2, mean_r2_all.values):\n",
    "    axes[1].text(val + 0.002, bar.get_y() + bar.get_height()/2,\n",
    "                 f\"{val:.3f}\", va=\"center\", fontsize=9,\n",
    "                 color=\"#4f8ef7\" if val == best_r2 else \"#5c6280\")\n",
    "\n",
    "fig.suptitle(\"Ranking Global â€” 6 Modelos Ã— 6 Targets\",\n",
    "             fontsize=13, fontweight=\"bold\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"ranking_global.png\", dpi=130, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nResumen de victorias:\")\n",
    "for n, w, r2 in zip(model_order, win_vals, mean_r2_all.values):\n",
    "    barra = \"â–ˆ\" * w + \"â–‘\" * (len(TARGETS) - w)\n",
    "    print(f\"  {n:<26}  {barra}  {w} victorias  |  RÂ² prom = {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d4abf0",
   "metadata": {},
   "source": [
    "### ğŸ’¾ 13.14 â€” SerializaciÃ³n de los mejores modelos"
   ]
  },
  {
   "id": "468a7d1a",
   "cell_type": "markdown",
   "source": "### ğŸ”§ 13.13b â€” Ajuste suave de hiperparÃ¡metros (Top 3 modelos)\n\nSe seleccionan los **3 mejores modelos** por RÂ² promedio en todos los targets  \ny se les aplica `HalvingGridSearchCV` â€” busqueda rapida que descarta candidatos  \nmalos en cada ronda, mucho mas eficiente que GridSearchCV completo.\n\n- **factor=3**: descarta 2/3 de candidatos por ronda\n- **CV=3 folds**: balanceo entre precision y velocidad\n- **Grids reducidos**: solo 2-3 hiperparametros clave, max 27 combinaciones\n- **Conservador**: solo reemplaza si RÂ² optimizado >= RÂ² base",
   "metadata": {}
  },
  {
   "id": "ae24e85a",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# 13.13b  AJUSTE SUAVE DE HIPERPARAMETROS â€” TOP 3 MODELOS\n# HalvingGridSearchCV: rapido y ejecutable en Colab sin timeout\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\nfrom sklearn.experimental import enable_halving_search_cv  # noqa\nfrom sklearn.model_selection import HalvingGridSearchCV\nimport copy, time\n\n# â”€â”€ Identificar top-3 modelos por RÂ² promedio en todos los targets â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nif df_res is not None and len(df_res) > 0:\n    ranking = (\n        df_res.groupby(\"model_id\")[\"r2\"]\n        .mean()\n        .sort_values(ascending=False)\n    )\n    top3_ids = ranking.head(3).index.tolist()\n    print(f\"Top 3 modelos para ajuste: {top3_ids}\")\n    print(ranking.head(5).to_string())\nelse:\n    top3_ids = [\"M6_HGB\", \"M7_GBR\", \"M5_RF\"]\n    print(f\"df_res no disponible, usando top3 por defecto: {top3_ids}\")\n\n# â”€â”€ Grids reducidos por modelo â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# Solo 2-3 hiperparametros mas influyentes, max 27 combinaciones por modelo\nGRIDS = {\n    \"M1_Ridge\"  : {\"alpha\": [0.01, 0.1, 1.0, 10.0, 100.0]},\n    \"M2_KNN\"    : {\"n_neighbors\": [5, 9, 15, 21],\n                   \"metric\": [\"euclidean\", \"manhattan\"]},\n    \"M3_DTree\"  : {\"max_depth\": [5, 8, 12, 16],\n                   \"min_samples_leaf\": [10, 20, 40]},\n    \"M4_MLP\"    : {\"hidden_layer_sizes\": [(128,64,32),(256,128,64)],\n                   \"alpha\": [1e-4, 1e-3],\n                   \"max_iter\": [500]},\n    \"M5_RF\"     : {\"n_estimators\": [100, 200],\n                   \"max_depth\": [8, 12, 16],\n                   \"min_samples_leaf\": [3, 5]},\n    \"M6_HGB\"    : {\"max_iter\": [100, 200, 300],\n                   \"learning_rate\": [0.03, 0.05, 0.1],\n                   \"max_depth\": [5, 8]},\n    \"M7_GBR\"    : {\"n_estimators\": [100, 200],\n                   \"learning_rate\": [0.03, 0.05, 0.1],\n                   \"max_depth\": [3, 5]},\n    \"M8_SVR\"    : {\"C\": [0.1, 1.0, 10.0], \"epsilon\": [0.05, 0.1]},\n    \"M9_Deep\"   : {\"hidden_layer_sizes\": [(256,128,64,32),(512,256,128)],\n                   \"alpha\": [1e-4, 1e-3]},\n    \"M10_Voting\": {},   # VotingRegressor: no se ajusta directamente\n}\n\n# â”€â”€ Funcion de ajuste para un modelo y un target â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ndef ajustar_modelo(mid, mname, modelo_base, target):\n    \"\"\"\n    Ajusta hiperparametros con HalvingGridSearchCV.\n    Retorna el mejor modelo solo si mejora el RÂ² base.\n    \"\"\"\n    if modelo_base is None or mid not in GRIDS or not GRIDS[mid]:\n        return None\n\n    grid = GRIDS[mid]\n    y_tr = y_tr_all[target].dropna()\n    X_tr_t = X_tr.loc[y_tr.index]\n\n    t0 = time.time()\n    try:\n        search = HalvingGridSearchCV(\n            copy.deepcopy(modelo_base),\n            param_grid=grid,\n            factor=3,\n            cv=3,\n            scoring=\"r2\",\n            n_jobs=-1,\n            random_state=RANDOM_STATE,\n            refit=True,\n            verbose=0,\n        )\n        search.fit(X_tr_t, y_tr)\n\n        # Evaluar en test\n        y_te = y_te_all[target].dropna()\n        X_te_t = X_te.loc[y_te.index]\n        y_pred_opt = search.best_estimator_.predict(X_te_t)\n        r2_opt = r2_score(y_te, y_pred_opt)\n\n        elapsed = time.time() - t0\n        return {\n            \"modelo_opt\": search.best_estimator_,\n            \"best_params\": search.best_params_,\n            \"r2_opt\": r2_opt,\n            \"elapsed\": elapsed,\n        }\n    except Exception as e:\n        print(f\"    Error en {mid}/{target}: {e}\")\n        return None\n\n# â”€â”€ Ejecutar ajuste para top-3 modelos en los 6 targets â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nFITS_OPT = {}   # {target: {model_id: resultado}}\nprint()\nprint(f\"{'Target':<35} {'Modelo':<15} {'R2_base':>8} {'R2_opt':>8} {'Delta':>7} {'Estado'}\")\nprint(\"-\" * 85)\n\nfor target in TARGETS:\n    FITS_OPT[target] = {}\n    y_te_t = y_te_all[target].dropna()\n    X_te_t = X_te.loc[y_te_t.index]\n\n    for mid in top3_ids:\n        # Buscar instancia base en MODELOS\n        modelo_base = next((m for i, n, m in MODELOS if i == mid), None)\n        mname       = next((n for i, n, m in MODELOS if i == mid), mid)\n\n        if modelo_base is None:\n            continue\n\n        # R2 base del entrenamiento original\n        if target in FITS_BASE and mid in FITS_BASE[target]:\n            info_base = FITS_BASE[target][mid]\n            r2_base   = info_base.get(\"r2\", 0.0)\n            modelo_ajustado_base = info_base.get(\"modelo_fit\")\n        else:\n            continue\n\n        resultado = ajustar_modelo(mid, mname, modelo_ajustado_base, target)\n\n        if resultado is None:\n            continue\n\n        r2_opt   = resultado[\"r2_opt\"]\n        delta    = r2_opt - r2_base\n        usar_opt = r2_opt >= r2_base  # conservador: solo si mejora o iguala\n\n        if usar_opt:\n            FITS_OPT[target][mid] = resultado\n            estado = \"MEJOR\" if delta > 0.001 else \"IGUAL\"\n        else:\n            estado = \"BASE\"\n\n        print(f\"{target:<35} {mid:<15} {r2_base:>8.4f} {r2_opt:>8.4f} {delta:>+7.4f}  {estado}\")\n\nprint()\nprint(\"Ajuste de hiperparametros completado.\")\nprint(f\"  Modelos mejorados: {sum(len(v) for v in FITS_OPT.values())}\")\n\n# â”€â”€ Actualizar MEJOR con modelos optimizados â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nn_actualizados = 0\nfor target, opts in FITS_OPT.items():\n    if target not in MEJOR:\n        continue\n    mid_mejor = MEJOR[target][\"model_id\"]\n    if mid_mejor in opts:\n        opt = opts[mid_mejor]\n        if opt[\"r2_opt\"] >= MEJOR[target][\"r2\"]:\n            MEJOR[target][\"modelo_fit\"] = opt[\"modelo_opt\"]\n            MEJOR[target][\"r2\"]         = opt[\"r2_opt\"]\n            MEJOR[target][\"best_params\"] = opt[\"best_params\"]\n            n_actualizados += 1\n            print(f\"  Actualizado MEJOR[{target}] con modelo optimizado  R2={opt['r2_opt']:.4f}\")\n\nprint(f\"\\n{n_actualizados} modelos actualizados con version optimizada.\")\n",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d7ca09",
   "metadata": {},
   "outputs": [],
   "source": "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# 13.14  EXPORTACION DE LOS MEJORES MODELOS\n# Nombre: icfes_mejor_{target}.joblib\n# Incluye: modelo_fit, vars_sig, metricas, categorias de encoders\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\nimport os\nprint(\"Exportando mejores modelos...\")\nOUTPUT_DIR = \".\"\n\nfor target, info in MEJOR.items():\n    fname = os.path.join(OUTPUT_DIR, f\"icfes_mejor_{target.lower()}.joblib\")\n\n    # Categorias texto de educacion madre/padre para la app (no niveles numericos)\n    edu_categorias = {}\n    for col in [\"FAMI_EDUCACIONMADRE\", \"FAMI_EDUCACIONPADRE\"]:\n        if col in encoders:\n            edu_categorias[col] = list(encoders[col].categories_[0])\n\n    payload = {\n        \"target\"          : target,\n        \"model_id\"        : info[\"model_id\"],\n        \"model_name\"      : info[\"model_name\"],\n        \"tipo\"            : info.get(\"tipo\", \"ensemble\"),\n        \"modelo_fit\"      : info[\"modelo_fit\"],   # objeto sklearn entrenado\n        \"vars_sig\"        : info.get(\"vars_sig\", list(X_tr.columns)),\n        \"n_vars\"          : info.get(\"n_vars\", X_tr.shape[1]),\n        \"metricas\"        : {\n            \"r2_test\"   : info[\"r2\"],\n            \"rmse_test\" : info[\"rmse\"],\n            \"mae_test\"  : info[\"mae\"],\n            \"mape_test\" : info[\"mape\"],\n            \"r2_cv\"     : info[\"r2_cv\"],\n            \"r2_cv_std\" : info[\"r2_cv_std\"],\n        },\n        \"y_pred_test\"     : info.get(\"y_pred\", []),\n        \"y_real_test\"     : info.get(\"y_real\", []),\n        \"n_train\"         : len(X_tr),\n        \"n_test\"          : len(X_te),\n        \"random_state\"    : RANDOM_STATE,\n        \"features_all\"    : list(X_tr.columns),\n        \"edu_categorias\"  : edu_categorias,   # categorias texto para la app\n        \"exportado_en\"    : pd.Timestamp.now().isoformat(),\n    }\n\n    joblib.dump(payload, fname, compress=3)\n    kb = os.path.getsize(fname) / 1024\n\n    # Verificacion critica: modelo_fit no debe ser None\n    check = joblib.load(fname)\n    estado = \"OK\" if check[\"modelo_fit\"] is not None else \"ERROR: modelo_fit es None\"\n    print(f\"  {fname:<55} {kb:>7.1f} KB  [{estado}]\")\n\n# â”€â”€ pipeline_artefacts.joblib actualizado â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nartefacts_path = os.path.join(OUTPUT_DIR, \"pipeline_artefacts.joblib\")\nfrom datetime import datetime\n\npipeline_artefacts = {\n    \"scaler\"          : scaler,\n    \"encoders\"        : encoders,\n    \"independent_vars\": EXPECTED_X_FINAL,\n    \"target_vars\"     : EXPECTED_Y_FINAL,\n    \"score_cols\"      : SCORE_COLS,\n    \"asset_cols\"      : ASSET_COLS,\n    \"yn_cols\"         : YN_COLS,\n    \"na_values\"       : NA_VALUES,\n    \"edu_categorias\"  : edu_categorias,   # categorias texto para la app\n    \"n_train_rows\"    : len(X_tr),\n    \"n_features\"      : len(EXPECTED_X_FINAL),\n    \"feature_range\"   : (0, 1),\n    \"created_at\"      : datetime.now().isoformat(),\n}\njoblib.dump(pipeline_artefacts, artefacts_path, compress=3)\nkb = os.path.getsize(artefacts_path) / 1024\nprint(f\"\\n  pipeline_artefacts.joblib actualizado: {kb:.1f} KB\")\nprint(\"\\nExportacion completada.\")\n"
  },
  {
   "cell_type": "markdown",
   "id": "d135e5f4",
   "metadata": {},
   "source": [
    "### ğŸ“‹ 13.15 â€” Resumen ejecutivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b74efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 13.15  RESUMEN EJECUTIVO FINAL\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=\" * 72)\n",
    "print(\"  RESUMEN â€” 6 Modelos Ã— 6 Targets  |  ICFES Saber 11\")\n",
    "print(\"=\" * 72)\n",
    "print(f\"  Dataset  : {len(X_final):,} registros Ã— {len(X_tr.columns)} variables X\")\n",
    "print(f\"  Train    : {len(X_tr):,}  |  Test: {len(X_te):,}\")\n",
    "print(f\"  CV       : {N_FOLDS}-Fold Stratified  |  scoring = RÂ²\")\n",
    "print(f\"  Modelos  : {len(MODELOS)}  |  Targets: {len(TARGETS)}\")\n",
    "print(f\"  Experim. : {len(df_res)}\")\n",
    "\n",
    "print(f\"\\n  {'Target':<35}  {'Mejor Modelo':<24}  {'RÂ²':>7}  {'RMSE':>7}  {'MAE':>7}\")\n",
    "print(f\"  {'â”€'*35}  {'â”€'*24}  {'â”€'*7}  {'â”€'*7}  {'â”€'*7}\")\n",
    "\n",
    "all_r2 = [v[\"r2\"] for v in MEJOR.values()]\n",
    "for target in TARGETS:\n",
    "    v  = MEJOR[target]\n",
    "    mk = \"ğŸ¥‡\" if v[\"r2\"] == max(all_r2) else \"  \"\n",
    "    print(f\"  {mk} {target:<33}  {v['model_name']:<24}  \"\n",
    "          f\"{v['r2']:>7.4f}  {v['rmse']:>7.3f}  {v['mae']:>7.3f}\")\n",
    "\n",
    "wins   = Counter(v[\"model_name\"] for v in MEJOR.values())\n",
    "top_m  = wins.most_common(1)[0]\n",
    "print(f\"\\n  RÂ² medio (mejores)  : {np.mean(all_r2):.4f}\")\n",
    "print(f\"  RÂ² mÃ­nimo           : {min(all_r2):.4f}  \"\n",
    "      f\"â€” {[t for t in TARGETS if MEJOR[t]['r2']==min(all_r2)][0]}\")\n",
    "print(f\"  RÂ² mÃ¡ximo           : {max(all_r2):.4f}  \"\n",
    "      f\"â€” {[t for t in TARGETS if MEJOR[t]['r2']==max(all_r2)][0]}\")\n",
    "print(f\"  Modelo dominante    : {top_m[0]}  ({top_m[1]}/{len(TARGETS)} targets)\")\n",
    "print(f\"\\n  {datetime.now().strftime('%Y-%m-%d  %H:%M:%S')}\")\n",
    "print(\"=\" * 72)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}